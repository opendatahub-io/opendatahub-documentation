:_module-type: ASSEMBLY

ifdef::context[:parent-context: {context}]


[id="running-kfto-based-distributed-training-workloads_{context}"]
= Running KFTO-based distributed training workloads

[role='_abstract']
To reduce the time needed to train a Large Language Model (LLM), you can run the training job in parallel.
In {productname-long}, the Kubeflow Training Operator (KFTO) and Kubeflow Training Operator Python Software Development Kit (KFTO SDK) simplify the job configuration.

You can use the KFTO and the KFTO SDK to configure a training job in a variety of ways.
For example, you can use multiple nodes and multiple GPUs per node, fine-tune a model, or configure a training job to use Remote Direct Memory Access (RDMA).

include::./using-the-kubeflow-training-operator-to-run-distributed-training-workloads.adoc[leveloffset=+1]
include::./using-the-kfto-sdk-to-run-distributed-training-workloads.adoc[leveloffset=+1]
include::./fine-tuning-a-model-by-using-kubeflow-training.adoc[leveloffset=+1]
include::modules/creating-a-multi-node-pytorch-training-job-with-rdma.adoc[leveloffset=+1]
include::modules/ref-example-kfto-pytorchjob-resource-configured-to-run-with-rdma.adoc[leveloffset=+1]

ifdef::parent-context[:context: {parent-context}]
ifndef::parent-context[:!context:]
