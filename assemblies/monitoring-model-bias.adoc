:_module-type: ASSEMBLY

ifdef::context[:parent-context: {context}]

:context: bias-monitoring

[id="monitoring-model-bias_{context}"]
= Monitoring model bias

[role='_abstract']

As a data scientist, you might want to monitor your machine learning models for bias. This means monitoring for algorithmic deficiencies that might skew the outcomes or decisions that the model produces. Importantly, this type of monitoring helps you to ensure that the model is not biased against particular protected groups or features.

{productname-long} provides a set of metrics that help you to monitor your models for bias. You can use the {productname-short} interface to choose an available metric and then configure model-specific details such as a protected attribute, the privileged and unprivileged groups, the outcome you want to monitor, and a threshold for bias. You then see a chart of the calculated values for a specified number of model inferences.


//Statistical Parity Difference (SPD) measures imbalances in classifications by calculating the difference between the proportion of the majority and protected classes getting a particular outcome. Typically, -0.1 < SPD < 0.1 indicates a fair model, while a value outside those bounds indicates an unfair model for the groups and outcomes in question.


include::modules/creating-a-bias-metric.adoc[leveloffset=+1]

include::modules/duplicating-a-bias-metric.adoc[leveloffset=+2]

include::modules/deleting-a-bias-metric.adoc[leveloffset=+1]

include::modules/viewing-bias-metrics.adoc[leveloffset=+1]

include::modules/supported-bias-metrics.adoc[leveloffset=+1]

ifdef::parent-context[:context: {parent-context}]
ifndef::parent-context[:!context:]
