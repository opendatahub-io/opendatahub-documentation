:_module-type: ASSEMBLY

ifdef::context[:parent-context: {context}]

[id="example-kfto-pytorch-training-scripts_{context}"]
= Example KFTO PyTorch training scripts

[role='_abstract']
The following examples show how to configure a PyTorch training script for NVIDIA Collective Communications Library (NCCL), Distributed Data Parallel (DDP), and Fully Sharded Data Parallel (FSDP) training jobs. 

[NOTE]
====
If you have the required resources, you can run the example code without editing it.

Alternatively, you can modify the example code to specify the appropriate configuration for your training job.
====


include::modules/ref-example-kfto-pytorch-training-script-nccl.adoc[leveloffset=+1]
include::modules/ref-example-kfto-pytorch-training-script-ddp.adoc[leveloffset=+1]
include::modules/ref-example-kfto-pytorch-training-script-fsdp.adoc[leveloffset=+1]
