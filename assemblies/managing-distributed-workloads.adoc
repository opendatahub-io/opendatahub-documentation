:_module-type: ASSEMBLY

ifdef::context[:parent-context: {context}]

[id="managing-distributed-workloads_{context}"]
= Managing distributed training workloads

[role='_abstract']
In {productname-short}, distributed workloads like `PyTorchJob`, `RayJob`, and `RayCluster` are created and managed by their respective workload operators. Kueue provides queueing and admission control and integrates with these operators to decide when workloads can run based on cluster-wide quotas.

You can perform advanced configuration for your distributed workloads environment, such as changing the default behavior of the CodeFlare Operator or setting up a cluster for RDMA.

include::modules/overview-of-kueue-resources.adoc[leveloffset=+1]

include::modules/ref-example-kueue-resource-configurations.adoc[leveloffset=+1]

include::modules/configuring-quota-management-for-distributed-workloads.adoc[leveloffset=+1]

include::assemblies/enforcing-local-queues.adoc[leveloffset=+1]

include::modules/configuring-the-codeflare-operator.adoc[leveloffset=+1]

include::modules/troubleshooting-common-problems-with-distributed-workloads-for-administrators.adoc[leveloffset=+1]

ifdef::parent-context[:context: {parent-context}]
ifndef::parent-context[:!context:]
