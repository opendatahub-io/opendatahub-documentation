:_module-type: ASSEMBLY

ifdef::context[:parent-context: {context}]


[id="using-the-kubeflow-training-operator-to-run-distributed-training-workloads_{context}"]
= Using the Kubeflow Training Operator to run distributed training workloads 

[role='_abstract']
You can use the KFTO `PyTorchJob` API to configure a `PyTorchJob` resource so that the training job runs on multiple nodes with multiple GPUs.

You can store the training script in a `ConfigMap` resource, or include it in a custom container image.


include::modules/creating-a-kfto-pytorch-training-script-configmap-resource.adoc[leveloffset=+1]
include::modules/creating-a-kfto-pytorchjob-resource.adoc[leveloffset=+1]
include::modules/creating-a-kfto-pytorchjob-resource-by-using-the-cli.adoc[leveloffset=+1]
include::./example-kfto-pytorch-training-scripts.adoc[leveloffset=+1]
include::modules/ref-example-dockerfile-for-a-kfto-pytorch-training-script.adoc[leveloffset=+1]
include::modules/ref-example-kfto-pytorchjob-resource-for-multi-node-training.adoc[leveloffset=+1]

ifdef::parent-context[:context: {parent-context}]
ifndef::parent-context[:!context:]
