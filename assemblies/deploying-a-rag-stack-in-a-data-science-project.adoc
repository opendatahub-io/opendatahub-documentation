:_module-type: ASSEMBLY

ifdef::context[:parent-context: {context}]

[id="deploying-a-rag-stack-in-a-data-science-project_{context}"]
= Deploying a RAG stack in a data science project

[role='_abstract']
As an {openshift-platform} cluster administrator, you can deploy a Retrievalâ€‘Augmented Generation (RAG) stack in {productname-short}. This stack provides the infrastructure, including LLM inference, vector storage, and retrieval services that data scientists and AI engineers use to build conversational workflows in their projects.

To deploy the RAG stack in a data science project, complete the following tasks:

* Activate the Llama Stack Operator in {productname-short}.
* Enable GPU support on the {openshift-platform} cluster. This task includes installing the required NVIDIA Operators.
* Deploy an inference model, for example, the llama-3.2-3b-instruct model. This task includes creating a storage connection and configuring GPU allocation.
* Create a `LlamaStackDistribution` instance to enable RAG functionality. This action deploys LlamaStack alongside a Milvus vector store and connects both components to the inference model.
* Ingest domain data into Milvus by running Docling in a data science pipeline or Jupyter notebook. This process keeps the embeddings synchronized with the source data.
* Expose and secure the model endpoints.

include::modules/installing-the-llamastack-operator.adoc[leveloffset=+1]
include::modules/ingesting-content-into-a-llama-model.adoc[leveloffset=+1]

ifdef::parent-context[:context: {parent-context}]
ifndef::parent-context[:!context:]