:_module-type: ASSEMBLY

ifdef::context[:parent-context: {context}]

:context: serving-large-models

[id="serving-large-models_{context}"]
= Serving large models

[role='_abstract']
For deploying large models such as large language models (LLMs), {productname-long} includes a single-model serving platform that is based on the KServe component. Because each model is deployed from its own model server, the single-model serving platform helps you to deploy, monitor, scale, and maintain large models that require more resources.

include::modules/about-the-single-model-serving-platform.adoc[leveloffset=+1]
include::modules/about-kserve-deployment-modes.adoc[leveloffset=+1]
include::modules/installing-kserve.adoc[leveloffset=+1]
include::modules/deploying-models-using-the-single-model-serving-platform.adoc[leveloffset=+1]
include::modules/enabling-the-single-model-serving-platform.adoc[leveloffset=+2]
include::modules/adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform.adoc[leveloffset=+2]
include::modules/adding-a-tested-and-verified-runtime-for-the-single-model-serving-platform.adoc[leveloffset=+2]
include::modules/deploying-models-on-the-single-model-serving-platform.adoc[leveloffset=+2]
include::modules/customizing-parameters-serving-runtime.adoc[leveloffset=+2]
include::modules/customizable-model-serving-runtime-parameters.adoc[leveloffset=+2]
include::modules/using-oci-containers-for-model-storage.adoc[leveloffset=+2]
include::modules/accessing-inference-endpoint-for-model-deployed-on-single-model-serving-platform.adoc[leveloffset=+2]
ifdef::self-managed[]
include::modules/deploying-models-on-single-node-openshift-using-kserve-raw-deployment-mode.adoc[leveloffset=+3]
include::modules/deploying-models-using-multiple-gpu-nodes.adoc[leveloffset=+3]
endif::[]
include::modules/deploying-models-using-multiple-gpu-nodes.adoc[leveloffset=+3]
// Conditionalized for self-managed because monitoring of user-defined projects is enabled on OSD and ROSA by default
ifdef::upstream,self-managed[]
include::modules/configuring-monitoring-for-the-single-model-serving-platform.adoc[leveloffset=+1]
endif::[]

include::modules/viewing-metrics-for-the-single-model-serving-platform.adoc[leveloffset=+1]

== Monitoring model performance

In the single-model serving platform, you can view performance metrics for a specific model that is deployed on the platform.

include::modules/viewing-performance-metrics-for-deployed-model.adoc[leveloffset=+2]

== Optimizing model-serving runtimes

You can optionally enhance the preinstalled model-serving runtimes available in {productname-short} to leverage additional benefits and capabilities, such as optimized inferencing, reduced latency, and fine-tuned resource allocation. 

include::modules/optimizing-the-vllm-runtime.adoc[leveloffset=+2]

== Performance tuning on the single-model serving platform
Certain performance issues might require you to tune the parameters of your inference service or model-serving runtime.

include::modules/resolving-cuda-oom-errors.adoc[leveloffset=+2]

include::modules/ref-supported-runtimes.adoc[leveloffset=+1]
include::modules/ref-tested-verified-runtimes.adoc[leveloffset=+1]
include::modules/ref-inference-endpoints.adoc[leveloffset=+1]

include::modules/about-the-NVIDIA-NIM-model-serving-platform.adoc[leveloffset=+1]
include::modules/enabling-the-nvidia-nim-model-serving-platform.adoc[leveloffset=+2]
include::modules/deploying-models-on-the-NVIDIA-NIM-model-serving-platform.adoc[leveloffset=+2]
// [role='_additional-resources']
// == Additional resources

ifdef::parent-context[:context: {parent-context}]
ifndef::parent-context[:!context:]
