:_module-type: ASSEMBLY

ifdef::context[:parent-context: {context}]

:context: ds-pipelines

[id="working-with-data-science-pipelines_{context}"]
= Working with data science pipelines

[role='_abstract']
As a data scientist, you can enhance your data science projects on {productname-short} by building portable machine learning (ML) workflows with data science pipelines, using Docker containers. This enables you to standardize and automate machine learning workflows to enable you to develop and deploy your data science models.

For example, the steps in a machine learning workflow might include items such as data extraction, data processing, feature extraction, model training, model validation, and model serving. Automating these activities enables your organization to develop a continuous process of retraining and updating a model based on newly received data. This can help address challenges related to building an integrated machine learning deployment and continuously operating it in production.

ifndef::upstream[]
You can also use the Elyra JupyterLab extension to create and run data science pipelines within JupyterLab. For more information, see link:{rhodsdocshome}{default-format-url}/working_on_data_science_projects/working-with-data-science-pipelines_ds-pipelines#working_with_pipelines_in_jupyterlab[Working with pipelines in JupyterLab].
endif::[]
ifdef::upstream[]
You can also use the Elyra JupyterLab extension to create and run data science pipelines within JupyterLab. For more information, see link:{odhdocshome}{default-format-url}/working_on_data_science_projects/working-with-data-science-pipelines_ds-pipelines#working_with_pipelines_in_jupyterlab[Working with pipelines in JupyterLab].
endif::[]

A data science pipeline in {productname-short} consists of the following components:

* Pipeline server: A server that is attached to your data science project and hosts your data science pipeline.
* Pipeline: A pipeline defines the configuration of your machine learning workflow and the relationship between each component in the workflow.
** Pipeline code: A definition of your pipeline in a Tekton-formatted YAML file.
** Pipeline graph: A graphical illustration of the steps executed in a pipeline run and the relationship between them.
* Pipeline run: An execution of your pipeline.
** Triggered run: A previously executed pipeline run.
** Scheduled run: A pipeline run scheduled to execute at least once.

This feature is based on Kubeflow Pipelines v1. Use the Kubeflow Pipelines SDK to build your data science pipeline in Python code. After you have built your pipeline, compile it into Tekton-formatted YAML code using kfp-tekton SDK (version 1.5.x only). The {productname-short} user interface enables you to track and manage pipelines and pipeline runs.

ifdef::upstream[]
Before you can use data science pipelines, you must install the Data Science Pipelines operator as described in link:https://github.com/opendatahub-io/data-science-pipelines-operator[Data Science Pipelines Operator].
endif::[]
ifndef::upstream[]
Before you can use data science pipelines, you must install the OpenShift Pipelines operator. For more information about installing a compatible version of the OpenShift Pipelines operator, see link:https://access.redhat.com/documentation/en-us/openshift_container_platform/{ocp-latest-version}/html/cicd/pipelines#op-release-notes[{org-name} OpenShift Pipelines release notes] and link:https://access.redhat.com/articles/6986416[{productname-long}: Supported Configurations].
endif::[]

You can store your pipeline artifacts in an S3-compatible object storage bucket so that you do not consume local storage. To do this, you must first configure write access to your S3 bucket on your storage account.

== Managing data science pipelines

include::modules/configuring-a-pipeline-server.adoc[leveloffset=+2]

include::modules/defining-a-pipeline.adoc[leveloffset=+2]

include::modules/importing-a-data-science-pipeline.adoc[leveloffset=+2]

include::modules/downloading-a-data-science-pipeline.adoc[leveloffset=+2]

include::modules/deleting-a-data-science-pipeline.adoc[leveloffset=+2]

include::modules/deleting-a-pipeline-server.adoc[leveloffset=+2]

include::modules/viewing-the-details-of-a-pipeline-server.adoc[leveloffset=+2]

include::modules/viewing-existing-pipelines.adoc[leveloffset=+2]


== Managing pipeline runs

include::modules/overview-of-pipeline-runs.adoc[leveloffset=+2]

//include::modules/overview-of-pipeline-experiments.adoc[leveloffset=+2] 

include::modules/scheduling-a-pipeline-run.adoc[leveloffset=+2]

//include::modules/creating-a-pipeline-experiment.adoc[leveloffset=+2] 

//include::modules/updating-a-pipeline-experiment.adoc[leveloffset=+2] 

//include::modules/deleting-a-pipeline-experiment.adoc[leveloffset=+2] 

include::modules/cloning-a-scheduled-pipeline-run.adoc[leveloffset=+2]

include::modules/stopping-a-triggered-pipeline-run.adoc[leveloffset=+2]

include::modules/deleting-a-scheduled-pipeline-run.adoc[leveloffset=+2]

include::modules/deleting-a-triggered-pipeline-run.adoc[leveloffset=+2]

include::modules/viewing-scheduled-pipeline-runs.adoc[leveloffset=+2]

include::modules/viewing-triggered-pipeline-runs.adoc[leveloffset=+2]

include::modules/viewing-the-details-of-a-pipeline-run.adoc[leveloffset=+2]

== Working with pipelines in JupyterLab

include::modules/overview-of-pipelines-in-jupyterlab.adoc[leveloffset=+2]

include::modules/accessing-the-pipeline-editor.adoc[leveloffset=+2]

include::modules/creating-a-runtime-configuration.adoc[leveloffset=+2]

include::modules/updating-a-runtime-configuration.adoc[leveloffset=+2]

include::modules/deleting-a-runtime-configuration.adoc[leveloffset=+2]

include::modules/duplicating-a-runtime-configuration.adoc[leveloffset=+2]

//include::modules/defining-pipeline-parameters-in-jupyterlab.adoc[leveloffset=+2] 

include::modules/running-a-pipeline-in-jupyterlab.adoc[leveloffset=+2]

include::modules/exporting-a-pipeline-in-jupyterlab.adoc[leveloffset=+2]


[role='_additional-resources']
== Additional resources
* link:https://www.kubeflow.org/docs/components/pipelines/v1/[Kubeflow Pipelines v1 Documentation]
ifndef::upstream[]
* link:{rhodsdocshome}{default-format-url}/working_on_data_science_projects/working-with-data-science-pipelines_ds-pipelines#working_with_pipelines_in_jupyterlab[Working with pipelines in JupyterLab].
endif::[]
ifdef::upstream[]
link:{odhdocshome}{default-format-url}/working_on_data_science_projects/working-with-data-science-pipelines_ds-pipelines#working_with_pipelines_in_jupyterlab[Working with pipelines in JupyterLab].
endif::[]

ifdef::parent-context[:context: {parent-context}]
ifndef::parent-context[:!context:]
