:_module-type: ASSEMBLY

ifdef::context[:parent-context: {context}]

:context: ds-pipelines

[id="working-with-data-science-pipelines_{context}"]
= Working with data science pipelines

[role='_abstract']
As a data scientist, you can enhance your data science projects on {productname-short} by building portable machine learning (ML) workflows with data science pipelines, using Docker containers. This enables you to standardize and automate machine learning workflows to enable you to develop and deploy your data science models.

For example, the steps in a machine learning workflow might include items such as data extraction, data processing, feature extraction, model training, model validation, and model serving. Automating these activities enables your organization to develop a continuous process of retraining and updating a model based on newly received data. This can help resolve challenges related to building an integrated machine learning deployment and continuously operating it in production.

ifndef::upstream[]
You can also use the Elyra JupyterLab extension to create and run data science pipelines within JupyterLab. For more information, see link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects/working-with-data-science-pipelines_ds-pipelines#working_with_pipelines_in_jupyterlab[Working with pipelines in JupyterLab].
endif::[]
ifdef::upstream[]
You can also use the Elyra JupyterLab extension to create and run data science pipelines within JupyterLab. For more information, see link:{odhdocshome}/working-on-data-science-projects/#_working_with_pipelines_in_jupyterlab[Working with pipelines in JupyterLab].
endif::[]

ifdef::upstream[]
From {productname-long} version 2.10.0, data science pipelines are based on link:https://www.kubeflow.org/docs/components/pipelines/v2/[KubeFlow Pipelines (KFP) version 2.0]. For more information, see link:{odhdocshome}/working-on-data-science-projects/#enabling-data-science-pipelines-2_ds-pipelines[Enabling Data Science Pipelines 2.0].
endif::[]
ifndef::upstream[]
ifdef::self-managed[]
From {productname-short} version 2.9, data science pipelines are based on link:https://www.kubeflow.org/docs/components/pipelines/v2/[KubeFlow Pipelines (KFP) version 2.0]. For more information, see link:{rhoaidocshome}{default-format-url}working_on_data_science_projects/working-with-data-science-pipelines_ds-pipelines#enabling-data-science-pipelines-2_ds-pipelines[Enabling Data Science Pipelines 2.0].
endif::[]
ifdef::cloud-service[]
Data science pipelines in {productname-short} are now based on link:https://www.kubeflow.org/docs/components/pipelines/v2/[KubeFlow Pipelines (KFP) version 2.0]. For more information, see link:{rhoaidocshome}{default-format-url}working_on_data_science_projects/working-with-data-science-pipelines_ds-pipelines#enabling-data-science-pipelines-2_ds-pipelines[Enabling Data Science Pipelines 2.0].
endif::[]
endif::[]

A data science pipeline in {productname-short} consists of the following components:

* Pipeline server: A server that is attached to your data science project and hosts your data science pipeline.
* Pipeline: A pipeline defines the configuration of your machine learning workflow and the relationship between each component in the workflow.
** Pipeline code: A definition of your pipeline in a YAML file.
** Pipeline graph: A graphical illustration of the steps executed in a pipeline run and the relationship between them.
* Pipeline run: An execution of your pipeline.
** Active run: A pipeline run that is in its execution phase, or is stopped.
** Scheduled run: A pipeline run scheduled to execute at least once.
** Archived run: A pipeline run that resides in the run archive and is no longer required. 

This feature is based on Kubeflow Pipelines 2.0. Use the latest Kubeflow Pipelines 2.0 SDK to build your data science pipeline in Python code. After you have built your pipeline, use the SDK to compile it into an Intermediate Representation (IR) YAML file. The {productname-short} user interface enables you to track and manage pipelines and pipeline runs.

You can store your pipeline artifacts in an S3-compatible object storage bucket so that you do not consume local storage. To do this, you must first configure write access to your S3 bucket on your storage account.

//Enabling DSPv2
include::modules/enabling-data-science-pipelines-2.adoc[leveloffset=+1]

== Managing data science pipelines

include::modules/configuring-a-pipeline-server.adoc[leveloffset=+2]

include::modules/defining-a-pipeline.adoc[leveloffset=+2]

include::modules/importing-a-data-science-pipeline.adoc[leveloffset=+2]

include::modules/downloading-a-data-science-pipeline-version.adoc[leveloffset=+2]

include::modules/deleting-a-data-science-pipeline.adoc[leveloffset=+2]

include::modules/deleting-a-pipeline-server.adoc[leveloffset=+2]

include::modules/viewing-the-details-of-a-pipeline-server.adoc[leveloffset=+2]

include::modules/viewing-existing-pipelines.adoc[leveloffset=+2]

// == Managing pipeline experiments (To be written for 2.11)

//include::modules/overview-of-pipeline-experiments.adoc[leveloffset=+2] 

//include::modules/creating-a-pipeline-experiment.adoc[leveloffset=+2] 

//include::modules/updating-a-pipeline-experiment.adoc[leveloffset=+2] 

//include::modules/deleting-a-pipeline-experiment.adoc[leveloffset=+2] 

== Managing pipeline runs

include::modules/overview-of-pipeline-runs.adoc[leveloffset=+2]

include::modules/viewing-active-pipeline-runs.adoc[leveloffset=+2]

include::modules/executing-a-pipeline-run.adoc[leveloffset=+2]

include::modules/stopping-an-active-pipeline-run.adoc[leveloffset=+2]

// include::modules/deleting-an-active-pipeline-run.adoc[leveloffset=+2]

include::modules/duplicating-an-active-pipeline-run.adoc[leveloffset=+2]

include::modules/viewing-scheduled-pipeline-runs.adoc[leveloffset=+2]

include::modules/scheduling-a-pipeline-run-using-a-cron-job.adoc[leveloffset=+2]

include::modules/scheduling-a-pipeline-run.adoc[leveloffset=+2]

include::modules/duplicating-a-scheduled-pipeline-run.adoc[leveloffset=+2]

include::modules/deleting-a-scheduled-pipeline-run.adoc[leveloffset=+2]

include::modules/viewing-the-details-of-a-pipeline-run.adoc[leveloffset=+2]

include::modules/viewing-archived-pipeline-runs.adoc[leveloffset=+2]

include::modules/archiving-a-pipeline-run.adoc[leveloffset=+2]

include::modules/restoring-an-archived-pipeline-run.adoc[leveloffset=+2]

include::modules/deleting-an-archived-pipeline-run.adoc[leveloffset=+2]

include::modules/duplicating-an-archived-pipeline-run.adoc[leveloffset=+2]

== Working with pipeline logs

include::modules/about-pipeline-logs.adoc[leveloffset=+2]

include::modules/viewing-pipeline-step-logs.adoc[leveloffset=+2]

include::modules/downloading-pipeline-step-logs.adoc[leveloffset=+2]


== Working with pipelines in JupyterLab

include::modules/overview-of-pipelines-in-jupyterlab.adoc[leveloffset=+2]

include::modules/accessing-the-pipeline-editor.adoc[leveloffset=+2]

include::modules/creating-a-runtime-configuration.adoc[leveloffset=+2]

include::modules/updating-a-runtime-configuration.adoc[leveloffset=+2]

include::modules/deleting-a-runtime-configuration.adoc[leveloffset=+2]

include::modules/duplicating-a-runtime-configuration.adoc[leveloffset=+2]

//include::modules/defining-pipeline-parameters-in-jupyterlab.adoc[leveloffset=+2] 

include::modules/running-a-pipeline-in-jupyterlab.adoc[leveloffset=+2]

include::modules/exporting-a-pipeline-in-jupyterlab.adoc[leveloffset=+2]


[role='_additional-resources']
== Additional resources
* link:https://pypi.org/project/kfp/[KubeFlow Pipelines SDK]
* link:https://www.kubeflow.org/docs/components/pipelines/v2/[Kubeflow Pipelines 2.0 Documentation]
ifndef::upstream[]
* link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects/working-with-data-science-pipelines_ds-pipelines#working_with_pipelines_in_jupyterlab[Working with pipelines in JupyterLab].
endif::[]
ifdef::upstream[]
link:{odhdocshome}/working-on-data-science-projects/working-with-data-science-pipelines_ds-pipelines#working_with_pipelines_in_jupyterlab[Working with pipelines in JupyterLab].
endif::[]

ifdef::parent-context[:context: {parent-context}]
ifndef::parent-context[:!context:]
