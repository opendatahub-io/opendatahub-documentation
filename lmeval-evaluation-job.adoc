:_module-type: REFERENCE

ifdef::context[:parent-context: {context}]
[id="lmeval-evaluation-job.adoc_{context}"]
= LM-Eval evaluation job

[role='_abstract']
LM-Eval service defines a new Custom Resource Definition (CRD) called `LMEvalJob`. An `LMEvalJob` object represents an evaluation job. `LMEvalJob` objects are monitored by the TrustyAI Kubernetes operator.

To run an evaluation job, you create an `LMEvalJob` object with the following information: `model`, `model arguments`, `task`, and `secret`. 

After the `LMEvalJob` is created, the LM-Eval service runs the evaluation job.  The status and results of the `LMEvalJob` object update when the information is available.

[NOTE]
--
TrustyAI does not support non-tabular models. Deploying the TrustyAI custom resource (CR) in a namespace that contains non-tabular models can cause errors within the TrustyAI service.
--

.Sample LMEvalJob object 

The sample `LMEvalJob` object contains the following features: 

* The `google/flan-t5-base` model from link:https://huggingface.co/google/flan-t5-base[Hugging Face]. 

* The dataset from the `wnli` subset of the link:https://huggingface.co/datasets/nyu-mll/glue[General Language Understanding Evaluation (GLUE)]. For more information about the `wnli` `TaskCard`, see link:https://www.unitxt.ai/en/latest/catalog/catalog.cards.wnli.html[Unitxt website].

* The following default parameters for the link:https://www.unitxt.ai/en/latest/catalog/catalog.tasks.classification.multi_class.relation.html[multi_class.relation template] Unitxt task:`f1_micro`, `f1_macro`, and `accuracy`.

[source]
----

apiVersion: trustyai.opendatahub.io/v1alpha1
kind: LMEvalJob
metadata:
  name: evaljob-sample
spec:
  model: hf
  modelArgs:
  - name: pretrained
    value: google/flan-t5-base 
  taskList:
    taskRecipes:
    - card:
        name: "cards.wnli" 
      template: "templates.classification.multi_class.relation.default" 
  logSamples: true

----

After you apply the sample `LMEvalJob`, check its state by using the following command:

[source]
----
oc get lmevaljob evaljob-sample
----
Output similar to the following appears:
NAME `evaljob-sample`
STATE: `Running`

Evaluation results are available when the object's state changes to `Complete`. Both the model and dataset in this example are small. The evaluation job should finish within 10 minutes on a CPU-only node.

Use the following command to get the results:

[source]
----
oc get lmevaljobs.trustyai.opendatahub.io evaljob-sample \
  -o template --template={{.status.results}} | jq '.results'
----

Output similar to the following appears:

[source]
----
{
  "tr_0": {
    "alias": "tr_0",
    "f1_micro,none": 0.5633802816901409,
    "f1_micro_stderr,none": "N/A",
    "accuracy,none": 0.5633802816901409,
    "accuracy_stderr,none": "N/A",
    "f1_macro,none": 0.36036036036036034,
    "f1_macro_stderr,none": "N/A"
  }
}
----

.Notes on the results

* The `f1_micro`, `f1_macro`, and `accuracy` scores are 0.56, 0.36, and 0.56. 
* The full results are stored in the `.status.results` of the `LMEvalJob` object as a JSON document. The command above only retrieves the results field of the JSON document.


.LMEvalJob properties

The following table lists each property in the `LMEvalJob` and its usage:

.LM-Eval properties
[cols="2,5"]
|===
| Parameter | Description

| `model`
| Specifies which model type or provider is evaluated. This field directly maps to the `--model` argument of the `lm-evaluation-harness`. Supported model types and providers include:

* `hf`: HuggingFace models

* `openai-completions`: OpenAI Completions API models

* `openai-chat-completions`: link:https://platform.openai.com/docs/guides/text-generation[Chat Completions API models]

* `local-completions` and `local-chat-completions`: OpenAI API-compatible servers

* `textsynth`: link:https://textsynth.com/documentation.html#engines[TextSynth APIs]

| `modelArgs`
| A list of paired name and value arguments for the model type. Each model type or provider supports different arguments:

* `hf` (HuggingFace): Check the link:https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/models/huggingface.py#L55[huggingface.py]

* `local-completions` (OpenAI API-compatible server): Check the link:https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/models/openai_completions.py#L13[openai_completions.py] and link:https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/models/api_models.py#L55[tapi_models.py]

* `local-chat-completions` (OpenAI API-compatible server): Check link:https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/models/openai_completions.py#L99[openai_completions.py] and link:https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/models/api_models.py#L55[tapi_models.py]

* openai-completions (OpenAI Completions API models): Check link:https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/models/openai_completions.py#L177[openai_completions.py] and link:https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/models/api_models.py#L55[tapi_models.py]

* openai-chat-completions (ChatCompletions API models): Check link:https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/models/openai_completions.py#L209[openai_completions.py] and link:https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/models/api_models.py#L55[tapi_models.py]

textsynth (TextSynth APIs): Check link:https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/models/textsynth.py#L52[textsynth.py]

| `taskList.taskNames`
| Specifies a list of tasks supported by `lm-evaluation-harness`.

| `taskList.taskRecipes`
| Specifies the task using the Unitxt recipe format:
* `card`: Use the `name` to specify a Unitxt card or `custom` for a custom card
** `name`: Specifies a Unitxt card from the link:https://www.unitxt.ai/en/latest/catalog/catalog.__dir__.html[Unitxt catalog]. Use the card's ID as the value. For example, the ID of link:https://www.unitxt.ai/en/latest/catalog/catalog.cards.wnli.html[Wnli card] is `cards.wnli`.
** `custom`: Define a custom card and use it. The value is a JSON string for a custom Unitxt card which contains the custom dataset. Use the link:https://www.unitxt.ai/en/latest/docs/adding_dataset.html#adding-to-the-catalog[Unitxt documentation] to compose a custom card, store it as a JSON file, and use the JSON content as the value here. If the dataset used by the custom card needs an API key from an environment variable or a persistent volume, you have to set up corresponding resources under the `pod` field. Check the `pod` field below.
* `template`: Specifies a Unitxt template from the link:https://www.unitxt.ai/en/latest/catalog/catalog.__dir__.html[Unitxt catalog]. Use the template's ID as the value.
* `task` (optional): Specifies a Unitxt task from the link:https://www.unitxt.ai/en/latest/catalog/catalog.__dir__.html[Unitxt catalog]. Use the task's ID as the value. A Unitxt card has a predefined task. Only specify a value for this if you want to run a different task.
* `metrics` (optional):  Specifies a Unitxt task from the link:https://www.unitxt.ai/en/latest/catalog/catalog.__dir__.html[Unitxt catalog]. Use the metric's ID as the value. A Unitxt task has a set of pre-defined metrics. Only specify a set of metrics if you need different metrics.
* `format` (optional): Specifies a Unitxt format from the link:https://www.unitxt.ai/en/latest/catalog/catalog.__dir__.html[Unitxt catalog]. Use the format's ID as the value.
* `loaderLimit` (optional): Specifies the maximum number of instances per stream to be returned from the loader (used to reduce loading time in large datasets).
* `numDemos` (optional): Number of fewshot to be used.
* `demosPoolSize` (optional): Size of the fewshot pool.


| `numFewShot`
| Sets the number of few-shot examples to place in context. If you are using a task from Unitxt, don't use this field. Use `numDemos` under the `taskRecipes` instead.

| `limit`
| Instead of running the whole dataset, set a limit to run the tasks. Accepts an integer, or a float between 0.0 and 1.0.

| `genArgs`
| Map to `--gen_kwargs` parameter for the lm-evaluation-harness. Read more in the link:https://github.com/EleutherAI/lm-evaluation-harness/blob/main/docs/interface.md#command-line-interface[lm-evaluation-harness documentation].

| `logSamples`
| If this flag is passed, then the model's outputs, and the text fed into the model, will be saved at per-document granularity.

| `batchSize`
| Batch size for the evaluation. The auto:`N` batch size is not used for API models, but numeric batch sizes are used for APIs. Only int batch size supported at the moment.

| `pod`
| Specify extra information for the lm-eval job's pod. 
* `container`: Extra container settings for the lm-eval container.
** `env`: Specify environment variables. It uses the `EnvVar` data structure of kubernetes.
** `volumeMounts`: Mount the volumes into the lm-eval container
** `resources`: Specify the resources for the lm-eval container.
* `volumes`: Specify the volume information for the lm-eval and other containers. It uses the `Volume`  data structure of kubernetes.
* `sideCars`: A list of containers that run along with the lm-eval container. It uses the `Container` data structure of kubernetes.


| `outputs`
| This section defines custom output locations for the evaluation results storage. At the moment only Persistent Volume Claims (PVC) are supported.

| `outputs.pvcManaged`
| Create an operator-managed PVC to store this job's results. The PVC will be named `<job-name>-pvc` and will be owned by the `LMEvalJob`. After job completion, the PVC will still be available, but it will be deleted upon deleting the `LMEvalJob`. Supports the following fields:
* `size`: The PVC's size, compatible with standard PVC syntax (e.g. 5Gi)

| `outputs.pvcName`
| Binds an existing PVC to a job by specifying its name. The PVC must be created separately and must already exist when creating the job.

|===



