<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 2.0.20">
<title>Monitoring data science models</title>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700">
<style>
/*! Asciidoctor default stylesheet | MIT License | https://asciidoctor.org */
/* Uncomment the following line when using as a custom stylesheet */
/* @import "https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700"; */
html{font-family:sans-serif;-webkit-text-size-adjust:100%}
a{background:none}
a:focus{outline:thin dotted}
a:active,a:hover{outline:0}
h1{font-size:2em;margin:.67em 0}
b,strong{font-weight:bold}
abbr{font-size:.9em}
abbr[title]{cursor:help;border-bottom:1px dotted #dddddf;text-decoration:none}
dfn{font-style:italic}
hr{height:0}
mark{background:#ff0;color:#000}
code,kbd,pre,samp{font-family:monospace;font-size:1em}
pre{white-space:pre-wrap}
q{quotes:"\201C" "\201D" "\2018" "\2019"}
small{font-size:80%}
sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}
sup{top:-.5em}
sub{bottom:-.25em}
img{border:0}
svg:not(:root){overflow:hidden}
figure{margin:0}
audio,video{display:inline-block}
audio:not([controls]){display:none;height:0}
fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}
legend{border:0;padding:0}
button,input,select,textarea{font-family:inherit;font-size:100%;margin:0}
button,input{line-height:normal}
button,select{text-transform:none}
button,html input[type=button],input[type=reset],input[type=submit]{-webkit-appearance:button;cursor:pointer}
button[disabled],html input[disabled]{cursor:default}
input[type=checkbox],input[type=radio]{padding:0}
button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}
textarea{overflow:auto;vertical-align:top}
table{border-collapse:collapse;border-spacing:0}
*,::before,::after{box-sizing:border-box}
html,body{font-size:100%}
body{background:#fff;color:rgba(0,0,0,.8);padding:0;margin:0;font-family:"Noto Serif","DejaVu Serif",serif;line-height:1;position:relative;cursor:auto;-moz-tab-size:4;-o-tab-size:4;tab-size:4;word-wrap:anywhere;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}
a:hover{cursor:pointer}
img,object,embed{max-width:100%;height:auto}
object,embed{height:100%}
img{-ms-interpolation-mode:bicubic}
.left{float:left!important}
.right{float:right!important}
.text-left{text-align:left!important}
.text-right{text-align:right!important}
.text-center{text-align:center!important}
.text-justify{text-align:justify!important}
.hide{display:none}
img,object,svg{display:inline-block;vertical-align:middle}
textarea{height:auto;min-height:50px}
select{width:100%}
.subheader,.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{line-height:1.45;color:#7a2518;font-weight:400;margin-top:0;margin-bottom:.25em}
div,dl,dt,dd,ul,ol,li,h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6,pre,form,p,blockquote,th,td{margin:0;padding:0}
a{color:#2156a5;text-decoration:underline;line-height:inherit}
a:hover,a:focus{color:#1d4b8f}
a img{border:0}
p{line-height:1.6;margin-bottom:1.25em;text-rendering:optimizeLegibility}
p aside{font-size:.875em;line-height:1.35;font-style:italic}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{font-family:"Open Sans","DejaVu Sans",sans-serif;font-weight:300;font-style:normal;color:#ba3925;text-rendering:optimizeLegibility;margin-top:1em;margin-bottom:.5em;line-height:1.0125em}
h1 small,h2 small,h3 small,#toctitle small,.sidebarblock>.content>.title small,h4 small,h5 small,h6 small{font-size:60%;color:#e99b8f;line-height:0}
h1{font-size:2.125em}
h2{font-size:1.6875em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.375em}
h4,h5{font-size:1.125em}
h6{font-size:1em}
hr{border:solid #dddddf;border-width:1px 0 0;clear:both;margin:1.25em 0 1.1875em}
em,i{font-style:italic;line-height:inherit}
strong,b{font-weight:bold;line-height:inherit}
small{font-size:60%;line-height:inherit}
code{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;color:rgba(0,0,0,.9)}
ul,ol,dl{line-height:1.6;margin-bottom:1.25em;list-style-position:outside;font-family:inherit}
ul,ol{margin-left:1.5em}
ul li ul,ul li ol{margin-left:1.25em;margin-bottom:0}
ul.circle{list-style-type:circle}
ul.disc{list-style-type:disc}
ul.square{list-style-type:square}
ul.circle ul:not([class]),ul.disc ul:not([class]),ul.square ul:not([class]){list-style:inherit}
ol li ul,ol li ol{margin-left:1.25em;margin-bottom:0}
dl dt{margin-bottom:.3125em;font-weight:bold}
dl dd{margin-bottom:1.25em}
blockquote{margin:0 0 1.25em;padding:.5625em 1.25em 0 1.1875em;border-left:1px solid #ddd}
blockquote,blockquote p{line-height:1.6;color:rgba(0,0,0,.85)}
@media screen and (min-width:768px){h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2}
h1{font-size:2.75em}
h2{font-size:2.3125em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.6875em}
h4{font-size:1.4375em}}
table{background:#fff;margin-bottom:1.25em;border:1px solid #dedede;word-wrap:normal}
table thead,table tfoot{background:#f7f8f7}
table thead tr th,table thead tr td,table tfoot tr th,table tfoot tr td{padding:.5em .625em .625em;font-size:inherit;color:rgba(0,0,0,.8);text-align:left}
table tr th,table tr td{padding:.5625em .625em;font-size:inherit;color:rgba(0,0,0,.8)}
table tr.even,table tr.alt{background:#f8f8f7}
table thead tr th,table tfoot tr th,table tbody tr td,table tr td,table tfoot tr td{line-height:1.6}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2;word-spacing:-.05em}
h1 strong,h2 strong,h3 strong,#toctitle strong,.sidebarblock>.content>.title strong,h4 strong,h5 strong,h6 strong{font-weight:400}
.center{margin-left:auto;margin-right:auto}
.stretch{width:100%}
.clearfix::before,.clearfix::after,.float-group::before,.float-group::after{content:" ";display:table}
.clearfix::after,.float-group::after{clear:both}
:not(pre).nobreak{word-wrap:normal}
:not(pre).nowrap{white-space:nowrap}
:not(pre).pre-wrap{white-space:pre-wrap}
:not(pre):not([class^=L])>code{font-size:.9375em;font-style:normal!important;letter-spacing:0;padding:.1em .5ex;word-spacing:-.15em;background:#f7f7f8;border-radius:4px;line-height:1.45;text-rendering:optimizeSpeed}
pre{color:rgba(0,0,0,.9);font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;line-height:1.45;text-rendering:optimizeSpeed}
pre code,pre pre{color:inherit;font-size:inherit;line-height:inherit}
pre>code{display:block}
pre.nowrap,pre.nowrap pre{white-space:pre;word-wrap:normal}
em em{font-style:normal}
strong strong{font-weight:400}
.keyseq{color:rgba(51,51,51,.8)}
kbd{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;display:inline-block;color:rgba(0,0,0,.8);font-size:.65em;line-height:1.45;background:#f7f7f7;border:1px solid #ccc;border-radius:3px;box-shadow:0 1px 0 rgba(0,0,0,.2),inset 0 0 0 .1em #fff;margin:0 .15em;padding:.2em .5em;vertical-align:middle;position:relative;top:-.1em;white-space:nowrap}
.keyseq kbd:first-child{margin-left:0}
.keyseq kbd:last-child{margin-right:0}
.menuseq,.menuref{color:#000}
.menuseq b:not(.caret),.menuref{font-weight:inherit}
.menuseq{word-spacing:-.02em}
.menuseq b.caret{font-size:1.25em;line-height:.8}
.menuseq i.caret{font-weight:bold;text-align:center;width:.45em}
b.button::before,b.button::after{position:relative;top:-1px;font-weight:400}
b.button::before{content:"[";padding:0 3px 0 2px}
b.button::after{content:"]";padding:0 2px 0 3px}
p a>code:hover{color:rgba(0,0,0,.9)}
#header,#content,#footnotes,#footer{width:100%;margin:0 auto;max-width:62.5em;*zoom:1;position:relative;padding-left:.9375em;padding-right:.9375em}
#header::before,#header::after,#content::before,#content::after,#footnotes::before,#footnotes::after,#footer::before,#footer::after{content:" ";display:table}
#header::after,#content::after,#footnotes::after,#footer::after{clear:both}
#content{margin-top:1.25em}
#content::before{content:none}
#header>h1:first-child{color:rgba(0,0,0,.85);margin-top:2.25rem;margin-bottom:0}
#header>h1:first-child+#toc{margin-top:8px;border-top:1px solid #dddddf}
#header>h1:only-child,body.toc2 #header>h1:nth-last-child(2){border-bottom:1px solid #dddddf;padding-bottom:8px}
#header .details{border-bottom:1px solid #dddddf;line-height:1.45;padding-top:.25em;padding-bottom:.25em;padding-left:.25em;color:rgba(0,0,0,.6);display:flex;flex-flow:row wrap}
#header .details span:first-child{margin-left:-.125em}
#header .details span.email a{color:rgba(0,0,0,.85)}
#header .details br{display:none}
#header .details br+span::before{content:"\00a0\2013\00a0"}
#header .details br+span.author::before{content:"\00a0\22c5\00a0";color:rgba(0,0,0,.85)}
#header .details br+span#revremark::before{content:"\00a0|\00a0"}
#header #revnumber{text-transform:capitalize}
#header #revnumber::after{content:"\00a0"}
#content>h1:first-child:not([class]){color:rgba(0,0,0,.85);border-bottom:1px solid #dddddf;padding-bottom:8px;margin-top:0;padding-top:1rem;margin-bottom:1.25rem}
#toc{border-bottom:1px solid #e7e7e9;padding-bottom:.5em}
#toc>ul{margin-left:.125em}
#toc ul.sectlevel0>li>a{font-style:italic}
#toc ul.sectlevel0 ul.sectlevel1{margin:.5em 0}
#toc ul{font-family:"Open Sans","DejaVu Sans",sans-serif;list-style-type:none}
#toc li{line-height:1.3334;margin-top:.3334em}
#toc a{text-decoration:none}
#toc a:active{text-decoration:underline}
#toctitle{color:#7a2518;font-size:1.2em}
@media screen and (min-width:768px){#toctitle{font-size:1.375em}
body.toc2{padding-left:15em;padding-right:0}
#toc.toc2{margin-top:0!important;background:#f8f8f7;position:fixed;width:15em;left:0;top:0;border-right:1px solid #e7e7e9;border-top-width:0!important;border-bottom-width:0!important;z-index:1000;padding:1.25em 1em;height:100%;overflow:auto}
#toc.toc2 #toctitle{margin-top:0;margin-bottom:.8rem;font-size:1.2em}
#toc.toc2>ul{font-size:.9em;margin-bottom:0}
#toc.toc2 ul ul{margin-left:0;padding-left:1em}
#toc.toc2 ul.sectlevel0 ul.sectlevel1{padding-left:0;margin-top:.5em;margin-bottom:.5em}
body.toc2.toc-right{padding-left:0;padding-right:15em}
body.toc2.toc-right #toc.toc2{border-right-width:0;border-left:1px solid #e7e7e9;left:auto;right:0}}
@media screen and (min-width:1280px){body.toc2{padding-left:20em;padding-right:0}
#toc.toc2{width:20em}
#toc.toc2 #toctitle{font-size:1.375em}
#toc.toc2>ul{font-size:.95em}
#toc.toc2 ul ul{padding-left:1.25em}
body.toc2.toc-right{padding-left:0;padding-right:20em}}
#content #toc{border:1px solid #e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;border-radius:4px}
#content #toc>:first-child{margin-top:0}
#content #toc>:last-child{margin-bottom:0}
#footer{max-width:none;background:rgba(0,0,0,.8);padding:1.25em}
#footer-text{color:hsla(0,0%,100%,.8);line-height:1.44}
#content{margin-bottom:.625em}
.sect1{padding-bottom:.625em}
@media screen and (min-width:768px){#content{margin-bottom:1.25em}
.sect1{padding-bottom:1.25em}}
.sect1:last-child{padding-bottom:0}
.sect1+.sect1{border-top:1px solid #e7e7e9}
#content h1>a.anchor,h2>a.anchor,h3>a.anchor,#toctitle>a.anchor,.sidebarblock>.content>.title>a.anchor,h4>a.anchor,h5>a.anchor,h6>a.anchor{position:absolute;z-index:1001;width:1.5ex;margin-left:-1.5ex;display:block;text-decoration:none!important;visibility:hidden;text-align:center;font-weight:400}
#content h1>a.anchor::before,h2>a.anchor::before,h3>a.anchor::before,#toctitle>a.anchor::before,.sidebarblock>.content>.title>a.anchor::before,h4>a.anchor::before,h5>a.anchor::before,h6>a.anchor::before{content:"\00A7";font-size:.85em;display:block;padding-top:.1em}
#content h1:hover>a.anchor,#content h1>a.anchor:hover,h2:hover>a.anchor,h2>a.anchor:hover,h3:hover>a.anchor,#toctitle:hover>a.anchor,.sidebarblock>.content>.title:hover>a.anchor,h3>a.anchor:hover,#toctitle>a.anchor:hover,.sidebarblock>.content>.title>a.anchor:hover,h4:hover>a.anchor,h4>a.anchor:hover,h5:hover>a.anchor,h5>a.anchor:hover,h6:hover>a.anchor,h6>a.anchor:hover{visibility:visible}
#content h1>a.link,h2>a.link,h3>a.link,#toctitle>a.link,.sidebarblock>.content>.title>a.link,h4>a.link,h5>a.link,h6>a.link{color:#ba3925;text-decoration:none}
#content h1>a.link:hover,h2>a.link:hover,h3>a.link:hover,#toctitle>a.link:hover,.sidebarblock>.content>.title>a.link:hover,h4>a.link:hover,h5>a.link:hover,h6>a.link:hover{color:#a53221}
details,.audioblock,.imageblock,.literalblock,.listingblock,.stemblock,.videoblock{margin-bottom:1.25em}
details{margin-left:1.25rem}
details>summary{cursor:pointer;display:block;position:relative;line-height:1.6;margin-bottom:.625rem;outline:none;-webkit-tap-highlight-color:transparent}
details>summary::-webkit-details-marker{display:none}
details>summary::before{content:"";border:solid transparent;border-left:solid;border-width:.3em 0 .3em .5em;position:absolute;top:.5em;left:-1.25rem;transform:translateX(15%)}
details[open]>summary::before{border:solid transparent;border-top:solid;border-width:.5em .3em 0;transform:translateY(15%)}
details>summary::after{content:"";width:1.25rem;height:1em;position:absolute;top:.3em;left:-1.25rem}
.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{text-rendering:optimizeLegibility;text-align:left;font-family:"Noto Serif","DejaVu Serif",serif;font-size:1rem;font-style:italic}
table.tableblock.fit-content>caption.title{white-space:nowrap;width:0}
.paragraph.lead>p,#preamble>.sectionbody>[class=paragraph]:first-of-type p{font-size:1.21875em;line-height:1.6;color:rgba(0,0,0,.85)}
.admonitionblock>table{border-collapse:separate;border:0;background:none;width:100%}
.admonitionblock>table td.icon{text-align:center;width:80px}
.admonitionblock>table td.icon img{max-width:none}
.admonitionblock>table td.icon .title{font-weight:bold;font-family:"Open Sans","DejaVu Sans",sans-serif;text-transform:uppercase}
.admonitionblock>table td.content{padding-left:1.125em;padding-right:1.25em;border-left:1px solid #dddddf;color:rgba(0,0,0,.6);word-wrap:anywhere}
.admonitionblock>table td.content>:last-child>:last-child{margin-bottom:0}
.exampleblock>.content{border:1px solid #e6e6e6;margin-bottom:1.25em;padding:1.25em;background:#fff;border-radius:4px}
.sidebarblock{border:1px solid #dbdbd6;margin-bottom:1.25em;padding:1.25em;background:#f3f3f2;border-radius:4px}
.sidebarblock>.content>.title{color:#7a2518;margin-top:0;text-align:center}
.exampleblock>.content>:first-child,.sidebarblock>.content>:first-child{margin-top:0}
.exampleblock>.content>:last-child,.exampleblock>.content>:last-child>:last-child,.exampleblock>.content .olist>ol>li:last-child>:last-child,.exampleblock>.content .ulist>ul>li:last-child>:last-child,.exampleblock>.content .qlist>ol>li:last-child>:last-child,.sidebarblock>.content>:last-child,.sidebarblock>.content>:last-child>:last-child,.sidebarblock>.content .olist>ol>li:last-child>:last-child,.sidebarblock>.content .ulist>ul>li:last-child>:last-child,.sidebarblock>.content .qlist>ol>li:last-child>:last-child{margin-bottom:0}
.literalblock pre,.listingblock>.content>pre{border-radius:4px;overflow-x:auto;padding:1em;font-size:.8125em}
@media screen and (min-width:768px){.literalblock pre,.listingblock>.content>pre{font-size:.90625em}}
@media screen and (min-width:1280px){.literalblock pre,.listingblock>.content>pre{font-size:1em}}
.literalblock pre,.listingblock>.content>pre:not(.highlight),.listingblock>.content>pre[class=highlight],.listingblock>.content>pre[class^="highlight "]{background:#f7f7f8}
.literalblock.output pre{color:#f7f7f8;background:rgba(0,0,0,.9)}
.listingblock>.content{position:relative}
.listingblock code[data-lang]::before{display:none;content:attr(data-lang);position:absolute;font-size:.75em;top:.425rem;right:.5rem;line-height:1;text-transform:uppercase;color:inherit;opacity:.5}
.listingblock:hover code[data-lang]::before{display:block}
.listingblock.terminal pre .command::before{content:attr(data-prompt);padding-right:.5em;color:inherit;opacity:.5}
.listingblock.terminal pre .command:not([data-prompt])::before{content:"$"}
.listingblock pre.highlightjs{padding:0}
.listingblock pre.highlightjs>code{padding:1em;border-radius:4px}
.listingblock pre.prettyprint{border-width:0}
.prettyprint{background:#f7f7f8}
pre.prettyprint .linenums{line-height:1.45;margin-left:2em}
pre.prettyprint li{background:none;list-style-type:inherit;padding-left:0}
pre.prettyprint li code[data-lang]::before{opacity:1}
pre.prettyprint li:not(:first-child) code[data-lang]::before{display:none}
table.linenotable{border-collapse:separate;border:0;margin-bottom:0;background:none}
table.linenotable td[class]{color:inherit;vertical-align:top;padding:0;line-height:inherit;white-space:normal}
table.linenotable td.code{padding-left:.75em}
table.linenotable td.linenos,pre.pygments .linenos{border-right:1px solid;opacity:.35;padding-right:.5em;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}
pre.pygments span.linenos{display:inline-block;margin-right:.75em}
.quoteblock{margin:0 1em 1.25em 1.5em;display:table}
.quoteblock:not(.excerpt)>.title{margin-left:-1.5em;margin-bottom:.75em}
.quoteblock blockquote,.quoteblock p{color:rgba(0,0,0,.85);font-size:1.15rem;line-height:1.75;word-spacing:.1em;letter-spacing:0;font-style:italic;text-align:justify}
.quoteblock blockquote{margin:0;padding:0;border:0}
.quoteblock blockquote::before{content:"\201c";float:left;font-size:2.75em;font-weight:bold;line-height:.6em;margin-left:-.6em;color:#7a2518;text-shadow:0 1px 2px rgba(0,0,0,.1)}
.quoteblock blockquote>.paragraph:last-child p{margin-bottom:0}
.quoteblock .attribution{margin-top:.75em;margin-right:.5ex;text-align:right}
.verseblock{margin:0 1em 1.25em}
.verseblock pre{font-family:"Open Sans","DejaVu Sans",sans-serif;font-size:1.15rem;color:rgba(0,0,0,.85);font-weight:300;text-rendering:optimizeLegibility}
.verseblock pre strong{font-weight:400}
.verseblock .attribution{margin-top:1.25rem;margin-left:.5ex}
.quoteblock .attribution,.verseblock .attribution{font-size:.9375em;line-height:1.45;font-style:italic}
.quoteblock .attribution br,.verseblock .attribution br{display:none}
.quoteblock .attribution cite,.verseblock .attribution cite{display:block;letter-spacing:-.025em;color:rgba(0,0,0,.6)}
.quoteblock.abstract blockquote::before,.quoteblock.excerpt blockquote::before,.quoteblock .quoteblock blockquote::before{display:none}
.quoteblock.abstract blockquote,.quoteblock.abstract p,.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{line-height:1.6;word-spacing:0}
.quoteblock.abstract{margin:0 1em 1.25em;display:block}
.quoteblock.abstract>.title{margin:0 0 .375em;font-size:1.15em;text-align:center}
.quoteblock.excerpt>blockquote,.quoteblock .quoteblock{padding:0 0 .25em 1em;border-left:.25em solid #dddddf}
.quoteblock.excerpt,.quoteblock .quoteblock{margin-left:0}
.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{color:inherit;font-size:1.0625rem}
.quoteblock.excerpt .attribution,.quoteblock .quoteblock .attribution{color:inherit;font-size:.85rem;text-align:left;margin-right:0}
p.tableblock:last-child{margin-bottom:0}
td.tableblock>.content{margin-bottom:1.25em;word-wrap:anywhere}
td.tableblock>.content>:last-child{margin-bottom:-1.25em}
table.tableblock,th.tableblock,td.tableblock{border:0 solid #dedede}
table.grid-all>*>tr>*{border-width:1px}
table.grid-cols>*>tr>*{border-width:0 1px}
table.grid-rows>*>tr>*{border-width:1px 0}
table.frame-all{border-width:1px}
table.frame-ends{border-width:1px 0}
table.frame-sides{border-width:0 1px}
table.frame-none>colgroup+*>:first-child>*,table.frame-sides>colgroup+*>:first-child>*{border-top-width:0}
table.frame-none>:last-child>:last-child>*,table.frame-sides>:last-child>:last-child>*{border-bottom-width:0}
table.frame-none>*>tr>:first-child,table.frame-ends>*>tr>:first-child{border-left-width:0}
table.frame-none>*>tr>:last-child,table.frame-ends>*>tr>:last-child{border-right-width:0}
table.stripes-all>*>tr,table.stripes-odd>*>tr:nth-of-type(odd),table.stripes-even>*>tr:nth-of-type(even),table.stripes-hover>*>tr:hover{background:#f8f8f7}
th.halign-left,td.halign-left{text-align:left}
th.halign-right,td.halign-right{text-align:right}
th.halign-center,td.halign-center{text-align:center}
th.valign-top,td.valign-top{vertical-align:top}
th.valign-bottom,td.valign-bottom{vertical-align:bottom}
th.valign-middle,td.valign-middle{vertical-align:middle}
table thead th,table tfoot th{font-weight:bold}
tbody tr th{background:#f7f8f7}
tbody tr th,tbody tr th p,tfoot tr th,tfoot tr th p{color:rgba(0,0,0,.8);font-weight:bold}
p.tableblock>code:only-child{background:none;padding:0}
p.tableblock{font-size:1em}
ol{margin-left:1.75em}
ul li ol{margin-left:1.5em}
dl dd{margin-left:1.125em}
dl dd:last-child,dl dd:last-child>:last-child{margin-bottom:0}
li p,ul dd,ol dd,.olist .olist,.ulist .ulist,.ulist .olist,.olist .ulist{margin-bottom:.625em}
ul.checklist,ul.none,ol.none,ul.no-bullet,ol.no-bullet,ol.unnumbered,ul.unstyled,ol.unstyled{list-style-type:none}
ul.no-bullet,ol.no-bullet,ol.unnumbered{margin-left:.625em}
ul.unstyled,ol.unstyled{margin-left:0}
li>p:empty:only-child::before{content:"";display:inline-block}
ul.checklist>li>p:first-child{margin-left:-1em}
ul.checklist>li>p:first-child>.fa-square-o:first-child,ul.checklist>li>p:first-child>.fa-check-square-o:first-child{width:1.25em;font-size:.8em;position:relative;bottom:.125em}
ul.checklist>li>p:first-child>input[type=checkbox]:first-child{margin-right:.25em}
ul.inline{display:flex;flex-flow:row wrap;list-style:none;margin:0 0 .625em -1.25em}
ul.inline>li{margin-left:1.25em}
.unstyled dl dt{font-weight:400;font-style:normal}
ol.arabic{list-style-type:decimal}
ol.decimal{list-style-type:decimal-leading-zero}
ol.loweralpha{list-style-type:lower-alpha}
ol.upperalpha{list-style-type:upper-alpha}
ol.lowerroman{list-style-type:lower-roman}
ol.upperroman{list-style-type:upper-roman}
ol.lowergreek{list-style-type:lower-greek}
.hdlist>table,.colist>table{border:0;background:none}
.hdlist>table>tbody>tr,.colist>table>tbody>tr{background:none}
td.hdlist1,td.hdlist2{vertical-align:top;padding:0 .625em}
td.hdlist1{font-weight:bold;padding-bottom:1.25em}
td.hdlist2{word-wrap:anywhere}
.literalblock+.colist,.listingblock+.colist{margin-top:-.5em}
.colist td:not([class]):first-child{padding:.4em .75em 0;line-height:1;vertical-align:top}
.colist td:not([class]):first-child img{max-width:none}
.colist td:not([class]):last-child{padding:.25em 0}
.thumb,.th{line-height:0;display:inline-block;border:4px solid #fff;box-shadow:0 0 0 1px #ddd}
.imageblock.left{margin:.25em .625em 1.25em 0}
.imageblock.right{margin:.25em 0 1.25em .625em}
.imageblock>.title{margin-bottom:0}
.imageblock.thumb,.imageblock.th{border-width:6px}
.imageblock.thumb>.title,.imageblock.th>.title{padding:0 .125em}
.image.left,.image.right{margin-top:.25em;margin-bottom:.25em;display:inline-block;line-height:0}
.image.left{margin-right:.625em}
.image.right{margin-left:.625em}
a.image{text-decoration:none;display:inline-block}
a.image object{pointer-events:none}
sup.footnote,sup.footnoteref{font-size:.875em;position:static;vertical-align:super}
sup.footnote a,sup.footnoteref a{text-decoration:none}
sup.footnote a:active,sup.footnoteref a:active{text-decoration:underline}
#footnotes{padding-top:.75em;padding-bottom:.75em;margin-bottom:.625em}
#footnotes hr{width:20%;min-width:6.25em;margin:-.25em 0 .75em;border-width:1px 0 0}
#footnotes .footnote{padding:0 .375em 0 .225em;line-height:1.3334;font-size:.875em;margin-left:1.2em;margin-bottom:.2em}
#footnotes .footnote a:first-of-type{font-weight:bold;text-decoration:none;margin-left:-1.05em}
#footnotes .footnote:last-of-type{margin-bottom:0}
#content #footnotes{margin-top:-.625em;margin-bottom:0;padding:.75em 0}
div.unbreakable{page-break-inside:avoid}
.big{font-size:larger}
.small{font-size:smaller}
.underline{text-decoration:underline}
.overline{text-decoration:overline}
.line-through{text-decoration:line-through}
.aqua{color:#00bfbf}
.aqua-background{background:#00fafa}
.black{color:#000}
.black-background{background:#000}
.blue{color:#0000bf}
.blue-background{background:#0000fa}
.fuchsia{color:#bf00bf}
.fuchsia-background{background:#fa00fa}
.gray{color:#606060}
.gray-background{background:#7d7d7d}
.green{color:#006000}
.green-background{background:#007d00}
.lime{color:#00bf00}
.lime-background{background:#00fa00}
.maroon{color:#600000}
.maroon-background{background:#7d0000}
.navy{color:#000060}
.navy-background{background:#00007d}
.olive{color:#606000}
.olive-background{background:#7d7d00}
.purple{color:#600060}
.purple-background{background:#7d007d}
.red{color:#bf0000}
.red-background{background:#fa0000}
.silver{color:#909090}
.silver-background{background:#bcbcbc}
.teal{color:#006060}
.teal-background{background:#007d7d}
.white{color:#bfbfbf}
.white-background{background:#fafafa}
.yellow{color:#bfbf00}
.yellow-background{background:#fafa00}
span.icon>.fa{cursor:default}
a span.icon>.fa{cursor:inherit}
.admonitionblock td.icon [class^="fa icon-"]{font-size:2.5em;text-shadow:1px 1px 2px rgba(0,0,0,.5);cursor:default}
.admonitionblock td.icon .icon-note::before{content:"\f05a";color:#19407c}
.admonitionblock td.icon .icon-tip::before{content:"\f0eb";text-shadow:1px 1px 2px rgba(155,155,0,.8);color:#111}
.admonitionblock td.icon .icon-warning::before{content:"\f071";color:#bf6900}
.admonitionblock td.icon .icon-caution::before{content:"\f06d";color:#bf3400}
.admonitionblock td.icon .icon-important::before{content:"\f06a";color:#bf0000}
.conum[data-value]{display:inline-block;color:#fff!important;background:rgba(0,0,0,.8);border-radius:50%;text-align:center;font-size:.75em;width:1.67em;height:1.67em;line-height:1.67em;font-family:"Open Sans","DejaVu Sans",sans-serif;font-style:normal;font-weight:bold}
.conum[data-value] *{color:#fff!important}
.conum[data-value]+b{display:none}
.conum[data-value]::after{content:attr(data-value)}
pre .conum[data-value]{position:relative;top:-.125em}
b.conum *{color:inherit!important}
.conum:not([data-value]):empty{display:none}
dt,th.tableblock,td.content,div.footnote{text-rendering:optimizeLegibility}
h1,h2,p,td.content,span.alt,summary{letter-spacing:-.01em}
p strong,td.content strong,div.footnote strong{letter-spacing:-.005em}
p,blockquote,dt,td.content,td.hdlist1,span.alt,summary{font-size:1.0625rem}
p{margin-bottom:1.25rem}
.sidebarblock p,.sidebarblock dt,.sidebarblock td.content,p.tableblock{font-size:1em}
.exampleblock>.content{background:#fffef7;border-color:#e0e0dc;box-shadow:0 1px 4px #e0e0dc}
.print-only{display:none!important}
@page{margin:1.25cm .75cm}
@media print{*{box-shadow:none!important;text-shadow:none!important}
html{font-size:80%}
a{color:inherit!important;text-decoration:underline!important}
a.bare,a[href^="#"],a[href^="mailto:"]{text-decoration:none!important}
a[href^="http:"]:not(.bare)::after,a[href^="https:"]:not(.bare)::after{content:"(" attr(href) ")";display:inline-block;font-size:.875em;padding-left:.25em}
abbr[title]{border-bottom:1px dotted}
abbr[title]::after{content:" (" attr(title) ")"}
pre,blockquote,tr,img,object,svg{page-break-inside:avoid}
thead{display:table-header-group}
svg{max-width:100%}
p,blockquote,dt,td.content{font-size:1em;orphans:3;widows:3}
h2,h3,#toctitle,.sidebarblock>.content>.title{page-break-after:avoid}
#header,#content,#footnotes,#footer{max-width:none}
#toc,.sidebarblock,.exampleblock>.content{background:none!important}
#toc{border-bottom:1px solid #dddddf!important;padding-bottom:0!important}
body.book #header{text-align:center}
body.book #header>h1:first-child{border:0!important;margin:2.5em 0 1em}
body.book #header .details{border:0!important;display:block;padding:0!important}
body.book #header .details span:first-child{margin-left:0!important}
body.book #header .details br{display:block}
body.book #header .details br+span::before{content:none!important}
body.book #toc{border:0!important;text-align:left!important;padding:0!important;margin:0!important}
body.book #toc,body.book #preamble,body.book h1.sect0,body.book .sect1>h2{page-break-before:always}
.listingblock code[data-lang]::before{display:block}
#footer{padding:0 .9375em}
.hide-on-print{display:none!important}
.print-only{display:block!important}
.hide-for-print{display:none!important}
.show-for-print{display:inherit!important}}
@media amzn-kf8,print{#header>h1:first-child{margin-top:1.25rem}
.sect1{padding:0!important}
.sect1+.sect1{border:0}
#footer{background:none}
#footer-text{color:rgba(0,0,0,.6);font-size:.9em}}
@media amzn-kf8{#header,#content,#footnotes,#footer{padding:0}}
</style>
</head>
<body class="article">
<div id="header">
</div>
<div id="content">
<hr>
<div class="paragraph">
<p>layout: docs
title: Monitoring Data Science Models
permalink: /docs/monitoring-data-science-models
custom_css: asciidoc.css
---
:upstream:
:YEAR: 2024</p>
</div>
<h1 id="_monitoring_data_science_models" class="sect0">Monitoring data science models</h1>
<div class="sect1">
<h2 id="overview-of-model-monitoring_monitor">Overview of model monitoring</h2>
<div class="sectionbody">
<div class="admonitionblock important _abstract">
<table>
<tr>
<td class="icon">
<div class="title">Important</div>
</td>
<td class="content">
<div class="paragraph">
<p>Technology Preview features are not supported with Red&#160;Hat production service level agreements (SLAs) and might not be functionally complete.
Red&#160;Hat does not recommend using them in production.
These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</p>
</div>
<div class="paragraph">
<p>For more information about the support scope of Red&#160;Hat Technology Preview features, see <a href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</a>.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>To ensure that machine-learning models are transparent, fair, and reliable, data scientists can use TrustyAI in {productname-short} to monitor their data science models.</p>
</div>
<div class="paragraph">
<p>Data scientists can monitor their data science models in {productname-short} for the following metrics:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Data drift</dt>
<dd>
<p>Detect changes in data over time by comparing the latest real-world data to the original training data. Comparing the data identifies shifts or deviations that could impact model performance, ensuring the model remains accurate and reliable.</p>
</dd>
<dt class="hdlist1">Explainability</dt>
<dd>
<p>Understand how your model makes its predictions and decisions.</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>Unresolved directive in monitoring-data-science-models.adoc - include::assemblies/configuring-trustyai.adoc[leveloffset=+1]</p>
</div>
<div class="paragraph">
<p>Unresolved directive in monitoring-data-science-models.adoc - include::assemblies/setting-up-trustyai-for-your-project.adoc[leveloffset=+1]</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="monitoring-model-bias_bias-monitoring">Monitoring model bias</h2>
<div class="sectionbody">
<div class="paragraph">
<p>As a data scientist, you might want to monitor your machine learning models for bias. This means monitoring for algorithmic deficiencies that might skew the outcomes or decisions that the model produces. Importantly, this type of monitoring helps you to ensure that the model is not biased against particular protected groups or features.</p>
</div>
<div class="paragraph">
<p>Open Data Hub provides a set of metrics that help you to monitor your models for bias. You can use the Open Data Hub interface to choose an available metric and then configure model-specific details such as a protected attribute, the privileged and unprivileged groups, the outcome you want to monitor, and a threshold for bias. You then see a chart of the calculated values for a specified number of model inferences.</p>
</div>
<div class="sect2">
<h3 id="creating-a-bias-metric_bias-monitoring">Creating a bias metric</h3>
<div class="paragraph _abstract">
<p>To monitor a deployed model for bias, you must first create bias metrics. When you create a bias metric, you specify details relevant to your model such as a protected attribute, privileged and unprivileged groups, a model outcome and a value that you want to monitor, and the acceptable threshold for bias.</p>
</div>
<div class="paragraph">
<p>You can create a bias metric for a model by using the Open Data Hub dashboard or by using the OpenShift command-line interface (CLI).</p>
</div>
<div class="sect3">
<h4 id="creating-a-bias-metric-using-dashboard_bias-monitoring">Creating a bias metric by using the dashboard</h4>
<div class="paragraph _abstract">
<p>The following procedure describes how to use the Open Data Hub dashboard to create a bias metric for a model.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>You are familiar with <a href="{odhdocshome}/monitoring-data-science-models/#supported-bias-metrics_monitor">the bias metrics that Open Data Hub supports</a> and how to interpret them.</p>
</li>
<li>
<p>You are familiar with the specific data set schema and understand the names and meanings of the inputs and outputs.</p>
</li>
<li>
<p>Your OpenShift cluster administrator has configured monitoring for the model serving platform, enabled the TrustyAI component in the Open Data Hub {install-package}, and installed the TrustyAI service for the data science project where the models are deployed.</p>
</li>
<li>
<p>You have a user token for authentication as described in <a href="{odhdocshome}/monitoring-data-science-models/#authenticating-trustyai-service_monitor">Authenticating the TrustyAI service</a>.</p>
</li>
<li>
<p>You have logged in to Open Data Hub.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Optional: To set the <code>TRUSTY_ROUTE</code> variable, follow these steps.</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>In a terminal window, log in to the OpenShift cluster where Open Data Hub is deployed.</p>
<div class="listingblock">
<div class="content">
<pre>oc login</pre>
</div>
</div>
</li>
<li>
<p>Set the <code>TRUSTY_ROUTE</code> variable to the external route for the TrustyAI service pod.</p>
<div class="listingblock">
<div class="content">
<pre>TRUSTY_ROUTE=https://$(oc get route/trustyai-service --template={{.spec.host}})</pre>
</div>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>In the left menu of the Open Data Hub dashboard, click <strong>Model Serving</strong>.</p>
</li>
<li>
<p>On the <strong>Deployed models</strong> page, select your project from the drop-down list.</p>
</li>
<li>
<p>Click the name of the model that you want to configure bias metrics for.</p>
</li>
<li>
<p>On the metrics page for the model, click the <strong>Model bias</strong> tab.</p>
</li>
<li>
<p>Click <strong>Configure</strong>.</p>
</li>
<li>
<p>In the <strong>Configure bias metrics</strong> dialog, complete the following steps to configure bias metrics:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>In the <strong>Metric name</strong> field, type a unique name for your bias metric. Note that you cannot change the name of this metric later.</p>
</li>
<li>
<p>From the <strong>Metric type</strong> list, select one of the metrics types that are available in Open Data Hub.</p>
</li>
<li>
<p>In the <strong>Protected attribute</strong> field, type the name of an attribute in your model that you want to monitor for bias.</p>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">
You can use a <code>curl</code> command to query the metadata endpoint and view input attribute names and values. For example: <code>curl -H "Authorization: Bearer $TOKEN" $TRUSTY_ROUTE/info | jq ".[0].data.inputSchema"</code>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>In the <strong>Privileged value</strong> field, type the name of a privileged group for the protected attribute that you specified.</p>
</li>
<li>
<p>In the <strong>Unprivileged value</strong> field, type the name of an unprivileged group for the protected attribute that you specified.</p>
</li>
<li>
<p>In the <strong>Output</strong> field, type the name of the model outcome that you want to monitor for bias.</p>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<div class="title">Tip</div>
</td>
<td class="content">
You can use a <code>curl</code> command to query the metadata endpoint and view output attribute names and values. For example: <code>curl -H "Authorization: Bearer $TOKEN" $TRUSTY_ROUTE/info | jq ".[0].data.outputSchema"</code>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>In the <strong>Output value</strong> field, type the value of the outcome that you want to monitor for bias.</p>
</li>
<li>
<p>In the <strong>Violation threshold</strong> field, type the bias threshold for your selected metric type. This threshold value defines how far the specified metric can be from the fairness value for your metric, before the model is considered biased.</p>
</li>
<li>
<p>In the <strong>Metric batch size</strong> field, type the number of model inferences that Open Data Hub includes each time it calculates the metric.</p>
</li>
</ol>
</div>
</li>
<li>
<p>Ensure that the values you entered are correct.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>You cannot edit a model bias metric configuration after you create it. Instead, you can duplicate a metric and then edit (configure) it; however, the history of the original metric is not applied to the copy.</p>
</div>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Click <strong>Configure</strong>.</p>
</li>
</ol>
</div>
<div class="ulist">
<div class="title">Verification</div>
<ul>
<li>
<p>The <strong>Bias metric configuration</strong> page shows the bias metrics that you configured for your model.</p>
</li>
</ul>
</div>
<div class="paragraph">
<div class="title">Next step</div>
<p>To view metrics, on the <strong>Bias metric configuration</strong> page, click <strong>View metrics</strong> in the upper-right corner.</p>
</div>
</div>
<div class="sect3">
<h4 id="creating-a-bias-metric-using-cli_bias-monitoring">Creating a bias metric by using the CLI</h4>
<div class="paragraph _abstract">
<p>The following procedure describes how to use the OpenShift command-line interface (CLI) to create a bias metric for a model.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>You are familiar with <a href="{odhdocshome}/monitoring-data-science-models/#supported-bias-metrics_monitor">the bias metrics that Open Data Hub supports</a> and how to interpret them.</p>
</li>
<li>
<p>You are familiar with the specific data set schema and understand the names and meanings of the inputs and outputs.</p>
</li>
<li>
<p>Your OpenShift cluster administrator has configured monitoring for the model serving platform, enabled the TrustyAI component in the Open Data Hub {install-package}, and installed the TrustyAI service for the data science project where the models are deployed.</p>
</li>
<li>
<p>You installed the OpenShift command line interface (<code>oc</code>) as described in <a href="https://docs.openshift.com/container-platform/4.16/cli_reference/openshift_cli/getting-started-cli.html">Get Started with the CLI</a>.</p>
</li>
<li>
<p>You have a user token for authentication as described in <a href="{odhdocshome}/monitoring-data-science-models/#authenticating-trustyai-service_monitor">Authenticating the TrustyAI service</a>.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>In a terminal window, log in to the OpenShift cluster where Open Data Hub is deployed.</p>
<div class="listingblock">
<div class="content">
<pre>oc login</pre>
</div>
</div>
</li>
<li>
<p>Set the <code>TRUSTY_ROUTE</code> variable to the external route for the TrustyAI service pod.</p>
<div class="listingblock">
<div class="content">
<pre>TRUSTY_ROUTE=https://$(oc get route/trustyai-service --template={{.spec.host}})</pre>
</div>
</div>
</li>
<li>
<p>Optionally, get the full list of TrustyAI service endpoints and payloads.</p>
<div class="listingblock">
<div class="content">
<pre>curl -H "Authorization: Bearer $TOKEN" --location $TRUSTY_ROUTE/q/openapi</pre>
</div>
</div>
</li>
<li>
<p>Configure bias metrics by running a command that uses the following syntax and payload structure:</p>
<div class="paragraph">
<p><strong>Syntax</strong>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>curl -sk -H "Authorization: Bearer $TOKEN" -X POST --location $TRUSTY_ROUTE/metrics/spd/request \
 --header 'Content-Type: application/json' \
 --data &lt;payload&gt;</pre>
</div>
</div>
<div class="paragraph">
<p><strong>Payload structure</strong>:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>modelId</code></dt>
<dd>
<p>The name of the model to query.</p>
</dd>
<dt class="hdlist1"><code>protectedAttribute</code></dt>
<dd>
<p>The name of the feature that distinguishes the groups that you are checking for fairness.</p>
</dd>
<dt class="hdlist1"><code>privilegedAttribute</code></dt>
<dd>
<p>The suspected favored (positively biased) class.</p>
</dd>
<dt class="hdlist1"><code>unprivilegedAttribute</code></dt>
<dd>
<p>The suspected unfavored (negatively biased) class.</p>
</dd>
<dt class="hdlist1"><code>outcomeName</code></dt>
<dd>
<p>The name of the output that provides the output you are examining for fairness.</p>
</dd>
<dt class="hdlist1"><code>favorableOutcome</code></dt>
<dd>
<p>The value of the <code>outcomeName</code> output that describes the favorable or desired model prediction.</p>
</dd>
<dt class="hdlist1"><code>batchSize</code></dt>
<dd>
<p>The number of previous inferences to include in the calculation.</p>
</dd>
</dl>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>curl -sk -H "Authorization: Bearer $TOKEN" -X POST --location $TRUSTY_ROUTE/metrics/group/fairness/spd/ \
     --header 'Content-Type: application/json' \
     --data "{
                 \"modelId\": \"demo-loan-nn-onnx-alpha\",
                 \"protectedAttribute\": \"Is Male-Identifying?\",
                 \"privilegedAttribute\": 1.0,
                 \"unprivilegedAttribute\": 0.0,
                 \"outcomeName\": \"Will Default?\",
                 \"favorableOutcome\": 0,
                 \"batchSize\": 5000
             }"</pre>
</div>
</div>
<div class="paragraph">
<div class="title">Verification</div>
<p>The bias metrics request returns an output similar to the following:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>{
   "timestamp":"2023-10-24T12:06:04.586+00:00",
   "type":"metric",
   "value":-0.0029676404469311524,
   "namedValues":null,
   "specificDefinition":"The SPD of -0.002968 indicates that the likelihood of Group:Is Male-Identifying?=1.0 receiving Outcome:Will Default?=0 was -0.296764 percentage points lower than that of Group:Is Male-Identifying?=0.0.",
   "name":"SPD",
   "id":"d2707d5b-cae9-41aa-bcd3-d950176cbbaf",
   "thresholds":{"lowerBound":-0.1,"upperBound":0.1,"outsideBounds":false}
}</pre>
</div>
</div>
<div class="paragraph">
<p>The <code>specificDefinition</code> field helps you understand the real-world interpretation of these metric values. For this example, the model is fair over the <code>Is Male-Identifying?</code> field, with the rate of positive outcome only differing by about -0.3%.</p>
</div>
</div>
<div class="sect3">
<h4 id="duplicating-a-bias-metric_bias-monitoring">Duplicating a bias metric</h4>
<div class="paragraph _abstract">
<p>If you want to edit an existing metric, you can duplicate (copy) it in the Open Data Hub interface and then edit the values in the copy. However, note that the history of the original metric is not applied to the copy.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>You are familiar with <a href="{odhdocshome}/monitoring-data-science-models/#supported-bias-metrics_monitor">the bias metrics that Open Data Hub supports</a> and how to interpret them.</p>
</li>
<li>
<p>You are familiar with the specific data set schema and understand the names and meanings of the inputs and outputs.</p>
</li>
<li>
<p>You have logged in to Open Data Hub.</p>
</li>
<li>
<p>There is an existing bias metric that you want to duplicate.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>In the left menu of the Open Data Hub dashboard, click <strong>Model Serving</strong>.</p>
</li>
<li>
<p>On the <strong>Deployed models</strong> page, click the name of the model with the bias metric that you want to duplicate.</p>
</li>
<li>
<p>On the metrics page for the model, click the <strong>Model bias</strong> tab.</p>
</li>
<li>
<p>Click <strong>Configure</strong>.</p>
</li>
<li>
<p>On the <strong>Bias metric configuration</strong> page, click the action menu (&#8942;) next to the metric that you want to copy and then click <strong>Duplicate</strong>.</p>
</li>
<li>
<p>In the <strong>Configure bias metric</strong> dialog, follow these steps:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>In the <strong>Metric name</strong> field, type a unique name for your bias metric. Note that you cannot change the name of this metric later.</p>
</li>
<li>
<p>Change the values of the fields as needed. For a description of these fields, see <a href="#creating-a-bias-metric-using-dashboard_bias-monitoring">Creating a bias metric by using the dashboard</a>.</p>
</li>
</ol>
</div>
</li>
<li>
<p>Ensure that the values you entered are correct, and then click <strong>Configure</strong>.</p>
</li>
</ol>
</div>
<div class="ulist">
<div class="title">Verification</div>
<ul>
<li>
<p>The <strong>Bias metric configuration</strong> page shows the bias metrics that you configured for your model.</p>
</li>
</ul>
</div>
<div class="paragraph">
<div class="title">Next step</div>
<p>To view metrics, on the <strong>Bias metric configuration</strong> page, click <strong>View metrics</strong> in the upper-right corner.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="deleting-a-bias-metric_bias-monitoring">Deleting a bias metric</h3>
<div class="paragraph _abstract">
<p>You can delete a bias metric for a model by using the Open Data Hub dashboard or by using the OpenShift command-line interface (CLI).</p>
</div>
<div class="sect3">
<h4 id="deleting-a-bias-metric-using-dashboard_bias-monitoring">Deleting a bias metric by using the dashboard</h4>
<div class="paragraph _abstract">
<p>The following procedure describes how to use the Open Data Hub dashboard to delete a bias metric for a model.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>You have logged in to Open Data Hub.</p>
</li>
<li>
<p>There is an existing bias metric that you want to delete.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>In the left menu of the Open Data Hub dashboard, click <strong>Model Serving</strong>.</p>
</li>
<li>
<p>On the <strong>Deployed models</strong> page, click the name of the model with the bias metric that you want to delete.</p>
</li>
<li>
<p>On the metrics page for the model, click the <strong>Model bias</strong> tab.</p>
</li>
<li>
<p>Click <strong>Configure</strong>.</p>
</li>
<li>
<p>Click the action menu (&#8942;) next to the metric that you want to delete and then click <strong>Delete</strong>.</p>
</li>
<li>
<p>In the <strong>Delete bias metric</strong> dialog, type the metric name to confirm the deletion.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>You cannot undo deleting a bias metric.</p>
</div>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Click <strong>Delete bias metric</strong>.</p>
</li>
</ol>
</div>
<div class="ulist">
<div class="title">Verification</div>
<ul>
<li>
<p>The <strong>Bias metric configuration</strong> page does not show the bias metric that you deleted.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="deleting-a-bias-metric-using-cli_bias-monitoring">Deleting a bias metric by using the CLI</h4>
<div class="paragraph _abstract">
<p>The following procedure describes how to use the OpenShift command-line interface (CLI) to delete a bias metric for a model.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>You have logged in to Open Data Hub.</p>
</li>
<li>
<p>There is an existing bias metric that you want to delete.</p>
</li>
<li>
<p>You have a user token for authentication as described in <a href="{odhdocshome}/monitoring-data-science-models/#authenticating-trustyai-service_monitor">Authenticating the TrustyAI service</a>.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>In a terminal window, log in to the OpenShift cluster where Open Data Hub is deployed.</p>
<div class="listingblock">
<div class="content">
<pre>oc login</pre>
</div>
</div>
</li>
<li>
<p>Optional: To list all currently active requests for a metric, use <code>GET /metrics/{{metric}}/requests</code>. For example, to list all currently scheduled SPD metrics, type:</p>
<div class="listingblock">
<div class="content">
<pre>curl -H "Authorization: Bearer $TOKEN" -X GET --location "$TRUSTY_ROUTE/metrics/spd/requests"</pre>
</div>
</div>
</li>
<li>
<p>To delete a metric, send an HTTP <code>DELETE</code> request to the <code>/metrics/$METRIC/request</code> endpoint to stop the periodic calculation, including the id of periodic task that you want to cancel in the payload. For example:</p>
<div class="listingblock">
<div class="content">
<pre>curl -H "Authorization: Bearer $TOKEN" -X DELETE --location "$TRUSTY_ROUTE/metrics/spd/request" \
    -H "Content-Type: application/json" \
    -d "{
          \"requestId\": \"3281c891-e2a5-4eb3-b05d-7f3831acbb56\"
        }"</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<div class="title">Verification</div>
<p>Use <code>GET /metrics/{{metric}}/requests</code> to list all currently active requests for the metric and verify the metric that you deleted is not shown. For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>curl -H "Authorization: Bearer $TOKEN" -X GET --location "$TRUSTY_ROUTE/metrics/spd/requests"</pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="viewing-bias-metrics_bias-monitoring">Viewing bias metrics for a model</h3>
<div class="paragraph _abstract">
<p>After you configure your model for bias monitoring, you can use the Open Data Hub dashboard to view and update the metrics that you configured.</p>
</div>
<div class="ulist">
<div class="title">Prerequisite</div>
<ul>
<li>
<p>You configured bias metrics for your model as described in <a href="{odhdocshome}/monitoring-data-science-models/#creating-a-bias-metric_bias-monitoring">Creating a bias metric</a>.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>In the Open Data Hub dashboard, click <strong>Model Serving</strong>.</p>
</li>
<li>
<p>On the <strong>Deployed models</strong> page, click the name of a model that you want to view bias metrics for.</p>
</li>
<li>
<p>To update the metrics shown on the page, follow these steps:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>In the <strong>Metrics to display</strong> section, use the <strong>Select a metric</strong> list to select a metric to show on the page.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
Each time you select a metric to show on the page, an additional <strong>Select a metric</strong> list appears. This enables you to show multiple metrics on the page.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>From the <strong>Time range</strong> list in the upper-right corner, select a value.</p>
</li>
<li>
<p>From the <strong>Refresh interval</strong> list in the upper-right corner, select a value.</p>
<div class="paragraph">
<p>The metrics page shows the metrics that you selected.</p>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Optional: To remove one or more metrics from the page, in the <strong>Metrics to display</strong> section, perform one of the following actions:</p>
<div class="ulist">
<ul>
<li>
<p>To remove an individual metric, click the cancel icon (&#10006;) next to the metric name.</p>
</li>
<li>
<p>To remove all metrics, click the cancel icon (&#10006;) in the <strong>Select a metric</strong> list.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Optional: To return to configuring bias metrics for the model, on the metrics page, click <strong>Configure</strong> in the upper-right corner.</p>
</li>
</ol>
</div>
<div class="ulist">
<div class="title">Verification</div>
<ul>
<li>
<p>The metrics page shows the metrics selections that you made.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="supported-bias-metrics_bias-monitoring">Supported bias metrics</h3>
<div class="paragraph">
<p>Open Data Hub supports the following bias metrics.</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Statistical Parity Difference</dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p><em>Statistical Parity Difference</em> (SPD) is the difference in the probability of a favorable outcome prediction between unprivileged and privileged groups.  The formal definition of SPD is the following:</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="images/bias-metric-spd.png" alt="SPD definition">
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><em>&#375;</em> = 1 is the favorable outcome.</p>
</li>
<li>
<p><em>D&#7524;</em> and <em>D&#8346;</em> are the unprivileged and privileged group data.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>You can interpret SPD values as follows:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>A value of <code>0</code> means that the model is behaving fairly for a selected attribute (for example,  race, gender).</p>
</li>
<li>
<p>A value in the range  <code>-0.1</code> to <code>0.1</code> means that the model is reasonably fair for a selected attribute. Instead, you can attribute the difference in probability to other factors, such as the sample size.</p>
</li>
<li>
<p>A value outside the range <code>-0.1</code> to <code>0.1</code> indicates that the model is unfair for a selected attribute.</p>
</li>
<li>
<p>A negative value indicates that the model has bias against the unprivileged group.</p>
</li>
<li>
<p>A positive value indicates that the model has bias against the privileged group.</p>
</li>
</ul>
</div>
</div>
</div>
</dd>
<dt class="hdlist1">Disparate Impact Ratio</dt>
<dd>
<div class="openblock">
<div class="content">
<div class="paragraph">
<p><em>Disparate Impact Ratio</em> (DIR) is the ratio of the probability of a favorable outcome prediction for unprivileged groups to that of privileged groups. The formal definition of DIR is the following:</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="images/bias-metric-dir.png" alt="DIR definition">
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><em>&#375;</em> = 1 is the favorable outcome.</p>
</li>
<li>
<p><em>D&#7524;</em> and <em>D&#8346;</em> are the unprivileged and privileged group data.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The threshold to identify bias depends on your own criteria and specific use case.</p>
</div>
<div class="paragraph">
<p>For example, if your threshold for identifying bias is represented  by a DIR value below <code>0.8</code> or above <code>1.2</code>, you can interpret the DIR values as follows:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>A value of <code>1</code> means that the model is fair for a selected attribute.</p>
</li>
<li>
<p>A value of between <code>0.8</code> and <code>1.2</code> means that the model is reasonably fair for a selected attribute.</p>
</li>
<li>
<p>A value below <code>0.8</code> or above <code>1.2</code> indicates bias.</p>
</li>
</ul>
</div>
</div>
</div>
</dd>
</dl>
</div>
<div class="paragraph">
<p>Unresolved directive in monitoring-data-science-models.adoc - include::assemblies/monitoring-data-drift.adoc[leveloffset=+1]</p>
</div>
<div class="paragraph">
<p>Unresolved directive in monitoring-data-science-models.adoc - include::assemblies/using-explainability.adoc[leveloffset=+1]</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="bias-monitoring-tutorial_bias-tutorial">Bias monitoring tutorial - Gender bias example</h2>
<div class="sectionbody">
<div class="paragraph _abstract">
<p>Step-by-step guidance for using TrustyAI in Open Data Hub to monitor machine learning models for bias.</p>
</div>
<div class="sect2">
<h3 id="t-bias-introduction_bias-tutorial">Introduction</h3>
<div class="paragraph">
<p>Ensuring that your machine learning models are fair and unbiased is essential for building trust with your users. Although you can assess fairness during model training, it is only in deployment that your models use real-world data. Even if your models are unbiased on training data, they can exhibit dangerous biases in real-world scenarios. Therefore, it is crucial to monitor your models for fairness during their real-world deployment.</p>
</div>
<div class="paragraph">
<p>In this tutorial, you learn how to monitor models for bias. You will use two example models to complete the following tasks:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Deploy the models by using multi-model serving.</p>
</li>
<li>
<p>Send training data to the models.</p>
</li>
<li>
<p>Examine the metadata for the models.</p>
</li>
<li>
<p>Check model fairness.</p>
</li>
<li>
<p>Schedule and check fairness and identity metric requests.</p>
</li>
<li>
<p>Simulate real-world data.</p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="_about_the_example_models">About the example models</h4>
<div class="paragraph">
<p>For this tutorial, your role is a DevOps engineer for a credit lending company. The company&#8217;s data scientists have created two candidate neural network models to predict whether a borrower will default on a loan. Both models make predictions based on the following information from the borrower&#8217;s application:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Number of Children</p>
</li>
<li>
<p>Total Income</p>
</li>
<li>
<p>Number of Total Family Members</p>
</li>
<li>
<p>Is Male-Identifying?</p>
</li>
<li>
<p>Owns Car?</p>
</li>
<li>
<p>Owns Realty?</p>
</li>
<li>
<p>Is Partnered?</p>
</li>
<li>
<p>Is Employed?</p>
</li>
<li>
<p>Lives with Parents?</p>
</li>
<li>
<p>Age (in days)</p>
</li>
<li>
<p>Length of Employment (in days)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>As the DevOps engineer, your task is to verify that the models are not biased against the <code>Is Male-Identifying?</code> gender field. To complete this task, you can monitor the models using the Statistical Parity Difference (SPD) metric, which reports whether there is a difference between how often male-identifying and non-male-identifying applicants are given favorable predictions (that is, they are predicted to pay off their loans). An ideal SPD metric is 0, meaning both groups are equally likely to receive a positive outcome. However, an SPD between -0.1 and 0.1 also indicates fairness, as it reflects only a +/-10% variation between the groups.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="t-bias-setting-up-your-environment_bias-tutorial">Setting up your environment</h3>
<div class="paragraph">
<p>To set up your environment for this tutorial, complete the following tasks:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Download tutorial files from the <a href="https://github.com/trustyai-explainability/odh-trustyai-demos/tree/main" target="_blank" rel="noopener">trustyai-explainability</a> repository.</p>
</li>
<li>
<p>Log in to the OpenShift cluster from the command line.</p>
</li>
<li>
<p>Configure monitoring for the model serving platform.</p>
</li>
<li>
<p>Enable the TrustyAI component in the Open Data Hub Operator.</p>
</li>
<li>
<p>Set up a project.</p>
</li>
<li>
<p>Authenticate the TrustyAI service.</p>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>The Open Data Hub Operator is installed on your {openshift-platform} cluster.</p>
</li>
<li>
<p>You have cluster administrator privileges for your {openshift-platform} cluster.</p>
</li>
<li>
<p>You have downloaded and installed the OpenShift command-line interface (CLI). See <a href="https://docs.openshift.com/container-platform/4.16/cli_reference/openshift_cli/getting-started-cli.html#installing-openshift-cli">Installing the OpenShift CLI</a>.</p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="_downloading_tutorial_files">Downloading tutorial files</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Go to <a href="https://github.com/trustyai-explainability/odh-trustyai-demos/tree/main" class="bare">https://github.com/trustyai-explainability/odh-trustyai-demos/tree/main</a>.</p>
</li>
<li>
<p>Click the <strong>Code</strong> button and then click <strong>Download ZIP</strong> to download the repository.</p>
</li>
<li>
<p>Extract the downloaded repository files.</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_logging_in_to_the_openshift_cluster_from_the_command_line">Logging in to the OpenShift cluster from the command line</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Obtain the command for logging in to the OpenShift cluster from the command line:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>In the upper-right corner of the {openshift-platform} web console, click your user name and select <strong>Copy login command</strong>.</p>
</li>
<li>
<p>Log in with your credentials and then click <strong>Display token</strong>.</p>
</li>
<li>
<p>Copy the <strong>Log in with this token</strong> command, which has the following syntax:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>$ oc login --token=<em>&lt;token&gt;</em> --server=<em>&lt;openshift_cluster_url&gt;</em></code></pre>
</div>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>In a terminal window, paste and run the login command.</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_configuring_monitoring_for_the_model_serving_platform">Configuring monitoring for the model serving platform</h4>
<div class="paragraph">
<p>To enable monitoring on user-defined projects, you must configure monitoring for the model serving platform.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Run the following command from the directory containing the downloaded tutorial files (<code>odh-trustyai-demos-main</code>):</p>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>oc apply -f 1-Installation/resources/enable_uwm.yaml</code></pre>
</div>
</div>
</li>
<li>
<p>To configure monitoring to hold metric data for 15 days, run the following command from the directory containing the downloaded tutorial files (<code>odh-trustyai-demos-main</code>):</p>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>oc apply -f 1-Installation/resources/uwm_configmap.yaml</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p>For more information, see <a href="https://opendatahub.io/docs/monitoring-data-science-models/#configuring-monitoring-for-the-multi-model-serving-platform_monitor">Configuring monitoring for the multi-model serving platform</a>.</p>
</div>
</div>
<div class="sect3">
<h4 id="enabling-trustyai-component_bias-tutorial">Enabling the TrustyAI component</h4>
<div class="paragraph _abstract">
<p>To allow your data scientists to use model monitoring, you must enable the TrustyAI component in Open Data Hub.</p>
</div>
<div class="ulist">
<div class="title">Prerequisites</div>
<ul>
<li>
<p>You have logged in to {openshift-platform} with the <code>cluster-admin</code> role.</p>
</li>
<li>
<p>You have access to the data science cluster.</p>
</li>
<li>
<p>You have installed Open Data Hub.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>In the {openshift-platform} console, click <strong>Operators</strong> &#8594; <strong>Installed Operators</strong>.</p>
</li>
<li>
<p>Search for the <strong>Open Data Hub Operator</strong>, and then click the Operator name to open the Operator details page.</p>
</li>
<li>
<p>Click the <strong>Data Science Cluster</strong> tab.</p>
</li>
<li>
<p>Click the default instance name (for example, <strong>default-dsc</strong>) to open the instance details page.</p>
</li>
<li>
<p>Click the <strong>YAML</strong> tab to show the instance specifications.</p>
</li>
<li>
<p>In the <code>spec:components</code> section, set the <code>managementState</code> field for the <code>trustyai</code> component to <code>Managed</code>:</p>
<div class="listingblock">
<div class="content">
<pre> trustyai:
    managementState: Managed</pre>
</div>
</div>
</li>
<li>
<p>Click <strong>Save</strong>.</p>
</li>
</ol>
</div>
<div class="paragraph">
<div class="title">Verification</div>
<p>Check the status of the <strong>trustyai-service-operator</strong> pod as follows:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>In the {openshift-platform} console, from the <strong>Project</strong> list, select <strong>opendatahub</strong>.</p>
</li>
<li>
<p>Click <strong>Workloads</strong> &#8594; <strong>Deployments</strong>.</p>
</li>
<li>
<p>Search for the <strong>trustyai-service-operator-contoller-manager</strong> deployment.
Check the status as follows:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Click the deployment name to open the deployment details page.</p>
</li>
<li>
<p>Click the <strong>Pods</strong> tab.</p>
</li>
<li>
<p>Check the pod status.</p>
<div class="paragraph">
<p>When the status of the <strong>trustyai-service-operator-controller-manager-<em>&lt;pod-id&gt;</em></strong> pod is <strong>Running</strong>, the pod is ready to use.</p>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_setting_up_a_project">Setting up a project</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>To create a new project named <code>model-namespace</code>, run the following command from the directory containing the downloaded tutorial files (<code>odh-trustyai-demos-main</code>):</p>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>oc new-project model-namespace</code></pre>
</div>
</div>
</li>
<li>
<p>Prepare the <code>model-namespace</code> project for multi-model serving:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>oc label namespace model-namespace "modelmesh-enabled=true" --overwrite=true</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
<div class="paragraph">
<p>Model monitoring with TrustyAI is only available on the ModelMesh-based <em>multi-model serving platform</em>. Model monitoring with TrustyAI is unavailable on the KServe-based <em>single-model serving platform</em>.</p>
</div>
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_authenticating_the_trustyai_service">Authenticating the TrustyAI service</h4>
<div class="paragraph">
<p>TrustyAI endpoints are authenticated with a Bearer token. To obtain this token and set a variable (TOKEN) to use later, run the following command:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>export TOKEN=$(oc whoami -t)</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="t-bias-deploying-models_bias-tutorial">Deploying models</h3>
<div class="paragraph">
<div class="title">Procedure</div>
<p>To deploy the models for this tutorial, run the following commands from the directory containing the downloaded tutorial files (<code>odh-trustyai-demos-main</code>).</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Navigate to the <code>model-namespace</code> project you created:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>oc project model-namespace</code></pre>
</div>
</div>
</li>
<li>
<p>Deploy the model&#8217;s storage container:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>oc apply -f 2-BiasMonitoring/resources/model_storage_container.yaml</code></pre>
</div>
</div>
</li>
<li>
<p>Deploy the OVMS 1.x serving runtime:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>oc apply -f 2-BiasMonitoring/resources/ovms-1.x.yaml</code></pre>
</div>
</div>
</li>
<li>
<p>Deploy the first model:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>oc apply -f 2-BiasMonitoring/resources/model_alpha.yaml</code></pre>
</div>
</div>
</li>
<li>
<p>Deploy the second model:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>oc apply -f 2-BiasMonitoring/resources/model_beta.yaml</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="olist arabic">
<div class="title">Verification</div>
<ol class="arabic">
<li>
<p>In the {openshift-platform} console, click <strong>Workloads</strong>  <strong>Pods</strong>.</p>
</li>
<li>
<p>Confirm that there are four pods:</p>
<div class="ulist">
<ul>
<li>
<p><code>minio</code></p>
</li>
<li>
<p><code>modelmesh-serving-ovms-1.x-xxxxxxxxxx-xxxxx</code></p>
</li>
<li>
<p><code>modelmesh-serving-ovms-1.x-xxxxxxxxxx-xxxxx</code></p>
</li>
<li>
<p><code>trustyai-service-xxxxxxxxxx-xxxxx</code></p>
<div class="paragraph">
<p>When the TrustyAI service has registered the deployed models, you will see the  <code>modelmesh-serving-ovms-1.x-xxxxx</code> pods get redeployed.</p>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>To verify that TrustyAI has registered the models:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Select one of the <code>modelmesh-serving-ovms-1.x-xxxxx</code> pods.</p>
</li>
<li>
<p>Click the <strong>Environment</strong> tab and confirm that the <code>MM_PAYLOAD_PROCESSORS</code> field is set.</p>
<div class="imageblock">
<div class="content">
<img src="images/model_environment.png" alt="model environment">
</div>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="t-bias-sending-training-data-to-the-models_bias-tutorial">Sending training data to the models</h3>
<div class="paragraph">
<p>Pass the training data through the models to be able to compute baseline fairness values.</p>
</div>
<div class="ulist">
<div class="title">Procedure</div>
<ul>
<li>
<p>In a terminal window, run the following command from the directory containing the downloaded tutorial files (<code>odh-trustyai-demos-main</code>):</p>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>for batch in 0 250 500 750 1000 1250 1500 1750 2000 2250; do
  2-BiasMonitoring/scripts/send_data_batch
2-BiasMonitoring/data/training/$batch.json
done</code></pre>
</div>
</div>
<div class="paragraph">
<p>This process can take several minutes.</p>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<div class="title">Verification</div>
<p>The script returns verification messages indicating whether TrustyAI is receiving the data, but you can also verify in the cluster metrics:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>In the {openshift-platform} web console, click <strong>Observe</strong> &#8594; <strong>Metrics</strong>.</p>
</li>
<li>
<p>In the <strong>Expression</strong> field, enter <code>trustyai_model_observations_total</code> and click <strong>Run Queries</strong>.</p>
<div class="paragraph">
<p>You should see both models listed with around 2250 inferences each, indicating that TrustyAI has cataloged enough inputs and outputs to begin analysis.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/observed_inferences.png" alt="observed inferences">
</div>
</div>
</li>
<li>
<p>Optional: After running a query, you can select a time range and refresh interval.</p>
<div class="ulist">
<ul>
<li>
<p>From the <strong>Time range</strong> list, select 5 minutes.</p>
</li>
<li>
<p>From the <strong>Refresh interval</strong> list, select 15 seconds.</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
</div>
<div class="sect2">
<h3 id="t-bias-examining-trustyai-model-metadata_bias-tutorial">Examining TrustyAI&#8217;s model metadata</h3>
<div class="paragraph">
<p>You can verify that TrustyAI can access the models by using the <code>/info</code> endpoint.</p>
</div>
<div class="olist arabic">
<div class="title">Procedure</div>
<ol class="arabic">
<li>
<p>Find the route to the TrustyAI service:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>TRUSTY_ROUTE=https://$(oc get route/trustyai-service --template={{.spec.host}}); echo $TRUSTY_ROUTE</code></pre>
</div>
</div>
</li>
<li>
<p>Query the <code>/info</code> endpoint:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>curl -H "Authorization: Bearer ${TOKEN}" $TRUSTY_ROUTE/info | jq</code></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<div class="title">Verification</div>
<p>A JSON file is generated with the following information for each model:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The names, data types, and positions of fields in the input and output.</p>
</li>
<li>
<p>The observed values that these fields take.</p>
</li>
<li>
<p>The total number of input-output pairs observed.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>For an example output file, see the <code>odh-trustyai-demos-main/2-BiasMonitoring/scripts/info_response.json</code> file in your downloaded tutorial files.</p>
</div>
</div>
<div class="sect2">
<h3 id="t-bias-labeling-data-fields_bias-tutorial">Labeling data fields</h3>
<div class="paragraph">
<p>You can apply name mappings to your inputs and outputs for more meaningful field names by sending a POST request to the <code>/info/names</code> endpoint.</p>
</div>
<div class="paragraph">
<p>For this tutorial, run the following command from the directory containing the downloaded tutorial files (<code>odh-trustyai-demos-main</code>):</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>2-BiasMonitoring/scripts/apply_name_mapping.sh</code></pre>
</div>
</div>
<div class="paragraph">
<p>For general steps, see <a href="{odhdocshome}/monitoring-data-science-models/#labeling-data-fields_monitor">Labeling data fields</a>.</p>
</div>
<div class="paragraph">
<p>To understand the payload structure, see the <code>odh-trustyai-demos-main/2-BiasMonitoring/scripts/apply_name_mapping.sh</code> file in your downloaded tutorial files.</p>
</div>
</div>
<div class="sect2">
<h3 id="t-bias-checking-model-fairness_bias-tutorial">Checking model fairness</h3>
<div class="ulist">
<div class="title">Procedure</div>
<ul>
<li>
<p>To compute the model&#8217;s cumulative fairness up to this point, you can check the <code>/metrics/group/fairness/spd</code> endpoint:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>echo -e "=== MODEL ALPHA ==="
curl -sk -H "Authorization: Bearer ${TOKEN}" -X POST --location $TRUSTY_ROUTE/metrics/group/fairness/spd/ \
     --header 'Content-Type: application/json' \
     --data "{
                 \"modelId\": \"demo-loan-nn-onnx-alpha\",
                 \"protectedAttribute\": \"Is Male-Identifying?\",
                 \"privilegedAttribute\": 1.0,
                 \"unprivilegedAttribute\": 0.0,
                 \"outcomeName\": \"Will Default?\",
                 \"favorableOutcome\": 0,
                 \"batchSize\": 5000
             }" | jq
echo -e "\n\n=== MODEL BETA ==="
curl -sk -H "Authorization: Bearer ${TOKEN}" -X POST --location $TRUSTY_ROUTE/metrics/group/fairness/spd \
     --header 'Content-Type: application/json' \
     --data "{
                 \"modelId\": \"demo-loan-nn-onnx-beta\",
                 \"protectedAttribute\": \"Is Male-Identifying?\",
                 \"privilegedAttribute\": 1.0,
                 \"unprivilegedAttribute\": 0.0,
                 \"outcomeName\": \"Will Default?\",
                 \"favorableOutcome\": 0,
                 \"batchSize\": 5000
             }" | jq
echo</code></pre>
</div>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>The payload structure is as follows:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>modelId</code>: The name of the model to query.</p>
</li>
<li>
<p><code>protectedAttribute</code>: The name of the feature that distinguishes the groups that you are checking for fairness over.</p>
</li>
<li>
<p><code>privilegedAttribute</code>: The value of the <code>protectedAttribute</code> that describes the suspected favored (positively biased) class.</p>
</li>
<li>
<p><code>unprivilegedAttribute</code>: The value of the <code>protectedAttribute</code> that describes the suspected unfavored (negatively biased) class.</p>
</li>
<li>
<p><code>outcomeName</code>: The name of the output that provides the output you are examining for fairness.</p>
</li>
<li>
<p><code>favorableOutcome</code>: The value of the <code>outcomeName</code> output that describes the favorable model prediction.</p>
</li>
<li>
<p><code>batchSize</code>: The number of previous inferences to include in the calculation.</p>
</li>
</ul>
</div>
<div class="paragraph">
<div class="title">Verification</div>
<p>Confirm that you see outputs similar to the following examples:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Model Alpha</dt>
</dl>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>=== MODEL ALPHA ===
{
  "timestamp": "2024-07-25T16:26:50.412+00:00",
  "type": "metric",
  "value": 0.003056835834369387,
  "namedValues": null,
  "specificDefinition": "The SPD of 0.003057 indicates that the likelihood of Group:Is Male-Identifying?=[1.0] receiving Outcome:Will Default?=[0] was 0.305684 percentage points higher than that of Group:Is Male-Identifying?=[0.0].",
  "name": "SPD",
  "id": "542bd51e-dd2f-40f6-947f-c1c22bd71765",
  "thresholds": {
    "lowerBound": -0.1,
    "upperBound": 0.1,
    "outsideBounds": false
  }
}</code></pre>
</div>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Model Beta</dt>
</dl>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>=== MODEL BETA ===
{
  "timestamp": "2024-07-25T16:26:50.648+00:00",
  "type": "metric",
  "value": 0.029078518433627354,
  "namedValues": null,
  "specificDefinition": "The SPD of 0.029079 indicates that the likelihood of Group:Is Male-Identifying?=[1.0] receiving Outcome:Will Default?=[0] was 2.907852 percentage points higher than that of Group:Is Male-Identifying?=[0.0].",
  "name": "SPD",
  "id": "df292f06-9255-4158-8b02-4813a8777b7b",
  "thresholds": {
    "lowerBound": -0.1,
    "upperBound": 0.1,
    "outsideBounds": false
  }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>specificDefinition</code> field is important in understanding the real-world interpretation of these metric values; you can see that both model Alpha and Beta are fair over the <code>Is Male-Identifying</code> field, with the two groups' rates of positive outcomes only differing by -0.3% for model Alpha and 2.8% for model Beta.</p>
</div>
</div>
<div class="sect2">
<h3 id="t-bias-scheduling-a-fairness-metric-request_bias-tutorial">Scheduling a fairness metric request</h3>
<div class="paragraph">
<p>While the models are fair over the training data, you also want to ensure that they remain fair over real-world inference data. To monitor their fairness, you can schedule a metric request to compute at recurring intervals throughout deployment by passing the same payloads to the <code>/metrics/group/fairness/spd/request</code> endpoint.</p>
</div>
<div class="ulist">
<div class="title">Procedure</div>
<ul>
<li>
<p>In a terminal window, run the following commands from the directory containing the downloaded tutorial files (<code>odh-trustyai-demos-main</code>):</p>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>echo -e "\n\n=== MODEL ALPHA ===\n"
curl -sk -H "Authorization: Bearer ${TOKEN}" -X POST --location $TRUSTY_ROUTE/metrics/group/fairness/spd/request \
     --header 'Content-Type: application/json' \
     --data "{
                 \"modelId\": \"demo-loan-nn-onnx-alpha\",
                 \"protectedAttribute\": \"Is Male-Identifying?\",
                 \"privilegedAttribute\": 1.0,
                 \"unprivilegedAttribute\": 0.0,
                 \"outcomeName\": \"Will Default?\",
                 \"favorableOutcome\": 0,
                 \"batchSize\": 5000
             }"
echo -e "\n\n=== MODEL BETA ===\n"
curl -sk -H "Authorization: Bearer ${TOKEN}" -X POST --location $TRUSTY_ROUTE/metrics/group/fairness/spd/request \
     --header 'Content-Type: application/json' \
     --data "{
                 \"modelId\": \"demo-loan-nn-onnx-beta\",
                 \"protectedAttribute\": \"Is Male-Identifying?\",
                 \"privilegedAttribute\": 1.0,
                 \"unprivilegedAttribute\": 0.0,
                 \"outcomeName\": \"Will Default?\",
                 \"favorableOutcome\": 0,
                 \"batchSize\": 5000
             }"
echo</code></pre>
</div>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>These commands return the IDs of the created requests, which you can use later to delete these scheduled requests.</p>
</div>
<div class="olist arabic">
<div class="title">Verification</div>
<ol class="arabic">
<li>
<p>In the {openshift-platform} web console, click <strong>Observe</strong> &#8594; <strong>Metrics</strong>.</p>
</li>
<li>
<p>In the <strong>Expression</strong> field, enter <code>trustyai_spd</code> and click <strong>Run Queries</strong>.</p>
</li>
<li>
<p>Optional: After running a query, you can select a time range and refresh interval.</p>
<div class="ulist">
<ul>
<li>
<p>From the <strong>Time range</strong> list, select 5 minutes.</p>
</li>
<li>
<p>From the <strong>Refresh interval</strong> list, select 15 seconds.</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
<div class="imageblock">
<div class="content">
<img src="images/initial_spd.png" alt="initial spd">
</div>
</div>
</div>
<div class="sect2">
<h3 id="t-bias-scheduling-an-identity-metric-request_bias-tutorial">Scheduling an identity metric request</h3>
<div class="paragraph">
<p>You can monitor the average values of various data fields over time to see the average ratio of loan-payback to loan-default predictions and the average ratio of male-identifying to non-male-identifying applicants. To monitor the average values, you create an identity metric request by sending a POST request to the <code>/metrics/identity/request</code> endpoint.</p>
</div>
<div class="ulist">
<div class="title">Procedure</div>
<ul>
<li>
<p>In a terminal window, run the following command from the directory containing the downloaded tutorial files (<code>odh-trustyai-demos-main</code>):</p>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>for model in "demo-loan-nn-onnx-alpha" "demo-loan-nn-onnx-beta"; do
  for field in "Is Male-Identifying?" "Will Default?"; do
      curl -sk -H "Authorization: Bearer ${TOKEN}" -X POST --location $TRUSTY_ROUTE/metrics/identity/request \
       --header 'Content-Type: application/json' \
       --data "{
                 \"columnName\": \"$field\",
                 \"batchSize\": 250,
                 \"modelId\": \"$model\"
               }"
	echo -e
  done
done</code></pre>
</div>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>The payload structure is as follows:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>columnName</code>: The name of the field to compute the averaging over.</p>
</li>
<li>
<p><code>batchSize</code>: The number of previous inferences to include in the average-value calculation.</p>
</li>
<li>
<p><code>modelId</code>: The name of the model to query.</p>
</li>
</ul>
</div>
<div class="olist arabic">
<div class="title">Verification</div>
<ol class="arabic">
<li>
<p>In the {openshift-platform} web console, click <strong>Observe</strong> &#8594; <strong>Metrics</strong>.</p>
</li>
<li>
<p>In the <strong>Expression</strong> field, enter <code>trustyai_identity</code> and click <strong>Run Queries</strong>.</p>
</li>
<li>
<p>Optional: After running a query, you can select a time range and refresh interval.</p>
<div class="ulist">
<ul>
<li>
<p>From the <strong>Time range</strong> list, select 5 minutes.</p>
</li>
<li>
<p>From the <strong>Refresh interval</strong> list, select 15 seconds.</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
<div class="imageblock">
<div class="content">
<img src="images/initial_identities.png" alt="initial identities">
</div>
</div>
</div>
<div class="sect2">
<h3 id="t-bias-simulating-real-world-data_bias-tutorial">Simulating real world data</h3>
<div class="paragraph">
<p>Now that you have scheduled your fairness and identify metric requests, you can send some "real world" data through your models to see if they remain fair.</p>
</div>
<div class="ulist">
<div class="title">Procedure</div>
<ul>
<li>
<p>In a terminal window, run the following command from the directory containing the downloaded tutorial files (<code>odh-trustyai-demos-main</code>):</p>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>for batch in "01" "02" "03" "04" "05" "06" "07" "08"; do
  ./2-BiasMonitoring/scripts/send_data_batch 2-BiasMonitoring/data/batch_$batch.json
  sleep 5
done</code></pre>
</div>
</div>
</li>
</ul>
</div>
<div class="ulist">
<div class="title">Verification</div>
<ul>
<li>
<p>In the {openshift-platform} web console, click <strong>Observe</strong> &#8594; <strong>Metrics</strong> and watch the SPD and identity metric request values change.</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="t-bias-reviewing-the-results_bias-tutorial">Reviewing the results</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_are_the_models_biased">Are the models biased?</h3>
<div class="imageblock">
<div class="content">
<img src="images/final_spd.png" alt="final spd">
</div>
</div>
<div class="paragraph">
<p>The two models have drastically different fairness levels when applied to real-world data. Model Alpha (blue) stayed within the "acceptably fair" range between -0.1 and 0.1, ending around 0.09. However, Model Beta (yellow) plummeted out of the fair range, ending at -0.274. This indicates that non-male-identifying applicants were 27% less likely to receive a favorable outcome from Model Beta compared to male-identifying applicants.</p>
</div>
<div class="paragraph">
<p>To explore this further, you can analyze your identity metrics, starting by looking at the inbound ratio of male-identifying to non-male-identifying applicants:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="images/final_male_ident.png" alt="final male ident">
</div>
</div>
<div class="paragraph">
<p>In the training data, the ratio between male and non-male was around 0.8, but in the real-world data, it dropped to 0, meaning all applicants were non-male. This is a strong indicator that the training data did not match the real-world data, which is likely to indicate poor or biased model performance.</p>
</div>
</div>
<div class="sect2">
<h3 id="_how_does_the_production_data_compare_to_the_training_data">How does the production data compare to the training data?</h3>
<div class="imageblock">
<div class="content">
<img src="images/final_default.png" alt="final default">
</div>
</div>
<div class="paragraph">
<p>Even though Model Alpha (green) was only exposed to non-male applicants, it still provided varying outcomes to the various applicants, predicting "will-default" in about 25% of cases. In contrast, Model Beta (purple) predicted "will-default" 100% of the time, meaning it predicted that every non-male applicant would default on their loan. This suggests that Model Beta is performing poorly on the real-world data or has encoded a systematic bias from its training, leading to the assumption that all non-male applicants will default.</p>
</div>
<div class="paragraph">
<p>These examples highlight the critical importance of monitoring bias in production. Models that are equally fair during training can perform very differently when applied to real-world data, with hidden biases emerging only in actual use. By using TrustyAI to detect these biases early, you can safeguard against the potential harm caused by biased models in production.</p>
</div>
</div>
</div>
</div>
</div>
<div id="footer">
<div id="footer-text">
Last updated 2024-09-20 16:42:12 -0400
</div>
</div>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  messageStyle: "none",
  tex2jax: {
    inlineMath: [["\\(", "\\)"]],
    displayMath: [["\\[", "\\]"]],
    ignoreClass: "nostem|nolatexmath"
  },
  asciimath2jax: {
    delimiters: [["\\$", "\\$"]],
    ignoreClass: "nostem|noasciimath"
  },
  TeX: { equationNumbers: { autoNumber: "none" } }
})
MathJax.Hub.Register.StartupHook("AsciiMath Jax Ready", function () {
  MathJax.InputJax.AsciiMath.postfilterHooks.Add(function (data, node) {
    if ((node = data.script.parentNode) && (node = node.parentNode) && node.classList.contains("stemblock")) {
      data.math.root.display = "block"
    }
    return data
  })
})
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script>
</body>
</html>