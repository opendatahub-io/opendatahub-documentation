:_module-type: PROCEDURE

[id="creating-a-project-for-lab-tuning_{context}"]
= Creating a project for LAB-tuning

[role='_abstract']
To run LAB-tuning, create a data science project in {productname-short} and set up its pipeline server. The data science project keeps your resources organized, and the pipeline server runs and manages each step of the LAB-tuning workflow.

.Prerequisites
* You have logged in to {productname-short}.
* You have the appropriate roles and permissions to create projects.
* You have an existing S3-compatible object storage bucket and you have configured write access to your S3 bucket on your storage account.
* If you are configuring a pipeline server for production pipeline workloads, you have an existing external MySQL or MariaDB database.
ifndef::upstream[]
** For an external MySQL database, your database must use at least MySQL version 5.x. However, {org-name} recommends that you use MySQL version 8.x. 
** For an external MariaDB database, your database must use MariaDB version 10.3 or later. However, {org-name} recommends that you use at least MariaDB version 10.5.
* Your cluster administrator has configured LAB-tuning in your cluster, as described in link:{rhoaidocshome}{default-format-url}/enabling_lab-tuning/index[Enabling LAB-tuning].
endif::[]
ifdef::upstream[]
** For an external MySQL database, your database must use at least MySQL version 5.x. However, MySQL version 8.x is recommended.
** For an external MariaDB database, your database must use MariaDB version 10.3 or later. However, MariaDB version 10.5 is recommended.
* Your cluster administrator has configured LAB-tuning in your cluster, as described in link:{odhdocshome}/customizing-models-with-lab-tuning/#enabling-lab-tuning_lab-tuning[Enabling LAB-tuning].
endif::[]

.Procedure
. From the {productname-short} dashboard, click *Data science projects*.
+
The *Data science projects* page opens.
. Create a data science project:
.. Click *Create project*.
+
The *Create project* form opens.
.. For *Name*, enter a unique display name for your project.
.. Optional: If you want to change the default resource name for your project, click *Edit resource name*. 
+
The resource name is how your resource is labeled in {openshift-platform}.
Valid characters include lowercase letters, numbers, and hyphens (-).
The resource name cannot exceed 30 characters, and it must start with a letter and end with a letter or number.
+
*Note:* You cannot change the resource name after the project is created.
You can edit only the display name and the description.
.. Optional: In the *Description* field, provide a project description.

.. Click *Create*.
+
The project details page opens. 
. Create a connection that links an object storage bucket to your data science project for saving pipeline artifacts:
.. Click the *Connections* tab, and then click *Create connection*.
+
The *Create connection* form opens.
.. For *Connection type* select *S3 compatible object storage - v1*.
ifndef::upstream[]
. Complete the *Connection details*. For more information, see link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects/using-connections_projects#adding-a-connection-to-your-data-science-project_projects[Adding a connection to your data science project].
endif::[]
ifdef::upstream[]
. Complete the *Connection details*. For more information, see link:{odhdocshome}/working-on-data-science-projects/#adding-a-connection-to-your-data-science-project_projects[Adding a connection to your data science project].
endif::[]
.. Click *Create*.
+
The new connection is displayed on the *Connections* tab for the project.
. Configure a pipeline server to run and track the InstructLab pipeline:
.. Click the *Pipelines* tab, and then click *Configure pipeline server*.
+
The *Configure pipeline server* form opens.
.. In the *Object storage connection* section, click *Autofill from connection* and then select the connection you just created.
+
The form populates with credentials for the connection.
.. Click *Advanced settings* to display the *Database*, *Install preconfigured pipelines*, *Pipeline definition storage*, and *Pipeline caching* sections. 

.. In the *Database* section, select one of the following options to specify where to store your pipeline metadata and run information:
+
* *Default database on the cluster*: For development and testing purposes only. 
ifndef::upstream[]
* *External MySQL database*: For production pipeline workloads. For more information, see link:{rhoaidocshome}{default-format-url}/working_with_data_science_pipelines/managing-data-science-pipelines_ds-pipelines#configuring-a-pipeline-server_ds-pipelines[Configuring a pipeline server].
endif::[]
ifdef::upstream[]
* *External MySQL database*: For production pipeline workloads. For more information, see link:{odhdocshome}/working-with-data-science-pipelines/#configuring-a-pipeline-server_ds-pipelines[Configuring a pipeline server].
endif::[]

.. In the *Install preconfigured pipelines* section, select the *InstructLab pipeline* checkbox.
+
This installs the InstructLab pipeline on your project, allowing you to customize your model with LAB-tuning. 
+
[IMPORTANT]
====
{productname-short} automatically updates the InstructLab pipeline. To disable automatic updates, go to the *Pipelines* tab of your data science project, click the drop-down arrow next to *Import pipeline*, select *Manage preconfigured pipelines*, clear the *InstructLab pipeline* checkbox, and click *Apply*.
====

.. Optional: By default, pipeline definitions are stored as Kubernetes resources, enabling version control, GitOps workflows, and integration with OpenShift GitOps or similar tools. To store pipeline definitions in the internal database instead, clear the *Store pipeline definitions in Kubernetes* checkbox in the *Pipeline definition storage* section.

.. Optional: By default, caching is configurable at both the pipeline and task levels. To disable caching for all pipelines and tasks in the pipeline server and override any pipeline-level and task-level caching settings, clear the *Allow caching to be configured per pipeline and task* checkbox in the *Pipeline caching* section.

.. Click *Configure pipeline server*.

.Verification

* The *InstructLab* pipeline appears on the *Pipelines* tab for your data science project.

[role='_additional-resources']
.Additional resources
ifndef::upstream[]
* link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects/using-data-science-projects_projects#creating-a-data-science-project_projects[Creating a data science project]
* link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects/using-connections_projects#adding-a-connection-to-your-data-science-project_projects[Adding a connection to your data science project]
* link:{rhoaidocshome}{default-format-url}/working_with_data_science_pipelines/managing-data-science-pipelines_ds-pipelines#configuring-a-pipeline-server_ds-pipelines[Configuring a pipeline server]
* link:{rhoaidocshome}{default-format-url}/working_with_data_science_pipelines/managing-data-science-pipelines_ds-pipelines#overview-of-data-science-pipelines-caching_ds-pipelines[Overview of data science pipelines caching]
endif::[]
ifdef::upstream[]
* link:{odhdocshome}/working-on-data-science-projects/#creating-a-data-science-project_projects[Creating a data science project]
* link:{odhdocshome}/working-on-data-science-projects/#adding-a-connection-to-your-data-science-project_projects[Adding a connection to your data science project]
* link:{odhdocshome}/working-with-data-science-pipelines/#configuring-a-pipeline-server_ds-pipelines[Configuring a pipeline server]
* link:{odhdocshome}/working-with-data-science-pipelines/#overview-of-data-science-pipelines-caching_ds-pipelines[Overview of data science pipelines caching]
endif::[]

