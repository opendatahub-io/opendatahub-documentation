
:_module-type: REFERENCE

[id='customizable-model-serving-runtime-parameters_{context}']
= Customizable model serving runtime parameters

[role='_abstract']
You can modify the parameters of an existing model serving runtime to suit your deployment needs.

For more information about parameters for each of the supported serving runtimes, see the following table:

|===
| Serving runtime | Resource 

| NVIDIA Triton Inference Server | link:https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/tensorrtllm_backend/docs/model_config.html?#model-configuration[NVIDIA Triton Inference Server: Model Parameters]
|OpenVINO Model Server | link:https://docs.openvino.ai/2024/openvino-workflow/model-server/ovms_docs_dynamic_input.html[OpenVINO Model Server Features: Dynamic Input Parameters]
|vLLM NVIDIA GPU ServingRuntime for KServe | vLLM: Engine Arguments +
OpenAI-Compatible Server
|vLLM AMD GPU ServingRuntime for KServe | vLLM: Engine Arguments +
OpenAI-Compatible Server
|vLLM Intel Gaudi Accelerator ServingRuntime for KServe | vLLM: Engine Arguments +
OpenAI-Compatible Server
|vLLM Spyre ppc64le ServingRuntime for KServe | link:https://docs.redhat.com/en/documentation/red_hat_ai_inference_server/latest/html/getting_started/inference-rhaiis-with-podman-spyre-power_getting-started#ibm-power-recommended-model-inference-settings_getting-started[Recommended model inference settings for IBM Power with IBM Spyre AI accelerators]
|===

[role='_additional-resources']
.Additional resources
ifdef::upstream[]
* link:{odhdocshome}/configuring-your-model-serving-platform/#customizing-parameters-serving-runtime_odh-admin[Customizing the parameters of a deployed model serving runtime]
endif::[]

ifndef::upstream[]
* link:{rhoaidocshome}{default-format-url}/configuring_your_model-serving_platform/customizing_model_deployments#customizing-parameters-serving-runtime_rhoai-admin[Customizing the parameters of a deployed model serving runtime]
endif::[]


