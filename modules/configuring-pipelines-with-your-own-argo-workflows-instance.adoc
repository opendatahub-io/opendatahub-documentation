:_module-type: PROCEDURE

[id="configuring-pipelines-with-your-own-argo-workflows-instance_{context}"]
= Configuring pipelines with your own Argo Workflows instance

[role="_abstract"]
You can configure {productname-short} to use an existing Argo Workflows instance instead of the embedded one included with AI pipelines. This configuration is useful if your {openshift-platform} cluster already includes a managed Argo Workflows instance and you want to integrate it with {productname-short} pipelines without conflicts. Disabling the embedded Argo Workflows controller allows cluster administrators to manage the lifecycles of {productname-short} and Argo Workflows independently.

[NOTE]
====
You cannot enable both the embedded Argo Workflows instance and your own Argo Workflows instance on the same cluster.
====

.Prerequisites
* You have cluster administrator privileges for your {openshift-platform} cluster.
* You have installed {productname-long}.

.Procedure
. Log in to the {openshift-platform} web console as a cluster administrator.
. In the {openshift-platform} console, click *Operators* â†’ *Installed Operators*.
. Search for the *{productname-long}* Operator, and then click the Operator name to open the Operator details page.
. Click the *Data Science Cluster* tab.
. Click the default instance name (for example, *default-dsc*) to open the instance details page.
. Click the *YAML* tab to show the instance specifications.
. Disable the embedded Argo Workflows controllers that are managed by the {productname-short} Operator:
.. In the `spec.components` section, set the value of the `managementState` field for the `aipipelines` component to `Managed`.
.. In the `spec.components.aipipelines` section, set the value of the `managementState` field for `argoWorkflowsControllers` to `Removed`, as shown in the following example:
+
.Example aipipelines specification
[source,yaml]
----
# ...
spec:
  components:
    aipipelines:
      argoWorkflowsControllers:
        managementState: Removed
      managementState: Managed
# ...
----
. Click *Save* to apply your changes.
. Install and configure a compatible version of Argo Workflows on your cluster. For compatible version information, see link:https://access.redhat.com/articles/rhoai-supported-configs-3.x[Supported Configurations for 3.x]. For installation information, see the link:https://argo-workflows.readthedocs.io/en/stable/installation/[Argo Workflows Installation documentation^].

.Verification
. On the *Details* tab of the `DataScienceCluster` instance (for example, *default-dsc*), verify that `AIPipelinesReady` has a *Status* of `True`.
. Verify that the `ds-pipeline-workflow-controller` pod does not exist:
.. Go to *Workloads* -> *Pods*.
.. Search for the `ds-pipeline-workflow-controller` pod.
.. Verify that this pod does not exist. The absence of this pod confirms that the embedded Argo Workflows controller is disabled.
