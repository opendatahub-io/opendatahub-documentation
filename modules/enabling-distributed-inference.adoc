:_module-type: PROCEDURE

[id="enabling-distributed-inference_{context}"]
= Enabling {llmd}

[role='_abstract']

This procedure describes how to create a custom resource (CR) for an `LLMInferenceService` resource. You replace the default `InferenceService` with the `LLMInferenceService`.

.Prerequisites

* You have enabled the model serving platform.
* You have access to an {openshift-platform} cluster running version 4.19.9 or later.
* {openshift-platform} Service Mesh v2 is not installed in the cluster.
* Your cluster administrator has created a `GatewayClass` and a `Gateway` named `openshift-ai-inference` in the `openshift-ingress` namespace as described in link:https://docs.redhat.com/en/documentation/openshift_container_platform/latest/html/ingress_and_load_balancing/configuring-ingress-cluster-traffic#ingress-gateway-api[Gateway API with {openshift-platform} Container Platform Networking]. 
+
[IMPORTANT] 
====
Review the link:https://docs.redhat.com/en/documentation/openshift_container_platform/latest/html/ingress_and_load_balancing/configuring-ingress-cluster-traffic#nw-ingress-gateway-api-deployment_ingress-gateway-api[Gateway API deployment topologies]. Only use shared Gateways across trusted namespaces.
====

* Your cluster administrator has installed the `LeaderWorkerSet` Operator in {openshift-platform}. For more information, see the link:https://docs.redhat.com/en/documentation/openshift_container_platform/latest/html/ai_workloads/leader-worker-set-operator[Leader Worker Set Operator documentation].
* If you are running {openshift-platform} on a bare metal cluster: Your cluster administrator has set up the MetalLB Operator to provision an external IP address for the `openshift-ai-inference` Gateway service with the type `LoadBalancer`. For more information, see link:https://docs.redhat.com/en/documentation/openshift_container_platform/latest/html/ingress_and_load_balancing/load-balancing-with-metallb[Load balancing with MetalLB]. Ensure that the LoadBalancer is configured as follows:
** Has a standard Kubernetes Service manifest.
** Has `type:LoadBalancer` in the `spec` section.

* You have enabled authentication as described in _Configuring authentication for {llmd}_.


.Procedure

. Log in to the {openshift-platform} console as a developer.

. Create the `LLMInferenceService` CR with the following information:
+
--
[source]
----
apiVersion: serving.kserve.io/v1alpha1
kind: LLMInferenceService
metadata:
  name: sample-llm-inference-service
spec:
  replicas: 2
  model:
    uri: hf://RedHatAI/Qwen3-8B-FP8-dynamic
    name: RedHatAI/Qwen3-8B-FP8-dynamic
  router:
    route: {}
    gateway: {}
    scheduler: {}
    template:
      containers:
      - name: main
        resources:
          limits:
            cpu: '4'
            memory: 32Gi
            nvidia.com/gpu: "1"
          requests:
            cpu: '2'
            memory: 16Gi
            nvidia.com/gpu: "1"
----

Customize the following parameters in the `spec` section of the inference service:

* `replicas` - Specify the number of replicas.
* `model` - Provide the URI to the model based on how the model is stored (`uri`) and the model name to use in chat completion requests (`name`).
** S3 bucket:  `s3://<bucket-name>/<object-key>`
** Persistent volume claim (PVC): `pvc://<claim-name>/<pvc-path>`
** OCI container image: `oci://<registry_host>/<org_or_username>/<repository_name><tag_or_digest>`
** HuggingFace: `hf://<model>/<optional-hash>`
* `router` - Provide an HTTPRoute and gateway, or leave blank to automatically create one.
--

. Save the file.

== PD Disaggregation

`LLMInferenceService` supports a serving mode called Prefill-Decode (PD) Disaggregation. To enable PD disaggregation, add the `prefill` section to your CR:

[WARNING]
====
Because KV transfers are latency sensitive, a high-performance secondary network (for example, InfiniBand or RoCE) or NVLink is required for this deployment topology.
====

[NOTE]
====
Configuration for secondary networks is intentionally omitted from the following example.
====

[source]
----
apiVersion: serving.kserve.io/v1alpha1
kind: LLMInferenceService
metadata:
  name: sample-llm-inference-service-pd
spec:
  replicas: 2
  model:
    uri: hf://RedHatAI/Qwen3-8B-FP8-dynamic
    name: RedHatAI/Qwen3-8B-FP8-dynamic
  router:
    route: {}
    gateway: {}
    scheduler: {}
  template:
    containers:
    - name: main
      env:
        - name: VLLM_ADDITIONAL_ARGS
          value: "--kv_transfer_config '{\"kv_connector\":\"NixlConnector\",\"kv_role\":\"kv_both\"}'"
      resources:
        limits:
          cpu: '4'
          memory: 32Gi
          nvidia.com/gpu: "1"
        requests:
          cpu: '2'
          memory: 16Gi
          nvidia.com/gpu: "1"
  prefill:
    template:
      containers:
      - name: main
        env:
          - name: VLLM_ADDITIONAL_ARGS
            value: "--kv_transfer_config '{\"kv_connector\":\"NixlConnector\",\"kv_role\":\"kv_both\"}'"
        resources:
          limits:
            cpu: '4'
            memory: 32Gi
            nvidia.com/gpu: "1"
          requests:
            cpu: '2'
            memory: 16Gi
            nvidia.com/gpu: "1"
----

=== KV load failure recovery

vLLM supports automatic KV load failure recovery as a feature in its KV Connector API.

[WARNING]
====
As of vLLM 0.13, `kv_load_failure_policy` defaults to `recompute`, which results in a local prefill on the decode instance if there is a failure while pulling KV blocks from the prefill instance. In high-throughput serving scenarios, local prefills on a decode node can significantly impact decode throughput.
====

To disable automatic recovery and cause failures to propagate to the API server, set `kv_load_failure_policy` to `fail`:

[source]
----
- name: VLLM_ADDITIONAL_ARGS
  value: "--kv_transfer_config '{\"kv_connector\":\"NixlConnector\",\"kv_role\":\"kv_both\",\"kv_load_failure_policy\":\"fail\"}'"
----