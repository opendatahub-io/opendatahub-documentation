:_module-type: PROCEDURE

[id='adding-a-model-server-to-your-data-science-project_{context}']
= Adding a model server to your data science project

[role='_abstract']
Before you can successfully deploy a data science model on {productname-short}, you must configure a model server. This includes configuring the number of replicas to deploy, the server size, the token authorization, and how to access the project. If you require extra power for use with large datasets, you can assign accelerators to your model server to optimize performance. 

.Prerequisites
* You have logged in to {productname-long}.
ifndef::upstream[]
* If you use specialized {productname-short} groups, you are part of the user group or admin group (for example, `rhods-users` or `rhods-admin` ) in OpenShift.
endif::[]
ifdef::upstream[]
* If you use specialized {productname-short} groups, you are part of the user group or admin group (for example, `odh-users` or `odh-admins`) in OpenShift.
endif::[]
* You have created a data science project that you can add a model server to.
ifndef::upstream[]
* If you want to use a custom model-serving runtime for your model server, you have added and enabled the runtime. See link:{rhodsdocshome}{default-format-url}/working_on_data_science_projects/working-on-data-science-projects_nb-server#adding-a-custom-model-serving-runtime_nb-server[Adding a custom model-serving runtime].
* If you want to use graphics processing units (GPUs) with your model server, you have enabled GPU support in {productname-short}. See link:{rhodsdocshome}{default-format-url}/managing_resources/managing-cluster-resources_cluster-mgmt#enabling-gpu-support_cluster-mgmt[Enabling GPU support in {productname-short}].
endif::[]
ifdef::upstream[]
* If you want to use a custom model-serving runtime for your model server, you have added and enabled the runtime. See link:{odhdocshome}/working-on-data-science-projects/#adding-a-custom-model-serving-runtime_nb-server[Adding a custom model-serving runtime].
* If you want to use graphics processing units (GPUs) with your model server, you have enabled GPU support. This includes installing the Node Feature Discovery and GPU Operators. For more information, see https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/index.html[NVIDIA GPU Operator on {org-name} OpenShift Container Platform] in the NVIDIA documentation.
endif::[]

.Procedure
. From the {productname-short} dashboard, click *Data Science Projects*.
+
The *Data science projects* page opens.
. Click the name of the project that you want to configure a model server for.
+
A project details page opens.
. In the *Models and model servers* section, click *Add server*.
+
The *Add model server* dialog opens.
. In the *Model server name* field, enter a unique name for the model server.
. From the *Serving runtime* list, select a model-serving runtime that is installed and enabled in your {productname-short} deployment.
. In the *Number of model replicas to deploy* field, specify a value.
. From the *Model server size* list, select one of the following server sizes:
* Small
* Medium
* Large
* Custom
. Optional: If you selected *Custom* in the preceding step, configure the following settings in the *Model server size* section to customize your model server:
.. In the *CPUs requested* field, specify the number of CPUs to use with your model server. Use the list beside this field to specify the value in cores or millicores.
.. In the *CPU limit* field, specify the maximum number of CPUs to use with your model server. Use the list beside this field to specify the value in cores or millicores.
.. In the *Memory requested* field, specify the requested memory for the model server in gibibytes (Gi).
.. In the *Memory limit* field, specify the maximum memory limit for the model server in gibibytes (Gi).
+
[IMPORTANT]
====
{productname-short} includes two versions of the OpenVINO Model Server (OVMS) runtime by default; a version that supports GPUs and one that does not. To use GPUs, from the *Serving runtime* list, you must select the version whose display name includes `Supports GPUs`.

If you are using a _custom_ model-serving runtime with your model server, you must ensure that your custom runtime supports GPUs and is appropriately configured to use them.
====
. Optional: From the *Accelerator* list, select an accelerator. 
.. If you selected an accelerator in the preceding step, specify the number of accelerators to use.
. Optional: In the *Model route* section, select the *Make deployed models available through an external route* checkbox to make your deployed models available to external clients.
. Optional: In the *Token authorization* section, select the *Require token authentication* checkbox to require token authentication for your model server. To finish configuring token authentication, perform the following actions:
.. In the *Service account name* field, enter a service account name for which the token will be generated. The generated token is created and displayed in the *Token secret* field when the model server is configured.
.. To add an additional service account, click *Add a service account* and enter another service account name.
. Click *Add*.

.Verification
* The model server that you configured appears in the *Models and model servers* section of the project details page.

//[role="_additional-resources"]
//.Additional resources
