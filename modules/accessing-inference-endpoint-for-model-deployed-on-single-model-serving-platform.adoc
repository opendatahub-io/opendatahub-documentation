:_module-type: PROCEDURE

[id="accessing-inference-endpoint-for-deployed-model_{context}"]
= Accessing the inference endpoint for a deployed model

[role='_abstract']
To make inference requests to your deployed model, you must know how to access the inference endpoint that is available.

.Prerequisites
* You have logged in to {productname-long}.
ifndef::upstream[]
* If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {oai-user-group} or {oai-admin-group}) in OpenShift.
endif::[]
ifdef::upstream[]
* If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group} or {odh-admin-group}) in OpenShift.
endif::[]
* You have deployed a model by using the single-model serving platform.
* If you enabled token authorization for your deployed model, you have the associated token value.

.Procedure
. From the {productname-short} dashboard, click *Model Serving*.
+
The inference endpoint for the model is shown in the *Inference endpoint* field.
. Depending on what action you want to perform with the model (and if the model supports that action), copy the inference endpoint shown and then add one of the following paths to the end of the URL:
+
--
*Caikit TGIS ServingRuntime for KServe*

* `:443/api/v1/task/text-generation`
* `:443/api/v1/task/server-streaming-text-generation`
// * `:443/api/v1/task/text-classification`
// * `:443/api/v1/task/token-classification`

*TGIS Standalone ServingRuntime for KServe*

* `:443 fmaas.GenerationService/Generate`
* `:443 fmaas.GenerationService/GenerateStream`
+
NOTE: To query the endpoint for the TGIS standalone runtime, you must also download the files in the link:https://github.com/opendatahub-io/text-generation-inference/blob/main/proto[proto^] directory of the Open Data Hub `text-generation-inference` repository.

*OpenVINO Model Server*

* `/v2/models/<model-name>/infer`

*vLLM ServingRuntime for KServe*

* `:443/version`
* `:443/docs`
* `:443/v1/models`
* `:443/v1/chat/completions`
* `:443/v1/completions`
* `:443/v1/embeddings`
+
NOTE: The vLLM runtime is compatible with the OpenAI REST API and TGIS gRPC API. If you have deployed a model by using the TGIS Standalone runtime, you have the option to migrate to the vLLM runtime, if desired. To use the TGIS gRPC API to query the endpoints for a model deployed using the vLLM runtime, refer to the paths that are used for the TGIS Standalone runtime. For a list of models supported by the vLLM runtime, see link:https://docs.vllm.ai/en/latest/models/supported_models.html[Supported models].
+
NOTE: To use the embeddings inference endpoint in vLLM, you must use an embeddings model that is supported by vLLM. You cannot use the embeddings endpoint with generative models. For more information, see link:https://github.com/vllm-project/vllm/pull/3734[Supported embeddings models in vLLM].
+

As indicated by the paths shown, the single-model serving platform uses the HTTPS port of your OpenShift router (usually port 443) to serve external API requests.
--

. Use the endpoint to make API requests to your deployed model, as shown in the following example commands.

ifdef::upstream[]
+
--
*Caikit TGIS ServingRuntime for KServe*
[source,subs="+quotes"]
----
curl --json '{"model_id": "<model_name>", "inputs": "<text>"}' \
https://<inference_endpoint_url>:443/api/v1/task/server-streaming-text-generation \
-H 'Authorization: Bearer <token>' <1>
----
<1> You must add the `Authorization` header and specify a token value _only_ if you enabled token authorization when deploying the model.

*TGIS Standalone ServingRuntime for KServe*
[source]
----
grpcurl -proto text-generation-inference/proto/generation.proto -d \
'{"requests": [{"text":"<text>"}]}' \
-insecure <inference_endpoint_url>:443 fmaas.GenerationService/Generate \
-H 'Authorization: Bearer <token>' <1>
----
<1> You must add the `Authorization` header and specify a token value _only_ if you enabled token authorization when deploying the model.

*OpenVINO Model Server*
[source]
----
curl -ks <inference_endpoint_url>/v2/models/<model_name>/infer -d \
'{ "model_name": "<model_name>", \
"inputs": [{ "name": "<name_of_model_input>", "shape": [<shape>], "datatype": "<data_type>", "data": [<data>] }]}' \
-H 'Authorization: Bearer <token>' <1>
----
<1> You must add the `Authorization` header and specify a token value _only_ if you enabled token authorization when deploying the model.

*vLLM ServingRuntime for KServe*
[source]
----
curl -v https://<inference_endpoint_url>:443/v1/chat/completions -H \
"Content-Type: application/json" -d '{ \
"messages": [{ \
"role": "<role>", \
"content": "<content>" \
}] -H 'Authorization: Bearer <token>' <1>
----
<1> You must add the `Authorization` header and specify a token value _only_ if you enabled token authorization when deploying the model.
--
endif::[]
ifdef::self-managed,cloud-service[]

+
--
*Caikit TGIS ServingRuntime for KServe*
[source]
----
curl --json '{"model_id": "<model_name>", "inputs": "<text>"}' https://<inference_endpoint_url>:443/api/v1/task/server-streaming-text-generation -H 'Authorization: Bearer <token>'  <1>
----
<1> You must add the `Authorization` header and specify a token value _only_ if you enabled token authorization when deploying the model.

*TGIS Standalone ServingRuntime for KServe*
[source]
----
grpcurl -proto text-generation-inference/proto/generation.proto -d '{"requests": [{"text":"<text>"}]}' -H 'Authorization: Bearer <token>' -insecure <inference_endpoint_url>:443 fmaas.GenerationService/Generate  <1>
----
<1> You must add the `Authorization` header and specify a token value _only_ if you enabled token authorization when deploying the model.

*OpenVINO Model Server*
[source]
----
curl -ks <inference_endpoint_url>/v2/models/<model_name>/infer -d '{ "model_name": "<model_name>", "inputs": [{ "name": "<name_of_model_input>", "shape": [<shape>], "datatype": "<data_type>", "data": [<data>] }]}' -H 'Authorization: Bearer <token>'  <1>
----
<1> You must add the `Authorization` header and specify a token value _only_ if you enabled token authorization when deploying the model.

*vLLM ServingRuntime for KServe*
[source]
----
curl -v https://<inference_endpoint_url>:443/v1/chat/completions -H "Content-Type: application/json" -d '{ "messages": [{ "role": "<role>", "content": "<content>" }] -H 'Authorization: Bearer <token>' <1>
----
<1> You must add the `Authorization` header and specify a token value _only_ if you enabled token authorization when deploying the model.
--
endif::[]

[role='_additional-resources']
.Additional resources
* link:https://github.com/IBM/text-generation-inference[Text Generation Inference Server (TGIS)^]
* link:https://caikit.readthedocs.io/en/latest/autoapi/caikit/index.html[Caikit API documentation^]
* link:https://docs.openvino.ai/2023.3/ovms_docs_rest_api_kfs.html[OpenVINO KServe-compatible REST API documentation^]
* link:https://platform.openai.com/docs/api-reference/introduction[OpenAI API documentation]
