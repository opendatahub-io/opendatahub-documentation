:_module-type: PROCEDURE

[id='adding-a-model-server-for-the-multi-model-serving-platform_{context}']
= Adding a model server for the multi-model serving platform

[role='_abstract']
When you have enabled the multi-model serving platform, you must configure a model server to deploy models. If you require extra computing power for use with large datasets, you can assign accelerators to your model server.

ifdef::self-managed[]
[NOTE]
====
In {productname-short} {vernum}, {org-name} supports only NVIDIA and AMD GPU accelerators for model serving.
====
endif::[]
ifdef::cloud-service[]
[NOTE]
====
In {productname-short}, {org-name} supports only NVIDIA and AMD GPU accelerators for model serving.
====
endif::[]

.Prerequisites
* You have logged in to {productname-long}.
ifndef::upstream[]
* If you use {productname-short} groups, you are part of the user group or admin group (for example, {oai-user-group} or {oai-admin-group} ) in OpenShift.
endif::[]
ifdef::upstream[]
* If you use {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group} or {odh-admin-group}) in OpenShift.
endif::[]
* You have created a data science project that you can add a model server to.
* You have enabled the multi-model serving platform.
ifndef::upstream[]
* If you want to use a custom model-serving runtime for your model server, you have added and enabled the runtime. See link:{rhoaidocshome}{default-format-url}/serving_models/serving-small-and-medium-sized-models_model-serving#adding-a-custom-model-serving-runtime-for-the-multi-model-serving-platform_model-serving[Adding a custom model-serving runtime].
* If you want to use graphics processing units (GPUs) with your model server, you have enabled GPU support in {productname-short}. If you use NVIDIA GPUs, see link:{rhoaidocshome}{default-format-url}/managing_openshift_ai/enabling_accelerators#enabling-nvidia-gpus_managing-rhoai[Enabling NVIDIA GPUs^]. If you use AMD GPUs, see link:{rhoaidocshome}{default-format-url}/managing_openshift_ai/enabling_accelerators#amd-gpu-integration_managing-rhoai[AMD GPU integration^].
endif::[]
ifdef::upstream[]
* If you want to use a custom model-serving runtime for your model server, you have added and enabled the runtime. See link:{odhdocshome}/serving-models/#adding-a-custom-model-serving-runtime-for-the-multi-model-serving-platform_model-serving[Adding a custom model-serving runtime].
* If you want to use graphics processing units (GPUs) with your model server, you have enabled GPU support. This includes installing the Node Feature Discovery and NVIDIA GPU and AMD GPU Operators. For more information, see https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/index.html[NVIDIA GPU Operator on {org-name} OpenShift Container Platform^] in the NVIDIA documentation.
endif::[]

.Procedure
. In the left menu of the {productname-short} dashboard, click *Data science projects*.
+
The *Data science projects* page opens.
. Click the name of the project that you want to configure a model server for.
+
A project details page opens.

. Click the *Models* tab.
. Perform one of the following actions:
+
--
* If you see a *â€‹Multi-model serving platform* tile, click *Add model server* on the tile.
* If you do not see any tiles, click the *Add model server* button.
--
+
The *Add model server* dialog opens.
. In the *Model server name* field, enter a unique name for the model server.
. From the *Serving runtime* list, select a model-serving runtime that is installed and enabled in your {productname-short} deployment.
+
[NOTE]
====
If you are using a _custom_ model-serving runtime with your model server and want to use GPUs, you must ensure that your custom runtime supports GPUs and is appropriately configured to use them.
====
. In the *Number of model replicas to deploy* field, specify a value.
. From the *Model server size* list, select a value.
. Optional: If you selected *Custom* in the preceding step, configure the following settings in the *Model server size* section to customize your model server:
.. In the *CPUs requested* field, specify the number of CPUs to use with your model server. Use the list beside this field to specify the value in cores or millicores.
.. In the *CPU limit* field, specify the maximum number of CPUs to use with your model server. Use the list beside this field to specify the value in cores or millicores.
.. In the *Memory requested* field, specify the requested memory for the model server in gibibytes (Gi).
.. In the *Memory limit* field, specify the maximum memory limit for the model server in gibibytes (Gi).
. Optional: From the *Accelerator* list, select an accelerator.
.. If you selected an accelerator in the preceding step, specify the number of accelerators to use.
. Optional: In the *Model route* section, select the *Make deployed models available through an external route* checkbox to make your deployed models available to external clients.
. Optional: In the *Token authentication* section, select the *Require token authentication* checkbox to require token authentication for your model server. To finish configuring token authentication, perform the following actions:
.. In the *Service account name* field, enter a service account name for which the token will be generated. The generated token is created and displayed in the *Token secret* field when the model server is configured.
.. To add an additional service account, click *Add a service account* and enter another service account name.
. Click *Add*.
+
* The model server that you configured appears on the *Models* tab for the project, in the *Models and model servers* list.
. Optional: To update the model server, click the action menu (*&#8942;*) beside the model server and select *Edit model server*.

//[role="_additional-resources"]
//.Additional resources
