:_module-type: PROCEDURE

[id='installing-trustyai-service-using-cli_{context}']
= Installing the TrustyAI service by using the CLI

[role='_abstract']
You can use the OpenShift command-line interface (CLI) to install an instance of the TrustyAI service.

.Prerequisites

* You have cluster administrator privileges for your {openshift-platform} cluster.

* You have downloaded and installed the OpenShift command-line interface (CLI). See link:https://docs.redhat.com/en/documentation/openshift_container_platform/{ocp-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli[Installing the OpenShift CLI^].

ifndef::upstream[]
* You have configured monitoring for the model serving platform, as described in link:{rhoaidocshome}{default-format-url}/monitoring_data_science_models/configuring-trustyai_monitor#configuring-monitoring-for-the-multi-model-serving-platform_monitor[Configuring monitoring for the multi-model serving platform].

* You have enabled the TrustyAI component, as described in link:{rhoaidocshome}{default-format-url}/monitoring_data_science_models/configuring-trustyai_monitor#enabling-trustyai-component_monitor[Enabling the TrustyAI component].

* If you are using TrustyAI with a database instead of PVC, you have configured TrustyAI to use the database, as described in link:{rhoaidocshome}{default-format-url}/monitoring_data_science_models/configuring-trustyai_monitor#configuring-trustyai-with-a-database_monitor[Configuring TrustyAI with a database].

* The data scientist has created a data science project, as described in link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects/using-data-science-projects_projects#creating-a-data-science-project_projects[Creating a data science project], that contains the models that the data scientist wants to monitor.  
endif::[]

ifdef::upstream[]
* You have configured monitoring for the model serving platform, as described in link:{odhdocshome}/monitoring-data-science-models/#configuring-monitoring-for-the-multi-model-serving-platform_monitor[Configuring monitoring for the multi-model serving platform].

* You have enabled the TrustyAI component, as described in link:{odhdocshome}/monitoring-data-science-models/#enabling-trustyai-component_monitor[Enabling the TrustyAI component].

* If you are using TrustyAI with a database instead of PVC, you have configured TrustyAI to use the database, as described in link:{odhdocshome}/monitoring-data-science-models/#configuring-trustyai-with-a-database_monitor[Configuring TrustyAI with a database].

* The data scientist has created a data science project, as described in link:{odhdocshome}/working-on-data-science-projects/#creating-a-data-science-project_projects[Creating a data science project], that contains the models that the data scientist wants to monitor.  
endif::[]

.Procedure
. Open a new terminal window.
. Follow these steps to log in to your {openshift-platform} cluster as a cluster administrator:
.. In the {openshift-platform} web console, click your user name and select *Copy login command*. 
.. After you have logged in, click *Display token*.
.. Copy the *Log in with this token* command and paste it in the OpenShift command-line interface (CLI).
+
[source,subs="+quotes"]
----
$ oc login --token=__<token>__ --server=__<openshift_cluster_url>__
----

. Navigate to the data science project that contains the models that the data scientist wants to monitor. 
+
[source,subs="+quotes"]
----
oc project __<project_name>__
----
+
For example:
+
----
oc project my-project
----

. Create a `TrustyAIService` custom resource (CR) file, for example `trustyai_crd.yaml`: 
+
.Example CR file for TrustyAI using a database
[source,subs="+quotes"]
----
apiVersion: trustyai.opendatahub.io/v1alpha1
kind: TrustyAIService
metadata:
  name: trustyai-service <1>
spec:
  storage:
	  format: "DATABASE" <2>
	  size: "1Gi" <3>
	  databaseConfigurations: __<database_secret_credentials>__ <4>
  metrics:
  	schedule: "5s" <5>
----
+
<1> The name of the TrustyAI service instance.
<2> The storage format for the data, either `DATABASE` or `PVC` (persistent volume claim). {org-name} recommends that you use a database setup for better scalability, performance, and data management in TrustyAI.
<3> The size of the data to request.
ifndef::upstream[]
<4> The name of the secret with your database credentials that you created in link:{rhoaidocshome}{default-format-url}/monitoring_data_science_models/configuring-trustyai_monitor#configuring-trustyai-with-a-database_monitor[Configuring TrustyAI with a database]. For example, `db-credentials`.
endif::[]
ifdef::upstream[]
<4> The name of the secret with your database credentials that you created in link:{odhdocshome}/monitoring-data-science-models/#configuring-trustyai-with-a-database_monitor[Configuring TrustyAI with a database]. For example, `db-credentials`.
endif::[]
<5> The interval at which to calculate the metrics. The default is `5s`. The duration is specified with the ISO-8601 format. For example, `5s` for 5 seconds, `5m` for 5 minutes, and `5h` for 5 hours.

+

.Example CR file for TrustyAI using a PVC
[source,subs="+quotes"]
----
apiVersion: trustyai.opendatahub.io/v1alpha1
kind: TrustyAIService
metadata:
  name: trustyai-service <1>
spec:
  storage:
	  format: "PVC" <2>
	  folder: "/inputs" <3>
	  size: "1Gi" <4>
  data:
	  filename: "data.csv" <5>
	  format: "CSV" <6>
  metrics:
  	schedule: "5s" <7>
  	batchSize: 5000 <8>
----
+

<1> The name of the TrustyAI service instance.
<2> The storage format for the data, either `DATABASE` or `PVC` (persistent volume claim).
<3> The location within the PVC where you want to store the data.
<4> The size of the PVC to request.
<5> The suffix for the stored data files.
<6> The format of the data. Currently, only comma-separated value (CSV) format is supported.
<7> The interval at which to calculate the metrics. The default is `5s`. The duration is specified with the ISO-8601 format. For example, `5s` for 5 seconds, `5m` for 5 minutes, and `5h` for 5 hours.
<8> (Optional) The observation's historical window size to use for metrics calculation. The default is `5000`, which means that the metrics are calculated using the 5,000 latest inferences.

. Add the TrustyAI service's CR to your project:
+
----
oc apply -f trustyai_crd.yaml
----
+
This command returns output similar to the following:
+
----
trusty-service created
----


.Verification

Verify that you installed the TrustyAI service:

----
oc get pods | grep trustyai 
----

You should see a response similar to the following:

----
trustyai-service-5d45b5884f-96h5z             1/1     Running
----
