:_module-type: PROCEDURE

[id="deploying-models-on-the-NVIDIA-NIM-model-serving-platform_{context}"]
= Deploying models on the NVIDIA NIM model serving platform

[role='_abstract']
When you have enabled the *NVIDIA NIM model serving platform*, you can start to deploy NVIDIA-optimized models on the platform.

.Prerequisites
* You have logged in to {productname-long}.
ifndef::upstream[]
* If you are using {productname-short} groups, you are part of the user group or admin group (for example, {oai-user-group} or {oai-admin-group}) in OpenShift.
endif::[]
ifdef::upstream[]
* If you are using {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group} or {odh-admin-group}) in OpenShift.
endif::[]
* You have enabled the *NVIDIA NIM model serving platform*.
* You have created a data science project.
ifdef::upstream[]
* You have enabled support for graphic processing units (GPUs) in {productname-short}. This includes installing the Node Feature Discovery and NVIDIA GPU Operators. For more information, see https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/index.html[NVIDIA GPU Operator on {org-name} OpenShift Container Platform^].
endif::[]
ifndef::upstream[]
* You have enabled support for graphic processing units (GPUs) in {productname-short}. This includes installing the Node Feature Discovery operator and NVIDIA GPU Operators. For more information, see link:https://docs.redhat.com/en/documentation/openshift_container_platform/{ocp-latest-version}/html/specialized_hardware_and_driver_enablement/psap-node-feature-discovery-operator#installing-the-node-feature-discovery-operator_psap-node-feature-discovery-operator[Installing the Node Feature Discovery operator^] and link:{rhoaidocshome}{default-format-url}/managing_openshift_ai/enabling_accelerators#enabling-nvidia-gpus_managing-rhoai[Enabling NVIDIA GPUs^].
endif::[]

.Procedure
. In the left menu, click *Data science projects*.
+
The *Data science projects* page opens.
. Click the name of the project that you want to deploy a model in.
+
A project details page opens.
. Click the *Models* tab.
. In the *Models* section, perform one of the following actions:
+
-- 
* On the *​​NVIDIA NIM model serving platform* tile, click *Select NVIDIA NIM* on the tile, and then click *Deploy model*.
* If you have previously selected the NVIDIA NIM model serving type, the *Models* page displays *NVIDIA model serving enabled* on the upper-right corner, along with the *Deploy model* button. To proceed, click *Deploy model*.
--
The *Deploy model* dialog opens.
. Configure properties for deploying your model as follows:
.. In the *Model deployment name* field, enter a unique name for the deployment.
.. From the *NVIDIA NIM* list, select the NVIDIA NIM model that you want to deploy. For more information, see link:https://docs.nvidia.com/nim/large-language-models/latest/supported-models.html[Supported Models^]
.. In the *NVIDIA NIM storage size* field, specify the size of the cluster storage instance that will be created to store the NVIDIA NIM model.
.. In the *Number of model server replicas to deploy* field, specify a value.
.. From the *Model server size* list, select a value.
.. From the *Accelerator* list, select an accelerator.
+
The *Number of accelerators* field appears.
.. In the *Number of accelerators* field, specify the number of accelerators to use. The default value is 1.
. Optional: In the *Model route* section, select the *Make deployed models available through an external route* checkbox to make your deployed models available to external clients.
. To require token authentication for inference requests to the deployed model, perform the following actions:
.. Select *Require token authentication*.
.. In the *Service account name* field, enter the service account name that the token will be generated for.
.. To add an additional service account, click *Add a service account* and enter another service account name.
. Click *Deploy*.

.Verification
* Confirm that the deployed model is shown on the *Models* tab for the project, and on the *Model Serving* page of the dashboard with a checkmark in the *Status* column.

[role="_additional-resources"]
.Additional resources

* link:https://docs.nvidia.com/nim/large-language-models/latest/api-reference.html[NVIDIA NIM API reference^]
* link:https://docs.nvidia.com/nim/large-language-models/latest/supported-models.html[Supported Models^]
