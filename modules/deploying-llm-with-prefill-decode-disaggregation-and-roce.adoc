:_module-type: PROCEDURE

[id="deploying-llm-with-prefill-decode-disaggregation-and-roce_{context}"]
= Deploying models with prefill/decode disaggregation using RoCE

[role='_abstract']
Deploy large language models using prefill/decode (P/D) disaggregation with RoCE networking to optimize inference performance and resource utilization.

Prefill/decode disaggregation separates the initial token generation, or prefill, from subsequent token generation, or decode, into separate GPU pools. RoCE enables high-speed KV cache transfers between the prefill and decode stages, improving throughput and latency.

.Prerequisites

ifdef::upstream[]
* You have configured RoCE networking for distributed LLM deployments. For more information, see link:{odhdocshome}/working-with-distributed-workloads#configuring-roce-networking-for-distributed-llm-deployments[Configuring RoCE networking for distributed LLM deployments].
endif::[]
ifndef::upstream[]
* You have configured RoCE networking for distributed LLM deployments.
endif::[]
* You have verified that RoCE networking is functioning correctly.
* You have installed Distributed Inference Server with llm-d.
* You have at least 4 GPUs available across your cluster: 2 for prefill, 2 for decode recommended.

.Procedure

. Create a ConfigMap for the disaggregation configuration:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: disaggregation-config
  namespace: <your-namespace>
data:
  config.yaml: |
    disaggregation:
      enabled: true
      prefill:
        replica_count: 2
        gpu_count_per_replica: 1
        max_batch_size: 128
      decode:
        replica_count: 2
        gpu_count_per_replica: 1
        max_batch_size: 256
    kv_cache:
      transfer_method: rdma
      rdma_device: mlx5_0
    nccl:
      ib_disable: 0
      net_gdr_level: 5
----

where:

kv_cache.transfer_method:: Specifies the transfer method for KV cache between prefill and decode stages. Use RDMA for high-performance transfers.
kv_cache.rdma_device:: Specifies the RDMA device name. Use `ibv_devices` to list available devices.

. Create an LLMInferenceService with disaggregation enabled:
+
[source,yaml]
----
apiVersion: serving.kserve.io/v1alpha1
kind: LLMInferenceService
metadata:
  name: llm-disaggregated
  namespace: <your-namespace>
  annotations:
    k8s.v1.cni.cncf.io/networks: roce-network
spec:
  model:
    uri: hf://meta-llama/Llama-2-70b-hf
    name: llama-2-70b
  disaggregation:
    enabled: true
    prefillPool:
      replicas: 2
      template:
        spec:
          containers:
          - name: vllm
            image: quay.io/opendatahub/vllm:latest
            env:
            - name: VLLM_DISAGGREGATION_ROLE
              value: "prefill"
            - name: VLLM_KV_CACHE_TRANSFER
              value: "rdma"
            - name: NCCL_IB_DISABLE
              value: "0"
            - name: NCCL_NET_GDR_LEVEL
              value: "5"
            resources:
              limits:
                nvidia.com/gpu: "1"
                rdma/roce: "1"
            volumeMounts:
            - name: rdma-device
              mountPath: /dev/infiniband
            - name: config
              mountPath: /config
          volumes:
          - name: rdma-device
            hostPath:
              path: /dev/infiniband
          - name: config
            configMap:
              name: disaggregation-config
    decodePool:
      replicas: 2
      template:
        spec:
          containers:
          - name: vllm
            image: quay.io/opendatahub/vllm:latest
            env:
            - name: VLLM_DISAGGREGATION_ROLE
              value: "decode"
            - name: VLLM_KV_CACHE_TRANSFER
              value: "rdma"
            - name: NCCL_IB_DISABLE
              value: "0"
            - name: NCCL_NET_GDR_LEVEL
              value: "5"
            resources:
              limits:
                nvidia.com/gpu: "1"
                rdma/roce: "1"
            volumeMounts:
            - name: rdma-device
              mountPath: /dev/infiniband
            - name: config
              mountPath: /config
          volumes:
          - name: rdma-device
            hostPath:
              path: /dev/infiniband
          - name: config
            configMap:
              name: disaggregation-config
  router:
    gateway: {}
    scheduler:
      disaggregation:
        enabled: true
----

where:

prefillPool:: Specifies the prefill pool configuration, which handles initial token generation with optimized batching.
decodePool:: Specifies the decode pool configuration, which handles subsequent token generation, receiving KV cache from prefill via RDMA.

. Apply the configuration:
+
[source,bash]
----
$ oc apply -f llm-disaggregated.yaml
----

. Monitor the deployment:
+
[source,bash]
----
$ oc get pods -l app=llm-disaggregated -w
----
+
Wait for all prefill and decode pods to reach the `Running` state.

.Verification

. Check that both prefill and decode pools are running:
+
[source,bash]
----
$ oc get pods -l serving.kserve.io/inferenceservice=llm-disaggregated
----
+
.Sample response
+
[source,text]
----
NAME                                      READY   STATUS    RESTARTS   AGE
llm-disaggregated-prefill-0               1/1     Running   0          5m
llm-disaggregated-prefill-1               1/1     Running   0          5m
llm-disaggregated-decode-0                1/1     Running   0          5m
llm-disaggregated-decode-1                1/1     Running   0          5m
llm-disaggregated-router-7d8f9b5c-xk2pl   1/1     Running   0          5m
----

. Verify RDMA communication in the logs:
+
[source,bash]
----
$ oc logs llm-disaggregated-prefill-0 | grep -i "kv.*rdma"
$ oc logs llm-disaggregated-decode-0 | grep -i "kv.*rdma"
----
+
You should see log messages indicating KV cache transfers are using RDMA.

. Send a test inference request:
+
[source,bash]
----
$ curl -X POST http://<inference-endpoint>/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama-2-70b",
    "messages": [
      {"role": "user", "content": "Write a short poem about machine learning"}
    ],
    "max_tokens": 200,
    "temperature": 0.7
  }'
----

. Monitor KV cache transfer metrics:
+
[source,bash]
----
$ oc exec llm-disaggregated-prefill-0 -- curl localhost:8000/metrics | grep kv_cache
----
+
Look for metrics showing:
+
* `kv_cache_transfer_bytes_total` - Total bytes transferred
* `kv_cache_transfer_duration_seconds` - Transfer latency
* `kv_cache_rdma_operations_total` - Number of RDMA operations

. Compare performance with and without disaggregation:
+
[source,bash]
----
# Run benchmark with disaggregation
$ python benchmark_inference.py --endpoint <disaggregated-endpoint> --requests 100

# Compare with non-disaggregated deployment
$ python benchmark_inference.py --endpoint <standard-endpoint> --requests 100
----
+
Disaggregated deployments typically show:
+
* Higher throughput for concurrent requests
* Better GPU utilization
* Lower latency for decode-heavy workloads

[role='_additional-resources']
.Additional resources

ifdef::upstream[]
* link:{odhdocshome}/deploying-models-using-distributed-inference[Deploying models using Distributed Inference Server with llm-d]
* link:{odhdocshome}/enabling-roce-for-distributed-llm-deployments[Enabling RoCE networking for distributed LLM deployments]
endif::[]
* link:https://docs.vllm.ai/en/latest/features/disaggregated_prefill.html[vLLM Disaggregated Prefill Documentation]
