:_module-type: PROCEDURE


[id="explore-the-data-processing-examples_{context}"]
= Explore the data processing examples

[role='_abstract']
To get started with data processing with Docling explore the provided examples.

*Prerequisites*

ifdef::upstream[]
* Install the data processing library as described in link:{odhdocshome}/customize-models-to-build-gen-ai-applications/#set-up-your-working-environment_custom-models[Set up your working environment].
endif::[]
ifndef::upstream[]
* Install the data processing library as described in link:{rhoaidocshome}{default-format-url}/customize_models_to_build_gen_ai_applications/set-up-your-working-environment_custom-models[Set up your working environment].
endif::[]

*Procedure*

. To access the data processing examples, clone the link:https://github.com/opendatahub-io/data-processing.git[data processing Git repository]:
+
ifdef::upstream[]
* To clone the https://github.com/opendatahub-io/data-processing.git repository from JupyterLab, follow the steps in link:{odhdocshome}/customize-models-to-build-gen-ai-applications/#clone-an-example-git-repository_custom-models[Clone an example Git repository] and specify the 3.0 branch.
endif::[]
ifndef::upstream[]
* To clone the https://github.com/opendatahub-io/data-processing.git repository from JupyterLab, follow the steps in link:{rhoaidocshome}{default-format-url}/customize_models_to_build_gen_ai_applications/set-up-your-working-environment_custom-models#clone-an-example-git-repository_custom-models[Clone an example Git repository] and specify the 3.0 branch.
endif::[]

* To create a local clone of the repository, run the following command:
+
[source, bash]
----
git clone https://github.com/opendatahub-io/data-processing -b stable-3.0
----
. Go to the link:https://github.com/opendatahub-io/data-processing/tree/main/notebooks[`notebooks`] directory to learn how to use Docling for the following tasks:
+
link:https://github.com/opendatahub-io/data-processing/tree/main/notebooks/use-cases[*Use cases*]

* Convert unstructured documents (PDF files) to structured format (Markdown) - with and without visionâ€‘language model (VLM)
* Chunk - Split documents into smaller, semantically meaningful pieces
* Information extraction - Use template formats to extract specific data fields from documents like invoices.
* Subset selection - Use this script or notebook to reduce the size of your dataset. The algorithm analyzes an input dataset and reduces it in size, while ensuring data diversity and coverage.

+
link:https://github.com/opendatahub-io/data-processing/tree/main/notebooks/tutorials[*Tutorials*] - An example notebook that provides a complete, end-to-end workflow for preparing a dataset of documents for a RAG (Retrieval-Augmented Generation) system.

*Additional resources*

* Docling community project: link:https://docling-project.github.io/docling/[https://docling-project.github.io/docling/]
* GitHub Repository for the Docling project source code: link:https://github.com/docling-project/docling[https://github.com/docling-project/docling]


