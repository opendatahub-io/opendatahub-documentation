
:_module-type: REFERENCE

[id='inference-endpoints_{context}']
= Inference endpoints

[role='_abstract']
These examples show how to use inference endpoints to query the model. 

[NOTE]
--
If you enabled token authorization when deploying the model, add the `Authorization` header and specify a token value.
--

== Caikit TGIS ServingRuntime for KServe

* `:443/api/v1/task/text-generation`
* `:443/api/v1/task/server-streaming-text-generation`

.Example command

ifdef::upstream[]
[source]
----
curl --json '{"model_id": "<model_name>", "inputs": "<text>"}' \
https://<inference_endpoint_url>:443/api/v1/task/server-streaming-text-generation \
-H 'Authorization: Bearer <token>'
----
endif::[]
ifdef::self-managed,cloud-service[]
[source]
----
curl --json '{"model_id": "<model_name__>", "inputs": "<text>"}' https://<inference_endpoint_url>:443/api/v1/task/server-streaming-text-generation -H 'Authorization: Bearer <token>'
----
endif::[]

== Caikit Standalone ServingRuntime for KServe

If you are serving multiple models, you can query `/info/models` or `:443 caikit.runtime.info.InfoService/GetModelsInfo` to view a list of served models.

--
.REST endpoints

* `/api/v1/task/embedding`
* `/api/v1/task/embedding-tasks`
* `/api/v1/task/sentence-similarity`
* `/api/v1/task/sentence-similarity-tasks`
* `/api/v1/task/rerank`
* `/api/v1/task/rerank-tasks`
* `/info/models`
* `/info/version`
* `/info/runtime`
--

--
.gRPC endpoints

* `:443 caikit.runtime.Nlp.NlpService/EmbeddingTaskPredict`
* `:443 caikit.runtime.Nlp.NlpService/EmbeddingTasksPredict`
* `:443 caikit.runtime.Nlp.NlpService/SentenceSimilarityTaskPredict`
* `:443 caikit.runtime.Nlp.NlpService/SentenceSimilarityTasksPredict`
* `:443 caikit.runtime.Nlp.NlpService/RerankTaskPredict`
* `:443 caikit.runtime.Nlp.NlpService/RerankTasksPredict`
* `:443 caikit.runtime.info.InfoService/GetModelsInfo`
* `:443 caikit.runtime.info.InfoService/GetRuntimeInfo`
--

ifdef::upstream[]
[NOTE]
--
By default, the Caikit Standalone Runtime exposes REST endpoints. To use gRPC protocol, manually deploy a custom Caikit Standalone ServingRuntime. For more information, see link:{odhdocshome}/serving-models/#adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models[Adding a custom model-serving runtime for the single-model serving platform]. 
--

An example manifest is available in the link:https://github.com/opendatahub-io/caikit-tgis-serving/blob/main/demo/kserve/custom-manifests/caikit/caikit-standalone/caikit-standalone-servingruntime-grpc.yaml[caikit-tgis-serving GitHub repository^].
endif::[]

ifndef::upstream[]
[NOTE]
--
By default, the Caikit Standalone Runtime exposes REST endpoints. To use gRPC protocol, manually deploy a custom Caikit Standalone ServingRuntime. For more information, see link:{rhoaidocshome}{default-format-url}/serving_models/serving-large-models_serving-large-models#adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models[Adding a custom model-serving runtime for the single-model serving platform].
--

An example manifest is available in the link:https://github.com/opendatahub-io/caikit-tgis-serving/blob/main/demo/kserve/custom-manifests/caikit/caikit-standalone/caikit-standalone-servingruntime-grpc.yaml[caikit-tgis-serving GitHub repository^].
endif::[]

.Example command
--
*REST*
[source]
----
curl -H 'Content-Type: application/json' -d '{"inputs": "<text>", "model_id": "<model_id>"}' <inference_endpoint_url>/api/v1/task/embedding -H 'Authorization: Bearer <token>'
----

*gRPC*
[source]
----
grpcurl -d '{"text": "<text>"}' -H \"mm-model-id: <model_id>\" <inference_endpoint_url>:443 caikit.runtime.Nlp.NlpService/EmbeddingTaskPredict -H 'Authorization: Bearer <token>'
----
--

== TGIS Standalone ServingRuntime for KServe

* `:443 fmaas.GenerationService/Generate`
* `:443 fmaas.GenerationService/GenerateStream`
+
[NOTE]
--
To query the endpoint for the TGIS standalone runtime, you must also download the files in the link:https://github.com/opendatahub-io/text-generation-inference/blob/main/proto[proto^] directory of the {productname-short} `text-generation-inference` repository.
--

.Example command

ifdef::upstream[]
[source]
----
grpcurl -proto text-generation-inference/proto/generation.proto -d \
'{"requests": [{"text":"<text>"}]}' \
-insecure <inference_endpoint_url>:443 fmaas.GenerationService/Generate \
-H 'Authorization: Bearer <token>'
----
endif::[]
ifdef::self-managed,cloud-service[]
[source]
----
grpcurl -proto text-generation-inference/proto/generation.proto -d '{"requests": [{"text":"<text>"}]}' -H 'Authorization: Bearer <token>' -insecure <inference_endpoint_url>:443 fmaas.GenerationService/Generate 
----
endif::[]

== OpenVINO Model Server

* `/v2/models/<model-name>/infer`

.Example command

ifdef::upstream[]
[source]
----
curl -ks <inference_endpoint_url>/v2/models/<model_name>/infer -d \
'{ "model_name": "<model_name>", \
"inputs": [{ "name": "<name_of_model_input>", "shape": [<shape>], "datatype": "<data_type>", "data": [<data>] }]}' \
-H 'Authorization: Bearer <token>'
----
endif::[]
ifdef::self-managed,cloud-service[]
[source]
----
curl -ks <inference_endpoint_url>/v2/models/<model_name>/infer -d '{ "model_name": "<model_name>", "inputs": [{ "name": "<name_of_model_input>", "shape": [<shape>], "datatype": "<data_type>", "data": [<data>] }]}' -H 'Authorization: Bearer <token>'
----
endif::[]

== vLLM ServingRuntime for KServe

* `:443/version`
* `:443/docs`
* `:443/v1/models`
* `:443/v1/chat/completions`
* `:443/v1/completions`
* `:443/v1/embeddings`
* `:443/tokenize`
* `:443/detokenize`
+
[NOTE]
====
* The vLLM runtime is compatible with the OpenAI REST API. For a list of models that the vLLM runtime supports, see link:https://docs.vllm.ai/en/latest/models/supported_models.html[Supported models].
* To use the embeddings inference endpoint in vLLM, you must use an embeddings model that the vLLM supports. You cannot use the embeddings endpoint with generative models. For more information, see link:https://github.com/vllm-project/vllm/pull/3734[Supported embeddings models in vLLM].
* As of vLLM v0.5.5, you must provide a chat template while querying a model using the `/v1/chat/completions` endpoint. If your model does not include a predefined chat template, you can use the `chat-template` command-line parameter to specify a chat template in your custom vLLM runtime, as shown in the example. Replace `<CHAT_TEMPLATE>` with the path to your template.
+
[source]
----
containers:
  - args:
      - --chat-template=<CHAT_TEMPLATE>
----
You can use the chat templates that are available as `.jinja` files link:https://github.com/opendatahub-io/vllm/tree/main/examples[here] or with the vLLM image under `/apps/data/template`. For more information, see link:https://huggingface.co/docs/transformers/main/chat_templating[Chat templates].
====
+
As indicated by the paths shown, the single-model serving platform uses the HTTPS port of your OpenShift router (usually port 443) to serve external API requests.

.Example command
ifdef::upstream[]
[source]
----
curl -v https://<inference_endpoint_url>:443/v1/chat/completions -H \
"Content-Type: application/json" -d '{ \
"messages": [{ \
"role": "<role>", \
"content": "<content>" \
}] -H 'Authorization: Bearer <token>'
----
endif::[]
ifdef::self-managed,cloud-service[]
[source]
----
curl -v https://<inference_endpoint_url>:443/v1/chat/completions -H "Content-Type: application/json" -d '{ "messages": [{ "role": "<role>", "content": "<content>" }] -H 'Authorization: Bearer <token>'
----
endif::[]

== vLLM ServingRuntime with Gaudi accelerators support for KServe 

ifndef::upstream[]
See link:{rhoaidocshome}{default-format-url}/serving_models/serving-large-models_serving-large-models#vllm_servingruntime_for_kserve[vLLM ServingRuntime for KServe^].
endif::[]
ifdef::upstream[]
See link:{odhdocshome}/serving_models/serving-large-models_serving-large-models#vllm_servingruntime_for_kserve[vLLM ServingRuntime for KServe^].
endif::[]

== NVIDIA Triton Inference Server

--
.REST endpoints
* `v2/models/[/versions/<model_version>]/infer`
* `v2/models/<model_name>[/versions/<model_version>]`
* `v2/health/ready`
* `v2/health/live`
* `v2/models/<model_name>[/versions/]/ready`
* `v2`
--

[NOTE]
====
ModelMesh does not support the following REST endpoints:

* `v2/health/live`
* `v2/health/ready`
* `v2/models/<model_name>[/versions/]/ready`
====

.Example command
ifndef::upstream[]
[source]
----
curl -ks <inference_endpoint_url>/v2/models/<model_name>/infer -d '{ "model_name": "<model_name>", "inputs": [{ "name": "<name_of_model_input>", "shape": [<shape>], "datatype": "<data_type>", "data": [<data>] }]}' -H 'Authorization: Bearer <token>'
----
endif::[]
ifdef::upstream[]
[source]
----
curl -ks <inference_endpoint_url>/v2/models/<model_name>/infer -d /
'{ "model_name": "<model_name>", \
   "inputs": \
	[{ "name": "<name_of_model_input>", \
           "shape": [<shape>], \
           "datatype": "<data_type>", \
           "data": [<data>] \
         }]}' -H 'Authorization: Bearer <token>'
----
endif::[]
--
.gRPC endpoints
* `:443 inference.GRPCInferenceService/ModelInfer`
* `:443 inference.GRPCInferenceService/ModelReady`
* `:443 inference.GRPCInferenceService/ModelMetadata`
* `:443 inference.GRPCInferenceService/ServerReady`
* `:443 inference.GRPCInferenceService/ServerLive`
* `:443 inference.GRPCInferenceService/ServerMetadata`
--

.Example command
ifdef::upstream[]
[source]
----
grpcurl -cacert ./openshift_ca_istio_knative.crt \
        -proto ./grpc_predict_v2.proto \
        -d @ \
        -H "Authorization: Bearer <token>" \
        <inference_endpoint_url>:443 \
        inference.GRPCInferenceService/ModelMetadata
----
endif::[]
ifndef::upstream[]
----
grpcurl -cacert ./openshift_ca_istio_knative.crt -proto ./grpc_predict_v2.proto -d @ -H "Authorization: Bearer <token>" <inference_endpoint_url>:443 inference.GRPCInferenceService/ModelMetadata
----
endif::[]

[role='_additional-resources']
== Additional resources
* link:https://github.com/IBM/text-generation-inference[Text Generation Inference Server (TGIS)^]
* link:https://caikit.readthedocs.io/en/latest/autoapi/caikit/index.html[Caikit API documentation^]
* link:https://github.com/caikit/caikit-nlp[Caikit NLP GitHub project^]
* link:https://docs.openvino.ai/2023.3/ovms_docs_rest_api_kfs.html[OpenVINO KServe-compatible REST API documentation^]
* link:https://platform.openai.com/docs/api-reference/introduction[OpenAI API documentation^]
* link:https://kserve.github.io/website/master/modelserving/data_plane/v2_protocol/[Open Inference Protocol]
ifdef::upstream[]
* link:{odhdocshome}/serving-models/#supported-runtimes_serving-large-models[Supported model-serving runtimes]
endif::[]
ifndef::upstream[]
* link:{rhoaidocshome}{default-format-url}/serving_models/serving-large-models_serving-large-models#supported-model-serving-runtimes_serving-large-models[Supported model-serving runtimes].
endif::[]
