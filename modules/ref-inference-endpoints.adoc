
:_module-type: REFERENCE

[id='inference-endpoints_{context}']
= Inference endpoints

[role='_abstract']
These examples show how to use inference endpoints to query the model.

Caikit TGIS ServingRuntime for KServe::

* `:443/api/v1/task/text-generation`
* `:443/api/v1/task/server-streaming-text-generation`

Caikit Standalone ServingRuntime for KServe::

If you are serving multiple models, you can query `/info/models` or `:443 caikit.runtime.info.InfoService/GetModelsInfo` to view a list of served models.
+
--
.REST endpoints

* `/api/v1/task/embedding`
* `/api/v1/task/embedding-tasks`
* `/api/v1/task/sentence-similarity`
* `/api/v1/task/sentence-similarity-tasks`
* `/api/v1/task/rerank`
* `/api/v1/task/rerank-tasks`
* `/info/models`
* `/info/version`
* `/info/runtime`
--
+
--
.gRPC endpoints

* `:443 caikit.runtime.Nlp.NlpService/EmbeddingTaskPredict`
* `:443 caikit.runtime.Nlp.NlpService/EmbeddingTasksPredict`
* `:443 caikit.runtime.Nlp.NlpService/SentenceSimilarityTaskPredict`
* `:443 caikit.runtime.Nlp.NlpService/SentenceSimilarityTasksPredict`
* `:443 caikit.runtime.Nlp.NlpService/RerankTaskPredict`
* `:443 caikit.runtime.Nlp.NlpService/RerankTasksPredict`
* `:443 caikit.runtime.info.InfoService/GetModelsInfo`
* `:443 caikit.runtime.info.InfoService/GetRuntimeInfo`
--

ifdef::upstream[]
[NOTE]
--
By default, the Caikit Standalone Runtime exposes REST endpoints. To use gRPC protocol, manually deploy a custom Caikit Standalone ServingRuntime. For more information, see link:{odhdocshome}/serving-models/#adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models[Adding a custom model-serving runtime for the single-model serving platform]. 
--

An example manifest is available in the link:https://github.com/opendatahub-io/caikit-tgis-serving/blob/main/demo/kserve/custom-manifests/caikit/caikit-standalone/caikit-standalone-servingruntime-grpc.yaml[caikit-tgis-serving GitHub repository^].
endif::[]

ifndef::upstream[]
[NOTE]
--
By default, the Caikit Standalone Runtime exposes REST endpoints. To use gRPC protocol, manually deploy a custom Caikit Standalone ServingRuntime. For more information, see link:{rhoaidocshome}{default-format-url}/serving_models/serving-large-models_serving-large-models#adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models[Adding a custom model-serving runtime for the single-model serving platform].
--

An example manifest is available in the link:https://github.com/opendatahub-io/caikit-tgis-serving/blob/main/demo/kserve/custom-manifests/caikit/caikit-standalone/caikit-standalone-servingruntime-grpc.yaml[caikit-tgis-serving GitHub repository^].
endif::[]

TGIS Standalone ServingRuntime for KServe::

* `:443 fmaas.GenerationService/Generate`
* `:443 fmaas.GenerationService/GenerateStream`
+
[NOTE]
--
To query the endpoint for the TGIS standalone runtime, you must also download the files in the link:https://github.com/opendatahub-io/text-generation-inference/blob/main/proto[proto^] directory of the {productname-short} `text-generation-inference` repository.
--

OpenVINO Model Server::

* `/v2/models/<model-name>/infer`

vLLM ServingRuntime for KServe::

* `:443/version`
* `:443/docs`
* `:443/v1/models`
* `:443/v1/chat/completions`
* `:443/v1/completions`
* `:443/v1/embeddings`
* `:443/tokenize`
* `:443/detokenize`
+
[NOTE]
--
The vLLM runtime is compatible with the OpenAI REST API. For a list of models that the vLLM runtime supports, see link:https://docs.vllm.ai/en/latest/models/supported_models.html[Supported models].
--
+
[NOTE]
--
To use the embeddings inference endpoint in vLLM, you must use an embeddings model that the vLLM supports. You cannot use the embeddings endpoint with generative models. For more information, see link:https://github.com/vllm-project/vllm/pull/3734[Supported embeddings models in vLLM].
--
+

As indicated by the paths shown, the single-model serving platform uses the HTTPS port of your OpenShift router (usually port 443) to serve external API requests.


== Example commands

[NOTE]
--
If you enabled token authorization when deploying the model, add the `Authorization` header and specify a token value.
--
ifdef::upstream[]

--
*Caikit TGIS ServingRuntime for KServe*
[source]
----
curl --json '{"model_id": "<model_name>", "inputs": "<text>"}' \
https://<inference_endpoint_url>:443/api/v1/task/server-streaming-text-generation \
-H 'Authorization: Bearer <token>'
----

*Caikit Standalone ServingRuntime for KServe*

.REST
[source]
----
curl -H 'Content-Type: application/json' -d '{"inputs": "<text>", "model_id": "<model_id>"}' <inference_endpoint_url>/api/v1/task/embedding -H 'Authorization: Bearer <token>'
----

.gRPC
[source]
----
grpcurl -d '{"text": "<text>"}' -H \"mm-model-id: <model_id>\" <inference_endpoint_url>:443 caikit.runtime.Nlp.NlpService/EmbeddingTaskPredict -H 'Authorization: Bearer <token>'
----

*TGIS Standalone ServingRuntime for KServe*
[source]
----
grpcurl -proto text-generation-inference/proto/generation.proto -d \
'{"requests": [{"text":"<text>"}]}' \
-insecure <inference_endpoint_url>:443 fmaas.GenerationService/Generate \
-H 'Authorization: Bearer <token>'
----

*OpenVINO Model Server*
[source]
----
curl -ks <inference_endpoint_url>/v2/models/<model_name>/infer -d \
'{ "model_name": "<model_name>", \
"inputs": [{ "name": "<name_of_model_input>", "shape": [<shape>], "datatype": "<data_type>", "data": [<data>] }]}' \
-H 'Authorization: Bearer <token>'
----

*vLLM ServingRuntime for KServe*
[source]
----
curl -v https://<inference_endpoint_url>:443/v1/chat/completions -H \
"Content-Type: application/json" -d '{ \
"messages": [{ \
"role": "<role>", \
"content": "<content>" \
}] -H 'Authorization: Bearer <token>'
----
--
endif::[]
ifdef::self-managed,cloud-service[]

--
*Caikit TGIS ServingRuntime for KServe*
[source]
----
curl --json '{"model_id": "<model_name__>", "inputs": "<text>"}' https://<inference_endpoint_url>:443/api/v1/task/server-streaming-text-generation -H 'Authorization: Bearer <token>'
----

*Caikit Standalone ServingRuntime for KServe*

.REST
[source]
----
curl -H 'Content-Type: application/json' -d '{"inputs": "<text>", "model_id": "<model_id>"}' <inference_endpoint_url>/api/v1/task/embedding -H 'Authorization: Bearer <token>'
----

.gRPC
[source]
----
grpcurl -insecure -d '{"text": "<text>"}' -H \"mm-model-id: <model_id>\" <inference_endpoint_url>:443 caikit.runtime.Nlp.NlpService/EmbeddingTaskPredict -H 'Authorization: Bearer <token>'
----

*TGIS Standalone ServingRuntime for KServe*
[source]
----
grpcurl -proto text-generation-inference/proto/generation.proto -d '{"requests": [{"text":"<text>"}]}' -H 'Authorization: Bearer <token>' -insecure <inference_endpoint_url>:443 fmaas.GenerationService/Generate 
----

*OpenVINO Model Server*
[source]
----
curl -ks <inference_endpoint_url>/v2/models/<model_name>/infer -d '{ "model_name": "<model_name>", "inputs": [{ "name": "<name_of_model_input>", "shape": [<shape>], "datatype": "<data_type>", "data": [<data>] }]}' -H 'Authorization: Bearer <token>'
----

*vLLM ServingRuntime for KServe*
[source]
----
curl -v https://<inference_endpoint_url>:443/v1/chat/completions -H "Content-Type: application/json" -d '{ "messages": [{ "role": "<role>", "content": "<content>" }] -H 'Authorization: Bearer <token>'
----
[NOTE]
--
As of vLLM v0.5.5, you must provide a chat template while querying a model using the `/v1/chat/completions` endpoint. If your model does not include a built-in chat template, you can provide one by using the `chat-template` parameter within your custom vLLM runtime and also use the example templates provided in the vLLM image under `/apps/data/template`:
+
[source]
----
--chat-template=/apps/data/template/template_chatml.jinja
----
--
--
endif::[]

[role='_additional-resources']
== Additional resources
* link:https://github.com/IBM/text-generation-inference[Text Generation Inference Server (TGIS)^]
* link:https://caikit.readthedocs.io/en/latest/autoapi/caikit/index.html[Caikit API documentation^]
* link:https://github.com/caikit/caikit-nlp[Caikit NLP GitHub project^]
* link:https://docs.openvino.ai/2023.3/ovms_docs_rest_api_kfs.html[OpenVINO KServe-compatible REST API documentation^]
* link:https://platform.openai.com/docs/api-reference/introduction[OpenAI API documentation^]
ifdef::upstream[]
* link:{odhdocshome}/serving-models/#supported-runtimes_serving-large-models[Supported model-serving runtimes]
endif::[]
ifndef::upstream[]
* link:{rhoaidocshome}{default-format-url}/serving_models/serving-large-models_serving-large-models#supported-model-serving-runtimes_serving-large-models[Supported model-serving runtimes].
endif::[]
