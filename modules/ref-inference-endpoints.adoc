
:_module-type: REFERENCE

[id='inference-endpoints_{context}']
= Inference endpoints

[role='_abstract']
These examples show how to use inference endpoints to query the model. 

[NOTE]
--
If you enabled token authentication when deploying the model, add the `Authorization` header and specify a token value.
--

== Caikit TGIS ServingRuntime for KServe

* `:443/api/v1/task/text-generation`
* `:443/api/v1/task/server-streaming-text-generation`

.Example command

ifdef::upstream[]
[source]
----
curl --json '{"model_id": "<model_name>", "inputs": "<text>"}' \
https://<inference_endpoint_url>:443/api/v1/task/server-streaming-text-generation \
-H 'Authorization: Bearer <token>'
----
endif::[]
ifdef::self-managed,cloud-service[]
[source]
----
curl --json '{"model_id": "<model_name__>", "inputs": "<text>"}' https://<inference_endpoint_url>:443/api/v1/task/server-streaming-text-generation -H 'Authorization: Bearer <token>'
----
endif::[]

== OpenVINO Model Server

* `/v2/models/<model-name>/infer`

.Example command

ifdef::upstream[]
[source]
----
curl -ks <inference_endpoint_url>/v2/models/<model_name>/infer -d \
'{ "model_name": "<model_name>", \
"inputs": [{ "name": "<name_of_model_input>", "shape": [<shape>], "datatype": "<data_type>", "data": [<data>] }]}' \
-H 'Authorization: Bearer <token>'
----
endif::[]
ifdef::self-managed,cloud-service[]
[source]
----
curl -ks <inference_endpoint_url>/v2/models/<model_name>/infer -d '{ "model_name": "<model_name>", "inputs": [{ "name": "<name_of_model_input>", "shape": [<shape>], "datatype": "<data_type>", "data": [<data>] }]}' -H 'Authorization: Bearer <token>'
----
endif::[]

== vLLM NVIDIA GPU ServingRuntime for KServe

* `:443/version`
* `:443/docs`
* `:443/v1/models`
* `:443/v1/chat/completions`
* `:443/v1/completions`
* `:443/v1/embeddings`
* `:443/tokenize`
* `:443/detokenize`
+
[NOTE]
====
* The vLLM runtime is compatible with the OpenAI REST API.
* To use the embeddings inference endpoint in vLLM, you must use an embeddings model that the vLLM supports. You cannot use the embeddings endpoint with generative models. For more information, see link:https://github.com/vllm-project/vllm/pull/3734[Supported embeddings models in vLLM].
* As of vLLM v0.5.5, you must provide a chat template while querying a model using the `/v1/chat/completions` endpoint. If your model does not include a predefined chat template, you can use the `chat-template` command-line parameter to specify a chat template in your custom vLLM runtime, as shown in the example. Replace `<CHAT_TEMPLATE>` with the path to your template.
+
[source]
----
containers:
  - args:
      - --chat-template=<CHAT_TEMPLATE>
----
You can use the chat templates that are available as `.jinja` files link:https://github.com/opendatahub-io/vllm/tree/main/examples[here] or with the vLLM image under `/app/data/template`. For more information, see link:https://huggingface.co/docs/transformers/main/chat_templating[Chat templates].
====
+
As indicated by the paths shown, the model serving platform uses the HTTPS port of your OpenShift router (usually port 443) to serve external API requests.

.Example command
ifdef::upstream[]
[source]
----
curl -v https://<inference_endpoint_url>:443/v1/chat/completions -H \
"Content-Type: application/json" -d '{ \
"messages": [{ \
"role": "<role>", \
"content": "<content>" \
}] -H 'Authorization: Bearer <token>'
----
endif::[]
ifdef::self-managed,cloud-service[]
[source]
----
curl -v https://<inference_endpoint_url>:443/v1/chat/completions -H "Content-Type: application/json" -d '{ "messages": [{ "role": "<role>", "content": "<content>" }] -H 'Authorization: Bearer <token>'
----
endif::[]

== vLLM Intel Gaudi Accelerator ServingRuntime for KServe 

ifndef::upstream[]
See link:{rhoaidocshome}{default-format-url}/deploying_models/making_inference_requests_to_deployed_models#vllm_nvidia_gpu_servingruntime_for_kserve[vLLM NVIDIA GPU ServingRuntime for KServe^].
endif::[]
ifdef::upstream[]
See link:{odhdocshome}/deploying-models/#_vllm_nvidia_gpu_servingruntime_for_kserve[vLLM NVIDIA GPU ServingRuntime for KServe^].
endif::[]

== vLLM AMD GPU ServingRuntime for KServe

ifndef::upstream[]
See link:{rhoaidocshome}{default-format-url}/deploying_models/making_inference_requests_to_deployed_models#vllm_nvidia_gpu_servingruntime_for_kserve[vLLM NVIDIA GPU ServingRuntime for KServe^].
endif::[]
ifdef::upstream[]
See link:{odhdocshome}/deploying-models/#_vllm_nvidia_gpu_servingruntime_for_kserve[vLLM NVIDIA GPU ServingRuntime for KServe^].
endif::[]

== vLLM Spyre AI Accelerator ServingRuntime for KServe
ifndef::upstream[]
[IMPORTANT]
====
ifdef::self-managed[]
Support for IBM Spyre AI Accelerators on x86 is currently available in {productname-long} {vernum} as a Technology Preview feature.
endif::[]
ifdef::cloud-service[]
Support for IBM Spyre AI Accelerators on x86 is currently available in {productname-long} as a Technology Preview feature.
endif::[]
Technology Preview features are not supported with {org-name} production service level agreements (SLAs) and might not be functionally complete.
{org-name} does not recommend using them in production.
These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.

For more information about the support scope of {org-name} Technology Preview features, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].
====
endif::[]

ifdef::upstream[]
You can serve models with IBM Spyre AI accelerators on x86 by using the *vLLM Spyre AI Accelerator ServingRuntime for KServe* runtime. To use the runtime, you must install the Spyre Operator and configure a hardware profile. For more information, see link:https://catalog.redhat.com/en/software/containers/ibm-aiu/spyre-operator/688a1121575e62c686a471d4[Spyre operator image^] and link:{odhdocshome}/working-with-accelerators/#working-with-hardware-profiles_accelerators[Working with hardware profiles^].
endif::[]

ifndef::upstream[]
You can serve models with IBM Spyre AI accelerators on x86 by using the *vLLM Spyre AI Accelerator ServingRuntime for KServe* runtime. To use the runtime, you must install the Spyre Operator and configure a hardware profile. For more information, see link:https://catalog.redhat.com/en/software/containers/ibm-aiu/spyre-operator/688a1121575e62c686a471d4[Spyre operator image^] and link:{rhoaidocshome}{default-format-url}/working_with_accelerators/working-with-hardware-profiles_accelerators[Working with hardware profiles^].
endif::[]

== vLLM Spyre s390x ServingRuntime for KServe

ifdef::upstream[]
You can serve models with IBM Spyre AI accelerators on IBM Z (s390x architecture) by using the *vLLM Spyre s390x ServingRuntime for KServe* runtime. To use the runtime, you must install the Spyre Operator and configure a hardware profile. For more information, see link:https://catalog.redhat.com/en/software/containers/ibm-aiu/spyre-operator/688a1121575e62c686a471d4[Spyre operator image^] and link:{odhdocshome}/working-with-accelerators/#working-with-hardware-profiles_accelerators[Working with hardware profiles^].
endif::[]

ifndef::upstream[]
You can serve models with IBM Spyre AI accelerators on IBM Z (s390x architecture) by using the *vLLM Spyre s390x ServingRuntime for KServe* runtime. To use the runtime, you must install the Spyre Operator and configure a hardware profile. For more information, see link:https://catalog.redhat.com/en/software/containers/ibm-aiu/spyre-operator/688a1121575e62c686a471d4[Spyre operator image^] and link:{rhoaidocshome}{default-format-url}/working_with_accelerators/working-with-hardware-profiles_accelerators[Working with hardware profiles^].
endif::[]

== NVIDIA Triton Inference Server

--
.REST endpoints
* `v2/models/[/versions/<model_version>]/infer`
* `v2/models/<model_name>[/versions/<model_version>]`
* `v2/health/ready`
* `v2/health/live`
* `v2/models/<model_name>[/versions/]/ready`
* `v2`
--

.Example command
ifndef::upstream[]
[source]
----
curl -ks <inference_endpoint_url>/v2/models/<model_name>/infer -d '{ "model_name": "<model_name>", "inputs": [{ "name": "<name_of_model_input>", "shape": [<shape>], "datatype": "<data_type>", "data": [<data>] }]}' -H 'Authorization: Bearer <token>'
----
endif::[]
ifdef::upstream[]
[source]
----
curl -ks <inference_endpoint_url>/v2/models/<model_name>/infer -d /
'{ "model_name": "<model_name>", \
   "inputs": \
	[{ "name": "<name_of_model_input>", \
           "shape": [<shape>], \
           "datatype": "<data_type>", \
           "data": [<data>] \
         }]}' -H 'Authorization: Bearer <token>'
----
endif::[]
--
.gRPC endpoints
* `:443 inference.GRPCInferenceService/ModelInfer`
* `:443 inference.GRPCInferenceService/ModelReady`
* `:443 inference.GRPCInferenceService/ModelMetadata`
* `:443 inference.GRPCInferenceService/ServerReady`
* `:443 inference.GRPCInferenceService/ServerLive`
* `:443 inference.GRPCInferenceService/ServerMetadata`
--

.Example command
ifdef::upstream[]
[source]
----
grpcurl -cacert ./openshift_ca_istio_knative.crt \
        -proto ./grpc_predict_v2.proto \
        -d @ \
        -H "Authorization: Bearer <token>" \
        <inference_endpoint_url>:443 \
        inference.GRPCInferenceService/ModelMetadata
----
endif::[]
ifndef::upstream[]
----
grpcurl -cacert ./openshift_ca_istio_knative.crt -proto ./grpc_predict_v2.proto -d @ -H "Authorization: Bearer <token>" <inference_endpoint_url>:443 inference.GRPCInferenceService/ModelMetadata
----
endif::[]

== Seldon MLServer

--
.REST endpoints
* `v2/models/[/versions/<model_version>]/infer`
* `v2/models/<model_name>[/versions/<model_version>]`
* `v2/health/ready`
* `v2/health/live`
* `v2/models/<model_name>[/versions/]/ready`
* `v2`
--

.Example command
ifndef::upstream[]
[source]
----
curl -ks <inference_endpoint_url>/v2/models/<model_name>/infer -d '{ "model_name": "<model_name>", "inputs": [{ "name": "<name_of_model_input>", "shape": [<shape>], "datatype": "<data_type>", "data": [<data>] }]}' -H 'Authorization: Bearer <token>'
----
endif::[]
ifdef::upstream[]
[source]
----
curl -ks <inference_endpoint_url>/v2/models/<model_name>/infer -d /
'{ "model_name": "<model_name>", \
   "inputs": \
        [{ "name": "<name_of_model_input>", \
           "shape": [<shape>], \
           "datatype": "<data_type>", \
           "data": [<data>] \
         }]}' -H 'Authorization: Bearer <token>'
----
endif::[]
--
.gRPC endpoints
* `:443 inference.GRPCInferenceService/ModelInfer`
* `:443 inference.GRPCInferenceService/ModelReady`
* `:443 inference.GRPCInferenceService/ModelMetadata`
* `:443 inference.GRPCInferenceService/ServerReady`
* `:443 inference.GRPCInferenceService/ServerLive`
* `:443 inference.GRPCInferenceService/ServerMetadata`
--

.Example command
ifdef::upstream[]
[source]
----
grpcurl -cacert ./openshift_ca_istio_knative.crt \
        -proto ./grpc_predict_v2.proto \
        -d @ \
        -H "Authorization: Bearer <token>" \
        <inference_endpoint_url>:443 \
        inference.GRPCInferenceService/ModelMetadata
----
endif::[]
ifndef::upstream[]
----
grpcurl -cacert ./openshift_ca_istio_knative.crt -proto ./grpc_predict_v2.proto -d @ -H "Authorization: Bearer <token>" <inference_endpoint_url>:443 inference.GRPCInferenceService/ModelMetadata
----
endif::[]

[role='_additional-resources']
== Additional resources
* link:https://caikit.readthedocs.io/en/latest/autoapi/caikit/index.html[Caikit API documentation^]
* link:https://docs.openvino.ai/2023.3/ovms_docs_rest_api_kfs.html[OpenVINO KServe-compatible REST API documentation^]
* link:https://platform.openai.com/docs/api-reference/introduction[OpenAI API documentation^]
* link:https://kserve.github.io/archive/0.11/modelserving/data_plane/v2_protocol/[Open Inference Protocol]
ifdef::upstream[]
* link:{odhdocshome}/configuring-your-model-serving-platform/#supported-runtimes_odh-admin[Supported model-serving runtimes]
endif::[]
ifndef::upstream[]
* link:{rhoaidocshome}{default-format-url}/configuring_your_model-serving_platform/configuring-your-model-serving-platform_rhoai-admin#supported-model-serving-runtimes_rhoai-admin[Supported model-serving runtimes].
endif::[]
