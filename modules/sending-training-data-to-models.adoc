:_module-type: PROCEDURE

[id="sending-training-data-to-models_{context}"]
= Sending training data to models

[role='_abstract']
You must send training data through your model for TrustyAI to be able to compute baseline fairness values. 

.Prerequisites

* Your OpenShift cluster administrator has installed {productname-short} and enabled the TrustyAI service for the data science project where the models are deployed.

* You have logged in to {productname-long}.

.Procedure
ifdef::upstream[]
. Get the inference endpoints for the deployed model, as described in link:{odhdocshome}{default-format-url}/serving-models/serving-small-and-medium-sized-models_model-serving#viewing-a-deployed-model_model-serving[Viewing a deployed model].
endif::[]

ifndef::upstream[]
. Get the inference endpoints for the deployed model, as described in link:{rhoaidocshome}{default-format-url}/serving_models/serving-small-and-medium-sized-models_model-serving#viewing-a-deployed-model_model-serving[Viewing a deployed model].
endif::[]

. Send data to this endpoint. For more information, see the link:https://kserve.github.io/website/0.8/modelserving/inference_api/#server-metadata-response-json-object[KServe v2 Inference Protocol documentation].

.Verification
Follow these steps to view cluster metrics and verify that TrustyAI is receiving data. 

. Log in to the {openshift-platform} web console.
. Switch to the *Developer* perspective.
. In the left menu, click *Observe* -> *Metrics*.
. On the *Metrics* page, set the *Time range* to 5 minutes and the *Refresh interval* to 15 seconds.
. In the *Expression* field, enter `trustyai_model_observations_total` and then click *Run queries*. Your models are listed and report observed inferences.

ifndef::upstream[]
.Next step
xref:assemblies/configuring-bias-metrics-for-a-model[Configuring bias metrics for a model]
endif::[]
