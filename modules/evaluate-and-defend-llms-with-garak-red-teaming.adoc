:_module-type: CONCEPT

[id='evaluate-and-defend-llms-with-garak-red-teaming_{context}']
= Evaluate and defend LLMs with Garak red-teaming

[role='_abstract']

As a {productname-long} user, you may want to evaluate and guard your Large Language Models (LLMs) to ensure they are safe for production use.  You can use a process called 'red-teaming' to assess your model's vulnerabilities and install shields to defend it.

Garak (Generative AI Red-teaming & Assessment Kit) is an open-source red-teaming tool to detect vulnerabilities, weaknesses, or flaws in Large Language Models (LLMs). It acts as a security scanner for AI and can suggest what guardrails you can use to help guard against those vulnerabilities. 

The process of securing your model involves three main actions:

. *Red-teaming*: Use Garak to run a baseline scan against the unguarded model, to uncover critical vulnerabilities that the model is susceptible to in real-world deployment.

. *Analyse and apply guardrails*: Configure input and output shields based on the baseline findings.

. *Verification*: Re-scan the guarded model to confirm that vulnerability scores have decreased.

