:_module-type: REFERENCE

ifdef::context[:parent-context: {context}]
[id="setting-up-lmeval.adoc_{context}"]
= Setting up LM-Eval

[role='_abstract']
LM-Eval is a service for large language model evaluation. It is integrated into the TrustyAI Kubernetes Operator and is underpinned by two open-source projects - link:https://github.com/EleutherAI/lm-evaluation-harness[lm-evaluation-harness] and link:https://www.unitxt.ai/en/latest/[Unitxt]. 

[NOTE]
--
LM-Eval is only available in the latest community builds. To use LM-Eval on {productname-long}, add the following `devFlag` to your `DataScienceCluster` resource:
+

[source]
----
    trustyai:
    devFlags:
        manifests:
        - contextDir: config
            sourcePath: ''
            uri: https://github.com/trustyai-explainability/trustyai-service-operator/tarball/main
    managementState: Managed
----
--
 
.Global settings for LM-Eval

Configurable global settings for LM-Eval services are stored in the TrustyAI operator global `ConfigMap`, named `trustyai-service-operator-config`. The global settings are located in the same namespace as the operator.

You can configure the following properties for LM-Eval:

.LM-Eval properties
[cols="1,1,5"]
|===
| Property | Default | Description

| `lmes-detect-device`
| `true/false`
| Detect if there are GPUs available and assign a value for `--device argument` for `lm-evaluation-harness`. If GPUs are available, the value is `cuda`. If there are no GPUs available, the value is `cpu`.

| `lmes-pod-image`
| `quay.io/trustyai/ta-lmes-job:latest`
| The image for the LM-Eval job. The image contains the Python packages for `lm-evaluation-harness` and Unitxt.

| `lmes-driver-image`
| `quay.io/trustyai/ta-lmes-driver:latest`
| The image for the LM-Eval driver. For detailed information about the driver, see the  `cmd/lmes_driver` directory.

| `lmes-image-pull-policy` 
| `Always`
| The image-pulling policy when running the evaluation job.

| `lmes-default-batch-size`
| 8
| The default batch size when invoking the model inference API. Default batch size is only available for local models.

| `lmes-max-batch-size`
| 24
| The maximum batch size that users can specify in an evaluation job.

| `lmes-pod-checking-interval`
| 10s
| The interval to check the job pod for an evaluation job.
 
|===

[NOTE]
--
After updating the settings in the `ConfigMap`, you must restart the operator to apply the new values.
--
