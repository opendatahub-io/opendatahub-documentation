:_module-type: PROCEDURE

[id="resolving-cuda-oom-errors-for-the-single-model-serving-platform_{context}"]
= Resolving CUDA out-of-memory errors

[role="_abstract"]

In certain cases, depending on the model and hardware accelerator used, the TGIS memory auto-tuning algorithm might underestimate the amount of GPU memory needed to process long sequences. This miscalculation can lead to Compute Unified Architecture (CUDA) out-of-memory (OOM) error responses from the model server. In such cases, you must update or add additional parameters in the TGIS model-serving runtime, as described in the following procedure.
[NOTE]
====
The *Text Generation Inference Server (TGIS) Standalone ServingRuntime for KServe* is deprecated. For more information, see link:{rhoaidocshome}{default-format-url}/release_notes/index[{productname-short} release notes].
====

.Prerequisites
* You have logged in to {productname-short} as a user with {productname-short} administrator privileges.

.Procedure
. From the {productname-short} dashboard, click *Settings* -> *Model resources and operations* -> *Serving runtimes*.
+
The *Serving runtimes* page opens and shows the model-serving runtimes that are already installed and enabled.
+
. Based on the runtime that you used to deploy your model, perform one of the following actions:
+
* If you used the preinstalled *Text Generation Inference Server (TGIS) Standalone ServingRuntime for KServe* runtime, duplicate the runtime to create a custom version and then follow the remainder of this procedure. For more information about duplicating the pre-installed TGIS runtime, see {rhoaidocshome}{default-format-url}/serving_models/serving-large-models_serving-large-models#adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models[Adding a custom model-serving runtime for the single-model serving platform].
* If you were already using a custom TGIS runtime, click the action menu (&#8942;) next to the runtime and select *Edit*.
+
The embedded YAML editor opens and shows the contents of the custom model-serving runtime.
. Add or update the `BATCH_SAFETY_MARGIN` environment variable and set the value to 30. Similarly, add or update the `ESTIMATE_MEMORY_BATCH_SIZE` environment variable and set the value to 8.
+
[source]
----
spec:
  containers:
    env:
    - name: BATCH_SAFETY_MARGIN
      value: 30
    - name: ESTIMATE_MEMORY_BATCH
      value: 8
----
+
[NOTE]
====
The `BATCH_SAFETY_MARGIN` parameter sets a percentage of free GPU memory to hold back as a safety margin to avoid OOM conditions. The default value of `BATCH_SAFETY_MARGIN` is `20`. The `ESTIMATE_MEMORY_BATCH_SIZE` parameter sets the batch size used in the memory auto-tuning algorithm. The default value of `ESTIMATE_MEMORY_BATCH_SIZE`  is `16`.
====
. Click *Update*.
+ 
The *Serving runtimes* page opens and shows the list of runtimes that are installed. Observe that the custom model-serving runtime you updated is shown.
+
. To redeploy the model for the parameter updates to take effect, perform the following actions:
.. From the {productname-short} dashboard, click *AI hub* -> *Deployments*. 
.. Find the model you want to redeploy, click the action menu (â‹®) next to the model, and select *Delete*.
ifndef::upstream[]
.. Redeploy the model as described in link:{rhoaidocshome}{default-format-url}/serving_models/serving-large-models_serving-large-models#deploying-models-on-the-single-model-serving-platform_serving-large-models[Deploying models on the single-model serving platform].
endif::[]
ifdef::upstream[]
.. Redeploy the model as described in link:{odhdocshome}/serving-models/#deploying-models-on-the-single-model-serving-platform_serving-large-models[Deploying models on the single-model serving platform].
endif::[]

.Verification
* You receive successful responses from the model server and no longer see CUDA OOM errors.
// [role="_additional-resources"]
// .Additional resources
