:_module-type: PROCEDURE

[id="introduction-bias-monitoring_{context}"]
== Introduction

Ensuring that your machine learning models are fair and unbiased is essential for building trust with your users. Although you can assess fairness during model training, it is only in deployment that your models encounter real-world data. Even if your models are unbiased on training data, they can exhibit dangerous biases in real-world scenarios. Therefore, it is crucial to monitor your models for fairness during their real-world deployment.


In this tutorial, you learn how to monitor models for bias. You will use two example models to complete the following tasks:

* Deploy the models by using multi-model serving
* Send training data to the models
* Examine the metadata for the models
* Check the fairness for the models
* Schedule and check fairness and identity metric requests
* Simulate real-world data


== About the example models
For this tutorial, your role is the dev-ops engineer for a credit lending company. The company's data scientists have created two candidate neural network models to predict whether a borrower will default on a loan. Both models make their predictions based on the following information from the borrower's application:

* Number of Children
* Total Income:
* Number of Total Family Members
* Is Male-Identifying?
* Owns Car?
* Owns Realty?
* Is Partnered?
* Is Employed?
* Lives with Parents?
* Age (in days)
* Length of Employment (in days)

Your task, as the dev-ops engineer, is to verify that the models are not biased towards the Is Male-Identifying? gender field. To complete this task, you can monitor the models with the Statistical Parity Difference (SPD) metric, which reports whether there is a difference between how often male-identifying and non-male-identifying applicants are given favorable predictions (that is, they are predicted to pay off their loans) Ideally, the SPD value would be 0, indicating that both groups have equal likelihood of getting a good outcome. However, an SPD value between -0.1 and 0.1 is also indicative of fairness, indicating that the two groups' rates of getting good outcomes only vary by +/-10%.

