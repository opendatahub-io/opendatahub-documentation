:_module-type: PROCEDURE

[id='enabling-intel-gaudi-ai-accelerators_{context}']
= Enabling Intel Gaudi AI accelerators

[role='_abstract']
Before you can use Intel Gaudi AI accelerators in {productname-short}, you must install the required dependencies, deploy the Intel Gaudi AI Accelerator Operator, and configure the environment.

.Prerequisites
ifdef::upstream,self-managed[]
* You have logged in to {openshift-platform}.
* You have the `cluster-admin` role in {openshift-platform}.
* You have installed your Intel Gaudi accelerator and confirmed that it is detected in your environment.
* Your OpenShift environment supports EC2 DL1 instances if you are running on Amazon Web Services (AWS).
* You have installed the OpenShift command-line interface (CLI). 

endif::[]
ifdef::cloud-service[]
* You have logged in to OpenShift.
* You have the `cluster-admin` role in OpenShift.
* You have installed your Intel Gaudi accelerator and confirmed that it is detected in your environment.
* Your OpenShift environment supports EC2 DL1 instances if you are running on Amazon Web Services (AWS).
* You have installed the OpenShift command-line interface (CLI). 
endif::[]

.Procedure
. Install the latest version of the Intel Gaudi AI Accelerator Operator, as described in link:https://docs.habana.ai/en/latest/Installation_Guide/Additional_Installation/OpenShift_Installation/index.html[Intel Gaudi AI Operator OpenShift installation].
. By default, {openshift-platform} sets a per-pod PID limit of 4096. If your workload requires more processing power, such as when you use multiple Gaudi accelerators or when using vLLM with Ray, you must manually increase the per-pod PID limit to avoid `Resource temporarily unavailable` errors. These errors occur due to PID exhaustion. {org-name} recommends setting this limit to 32768, although values over 20000 are sufficient.
.. Run the following command to label the node: 
+
[source]
----
oc label node <node_name> custom-kubelet=set-pod-pid-limit-kubelet
----
.. Optional: To prevent workload distribution on the affected node, you can mark the node as unschedulable and then drain it in preparation for maintenance. For more information, see link:https://docs.redhat.com/en/documentation/openshift_container_platform/{ocp-latest-version}/html/nodes/working-with-nodes#nodes-nodes-working-evacuating_nodes-nodes-working[Understanding how to evacuate pods on nodes].
.. Create a `custom-kubelet-pidslimit.yaml` KubeletConfig resource file: 
+
[source]
----
oc create -f custom-kubelet-pidslimit.yaml
----
.. Populate the file with the following YAML code. Set the `PodPidsLimit` value to 32768:
+
[source,YAML]
----
apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: custom-kubelet-pidslimit
spec:
  kubeletConfig:
    PodPidsLimit: 32768
  machineConfigPoolSelector:
    matchLabels:
      custom-kubelet: set-pod-pid-limit-kubelet
----
.. Apply the configuration: 
+
[source]
----
oc apply -f custom-kubelet-pidslimit.yaml
----
+
This operation causes the node to reboot. For more information, see link:https://docs.redhat.com/en/documentation/openshift_container_platform/{ocp-latest-version}/html/nodes/working-with-nodes#nodes-nodes-rebooting[Understanding node rebooting].
.. Optional: If you previously marked the node as unschedulable, you can allow scheduling again after the node reboots.

ifndef::upstream[]
. Create a custom workbench image for Intel Gaudi AI accelerators, as described in link:{rhoaidocshome}{default-format-url}/managing_openshift_ai/creating-custom-workbench-images[Creating custom workbench images].
endif::[]
ifdef::upstream[]
. Create a custom workbench image for Intel Gaudi AI accelerators, as described in link:{odhdocshome}/managing-odh/#creating-custom-workbench-images[Creating custom workbench images].
endif::[]
//downstream - all
ifndef::upstream[]
. After installing the Intel Gaudi AI Accelerator Operator, create an accelerator profile, as described in link:{rhoaidocshome}{default-format-url}/working_with_accelerators/#working-with-accelerator-profiles_accelerators[Working with accelerator profiles].
endif::[]
//upstream only
ifdef::upstream[]
. After installing the Intel Gaudi AI Accelerator Operator, create an accelerator profile, as described in link:{odhdocshome}/working-with-accelerators/#working-with-accelerator-profiles_accelerators[Working with accelerator profiles].
endif::[]
+
[IMPORTANT]
====
By default, hardware profiles are hidden in the dashboard navigation menu and user interface, while accelerator profiles remain visible. In addition, user interface components associated with the deprecated accelerator profiles functionality are still displayed. To show the *Settings -> Hardware profiles* option in the dashboard navigation menu, and the user interface components associated with hardware profiles, set the `disableHardwareProfiles` value to `false` in the `OdhDashboardConfig` custom resource (CR) in {openshift-platform}. 
ifdef::upstream[]
For more information about setting dashboard configuration options, see link:{odhdocshome}/managing-resources/#customizing-the-dashboard[Customizing the dashboard].
endif::[]
ifndef::upstream[]
For more information about setting dashboard configuration options, see link:{rhoaidocshome}{default-format-url}/managing_resources/customizing-the-dashboard[Customizing the dashboard].
endif::[]
==== 

.Verification
From the *Administrator* perspective, go to the *Operators* -> *Installed Operators* page. Confirm that the following Operators appear:

* Intel Gaudi AI Accelerator
* Node Feature Discovery (NFD)
* Kernel Module Management (KMM)

//[role='_additional-resources']
//.Additional resources

