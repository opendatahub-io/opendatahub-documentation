:_module-type: PROCEDURE

[id='enabling-trustyai-service-using-cli_{context}']
= Enabling the TrustyAI Service by using the CLI

[role='_abstract']
Enable an instance of the TrustyAI service on each data science project that contains models that the data scientist wants to monitor. The TrustyAI service instance provides the URL that a data scientist uses to monitor and analyze any number of models deployed into a data science project.

The following procedure describes how to use the OpenShift command-line interface (CLI) to enable an instance of the TrustyAI service.

.Prerequisites

* On the OpenShift cluster where {productname-short} is installed, you have enabled user workload monitoring as described in link:https://docs.openshift.com/container-platform/{ocp-latest-version}/monitoring/enabling-monitoring-for-user-defined-projects.html[Enabling monitoring for user-defined projects].

* You have enabled the multi-model serving platform. 

ifdef::upstream[]
* You have enabled the multi-model serving platform as described in link:{odhdocshome}/serving-models/enabling-the-multi-model-serving-platform_model-serving[Enabling the multi-model serving platform].
endif::[]
ifndef::upstream[]
* You have enabled the multi-model serving platform as described in link:{rhoaidocshome}{default-format-url}/serving_models/serving-small-and-medium-sized-models_model-serving#enabling-the-multi-model-serving-platform_model-serving[Enabling the multi-model serving platform].
endif::[]

[NOTE]
====
The monitoring models for bias (TrustyAI feature) is supported only on a _multi-model serving platform_ that is based on the ModelMesh component. It is not supported on a _single model serving platform_ that is based on the KServe component.
====

ifdef::upstream[]
* You have installed {productname-short} as described in link:https://opendatahub.io/docs/quick-installation-new-operator/[Quick Installation(v2)].

* The `trustyai` component is set to *Managed* for the {productname-short} Operator.
+
To verify this setting, navigate to *Operators* -> *Installed Operators* -> *{productname-short} Operator* -> *Data Science Cluster*. Select the *default* instance and then click *YAML*. Scroll down to view the `spec:components` setting:
+
----
 trustyai:
      devFlags: {}
      managementState: Managed
----
endif::[]

ifndef::upstream[]

ifdef::self-managed[]
* You have installed {productname-short} as described in link:{rhoaidocshome}{default-format-url}/installing_and_uninstalling_{url-productname-short}/installing-the-openshift-data-science-operator_operator-install[Installing the {productname-long} Operator].
endif::[]

ifndef::self-managed[]
* You have installed {productname-short} as described in link:{rhoaidocshome}{default-format-url}/installing_and_uninstalling_{url-productname-short}/installing-openshift-ai-managed_install[Installing OpenShift AI on your OpenShift cluster].
endif::[]

* The `trustyai` component is set to *Managed* for the {productname-long} Operator.
+
To verify this setting, navigate to *Operators* -> *Installed Operators* -> *{productname-long} Operator* -> *Data Science Cluster*. Select the *default* instance and then click *YAML*. Scroll down to view the `spec:components` setting:
+
----
 trustyai:
      devFlags: {}
      managementState: Managed
----
endif::[]
+
*NOTE:* If the `trustyai` component is set to *Removed*, edit the YAML file to set it to *Managed*.

* You have logged in to {productname-short}.

ifndef::upstream[]
* If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {oai-user-group} or {oai-admin-group}) in OpenShift.

* The data scientist has created a data science project, as described in link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects/working-on-data-science-projects_nb-server#creating-a-data-science-project_nb-server[Creating a data science project], that contains (or will contain) the models that the data scientist wants to monitor.  
endif::[]

ifdef::upstream[]
* If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group} or {odh-admin-group}) in OpenShift.

* The data scientist has created a data science project, as described in link:{odhdocshome}/working-on-data-science-projects/#working-on-data-science-projects_nb-server[Creating a data science project], that contains (or will contain) the models that the data scientist wants to monitor.  
endif::[]

.Procedure
. Login to your cluster.
+
----
 oc login
----

. Navigate to the data science project that contains (or will contain) the models that the data scientist wants to monitor. 
+
----
oc project <project-name>
----
+
For example:
+
----
oc project mydsproject
----

. Create a `TrustyAIService` custom resource (CR) file, for example `trustyai_crd.yaml`: 
+
----
apiVersion: trustyai.opendatahub.io/v1alpha1
kind: TrustyAIService
metadata:
  name: trustyai-service
spec:
  storage:
	format: "PVC"
	folder: "/inputs"
	size: "1Gi"
  data:
	filename: "data.csv"
	format: "CSV"
  metrics:
	schedule: "5s"
	batchSize: 5000 # Optional, default is 5000
----
+ 
Here is a description of the fields:
+
`metadata.name`:: The name of the TrustyAI service instance.
`spec.storage.format`:: The storage format for the data. Currently, only persistent volume control (PVC) format is supported.
`spec.storage.folder`:: The location within the PVC where you want to store the data.
`spec.storage.size`:: The size of the PVC to request.
`spec.data.filename`:: The suffix for the stored data files.
`spec.data.format`:: The format of the data. Currently, only comma-separated value (CSV) format is supported.
`spec.metrics.schedule`:: The interval at which to calculate the metrics. The default is 5s. The duration is specified with the ISO-8601 format. For example, `5s` for 5 seconds, `5m` for 5 minutes, and `5h` for 5 hours.
`spec.metrics.batchSize`:: The observation's historical window size to use for metrics calculation. The default is `5000` (that is, the metrics are calculated by using the latest 5000 inferences).

. Add the TrustyAI service's CR to your project:
+
----
oc apply -f trustyai_crd.yaml
----
+
This command returns output similar to the following:
+
----
trusty-service created
----


.Verification

Use this command to verify that you enabled the TrustyAI Service:

----
oc get pods | grep trustyai 
----

You should see a response similar to the following:

----
trustyai-service-5d45b5884f-96h5z             1/1     Running
----
