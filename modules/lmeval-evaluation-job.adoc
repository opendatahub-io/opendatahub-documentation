:_module-type: REFERENCE

ifdef::context[:parent-context: {context}]
[id="lmeval-evaluation-job_{context}"]
= LM-Eval evaluation job

[role='_abstract']
LM-Eval service defines a new Custom Resource Definition (CRD) called `LMEvalJob`. An `LMEvalJob` object represents an evaluation job. `LMEvalJob` objects are monitored by the TrustyAI Kubernetes operator.

To run an evaluation job, create an `LMEvalJob` object with the following information: `model`, `model arguments`, `task`, and `secret`. 

[NOTE]
--
For a list of TrustyAI-supported tasks, see link:https://trustyai.org/docs/main/component-lm-eval#_lm_eval_task_support[LMEval task support]. 
--

After the `LMEvalJob` is created, the LM-Eval service runs the evaluation job.  The status and results of the `LMEvalJob` object update when the information is available.

[NOTE]
--
Other TrustyAI features (such as bias and drift metrics) cannot be used with non-tabular models (including LLMs). Deploying the `TrustyAIService` custom resource (CR) in a namespace that contains non-tabular models (such as the namespace where an evaluation job is being executed) can cause errors within the TrustyAI service.
--
 
.Sample LMEvalJob object 

The sample `LMEvalJob` object contains the following features: 

* The `google/flan-t5-base` model from Hugging Face. 

* The dataset from the `wnli` card, a subset of the GLUE (General Language Understanding Evaluation) benchmark evaluation framework from Hugging Face. For more information about the `wnli` Unitxt card, see the link:https://www.unitxt.ai/[Unitxt website].

* The following default parameters for the `multi_class.relation` Unitxt task: `f1_micro`, `f1_macro`, and `accuracy`. This template can be found on the Unitxt website: click *Catalog*, then click *Tasks* and select *Classification* from the menu.

The following is an example of an `LMEvalJob` object:

[source]
----

apiVersion: trustyai.opendatahub.io/v1alpha1
kind: LMEvalJob
metadata:
  name: evaljob-sample
spec:
  model: hf
  modelArgs:
  - name: pretrained
    value: google/flan-t5-base 
  taskList:
    taskRecipes:
    - card:
        name: "cards.wnli" 
      template: "templates.classification.multi_class.relation.default" 
  logSamples: true

----

After you apply the sample `LMEvalJob`, check its state by using the following command:

[source]
----
oc get lmevaljob evaljob-sample
----
Output similar to the following is displayed:
NAME: `evaljob-sample`
STATE: `Running`

Evaluation results are available when the state of the object changes to `Complete`. Both the model and dataset in this example are small. The evaluation job should finish within 10 minutes on a CPU-only node.

Use the following command to get the results:

[source]
----
oc get lmevaljobs.trustyai.opendatahub.io evaljob-sample \
  -o template --template={{.status.results}} | jq '.results'
----

The command returns results similar to the following example:

[source]
----
{
  "tr_0": {
    "alias": "tr_0",
    "f1_micro,none": 0.5633802816901409,
    "f1_micro_stderr,none": "N/A",
    "accuracy,none": 0.5633802816901409,
    "accuracy_stderr,none": "N/A",
    "f1_macro,none": 0.36036036036036034,
    "f1_macro_stderr,none": "N/A"
  }
}
----

.Notes on the results

* The `f1_micro`, `f1_macro`, and `accuracy` scores are 0.56, 0.36, and 0.56. 
* The full results are stored in the `.status.results` of the `LMEvalJob` object as a JSON document. 
* The command above only retrieves the results field of the JSON document.

