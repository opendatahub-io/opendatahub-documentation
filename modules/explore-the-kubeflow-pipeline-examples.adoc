:_module-type: CONCEPT

ifdef::context[:parent-context: {context}]
[id="explore-the-kubeflow-pipeline-examples_{context}"]
= Explore the kubeflow pipeline examples

[role='_abstract']

To get started with kubeflow pipelines, explore the provided examples. You can download and modify the example code to quickly create a Docling data processing or model training pipeline.

*Prerequisites*

* Install the data processing library, as described in Set up your working environment.

*Procedure*

1. To access the data processing examples, clone the data processing Git repository:
+
* To clone the repository from JupyterLab, follow the steps in xref:2.4.1[Clone an example Git repository].
* To create a local clone of the repository, run the following command:
+
[source, bash]
----
git clone https://github.com/opendatahub-io/data-processing -b stable-3.0
----
2. Go to the `kubeflow-pipelines` directory which contains the following tested examples for running Docling as a scalable pipeline. For instructions on how to import, configure and run the examples, see the link:https://github.com/opendatahub-io/data-processing/tree/stable-3.0/kubeflow-pipelines[README file] and the {org-name} AI link:https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/2.25/html/working_with_data_science_pipelines/managing-data-science-pipelines_ds-pipelines[Managing data science pipelines] guide.
+
* Standard Pipeline: For converting standard documents that contain text and structured elements. For more information, see the link:https://github.com/opendatahub-io/data-processing/blob/stable-3.0/kubeflow-pipelines/docling-standard/README.md[Standard Conversion Pipelines documentation].
* VLM (Vision Language Model): For converting highly complex or difficult-to-parse documents, such as those with custom instructions or complex layouts, or to add image descriptors. For more information, see the link:https://github.com/opendatahub-io/data-processing/blob/main/kubeflow-pipelines/docling-vlm/README.md[VLM Pipelines documentation].
