:_module-type: PROCEDURE

[id="explore-the-kubeflow-pipeline-examples_{context}"]
= Explore the Kubeflow Pipeline examples

[role='_abstract']

To get started with Kubeflow Pipelines, explore the provided examples. You can download and modify the example code to quickly create a Docling data processing or model training pipeline.

.Prerequisites

ifdef::upstream[]
* Install the data processing library as described in link:{odhdocshome}/customize-models-to-build-gen-ai-applications/#set-up-your-working-environment_custom-models[Set up your working environment].
endif::[]
ifndef::upstream[]
* Install the data processing library as described in link:{rhoaidocshome}{default-format-url}/customize_models_to_build_gen_ai_applications/set-up-your-working-environment_custom-models[Set up your working environment].
endif::[]

.Procedure

. To access the link:https://github.com/opendatahub-io/data-processing/tree/stable/kubeflow-pipelines[Kubeflow Pipeline examples], run the following command to clone the link:https://github.com/opendatahub-io/data-processing/tree/stable[data processing Git repository]:
+
[source, bash]
----
git clone https://github.com/opendatahub-io/data-processing -b stable
----
ifdef::upstream[]
. Go to the `kubeflow-pipelines` directory, which contains the following tested examples for running Docling as a scalable pipeline. For instructions on how to import, configure, and run the examples, see the link:https://github.com/opendatahub-io/data-processing/tree/stable/kubeflow-pipelines[README file] and the {org-name} AI link:{odhdocshome}/working_with_ai_pipelines/#managing-ai-pipelines_ai-pipelines[Working with AI pipelines] guide.
endif::[]
ifndef::upstream[]
. Go to the `kubeflow-pipelines` directory, which contains the following tested examples for running Docling as a scalable pipeline. For instructions on how to import, configure, and run the examples, see the link:https://github.com/opendatahub-io/data-processing/tree/stable/kubeflow-pipelines[README file] and the {org-name} AI link:{rhoaidocshome}{default-format-url}/working_with_ai_pipelines/managing-ai-pipelines_ai-pipelines[Working with AI pipelines] guide.
endif::[]
+
* Standard Pipeline: For converting standard documents that contain text and structured elements. For more information, see the link:https://github.com/opendatahub-io/data-processing/blob/stable/kubeflow-pipelines/docling-standard/README.md[Standard Conversion Pipelines documentation].
* VLM (Vision Language Model): For converting highly complex or difficult-to-parse documents, such as those with custom instructions or complex layouts, or to add image descriptors. For more information, see the link:https://github.com/opendatahub-io/data-processing/blob/stable/kubeflow-pipelines/docling-vlm/README.md[VLM Pipelines documentation].

*NOTE:* If you want to use a Red Hat container image in one of the above pipelines, replace the image on link:https://github.com/opendatahub-io/data-processing/blob/stable/kubeflow-pipelines/common/constants.py#L8[this line] with link:https://catalog.redhat.com/en/software/containers/rhai/docling-cuda-rhel9/696622c8c7bf8e4b645f4157#overview[the URL for the Red Hat Docling container image] and recompile the pipeline that you want to use.