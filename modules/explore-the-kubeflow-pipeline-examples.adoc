:_module-type: PROCEDURE

ifdef::context[:parent-context: {context}]
[id="explore-the-kubeflow-pipeline-examples_{context}"]
= Explore the kubeflow pipeline examples

[role='_abstract']

To get started with kubeflow pipelines, explore the provided examples. You can download and modify the example code to quickly create a Docling data processing or model training pipeline.

*Prerequisites*

* Install the data processing library, as described in _Set up your working environment_.

*Procedure*

. To access the link:https://github.com/opendatahub-io/data-processing/tree/main/kubeflow-pipelines[kubeflow pipeline examples], run the following command to clone the link:https://github.com/opendatahub-io/data-processing/tree/stable-3.0[data processing Git repository]:
+
[source, bash]
----
git clone https://github.com/opendatahub-io/data-processing -b stable-3.0
----
<<<<<<< HEAD
. Go to the `kubeflow-pipelines` directory which contains the following tested examples for running Docling as a scalable pipeline. For instructions on how to import, configure, and run the examples, see the link:https://github.com/opendatahub-io/data-processing/tree/stable-3.0/kubeflow-pipelines[README file] and the link:{rhoaidocshome}{default-format-url}/working_with_ai_pipelines/managing-ai-pipelines_ai-pipelines[Managing AI pipelines] guide.
=======
ifdef::upstream[]
. Go to the `kubeflow-pipelines` directory which contains the following tested examples for running Docling as a scalable pipeline. For instructions on how to import, configure, and run the examples, see the link:https://github.com/opendatahub-io/data-processing/tree/stable-3.0/kubeflow-pipelines[README file] and the {org-name} AI link:{odhdocshome}/working_with_ai_pipelines/#managing-ai-pipelines_ds-pipelines[Working with AI pipelines] guide. 
endif::[]
ifndef::upstream[]
. Go to the `kubeflow-pipelines` directory which contains the following tested examples for running Docling as a scalable pipeline. For instructions on how to import, configure, and run the examples, see the link:https://github.com/opendatahub-io/data-processing/tree/stable-3.0/kubeflow-pipelines[README file] and the {org-name} AI link:{rhoaidocshome}{default-format-url}/working_with_ai_pipelines/managing-ai-pipelines_ds-pipelines[Working with AI pipelines] guide. 
endif::[]
>>>>>>> e978828 (odh-1763-3 update links to rh docs)
+
* Standard Pipeline: For converting standard documents that contain text and structured elements. For more information, see the link:https://github.com/opendatahub-io/data-processing/blob/stable-3.0/kubeflow-pipelines/docling-standard/README.md[Standard Conversion Pipelines documentation].
* VLM (Vision Language Model): For converting highly complex or difficult-to-parse documents, such as those with custom instructions or complex layouts, or to add image descriptors. For more information, see the link:https://github.com/opendatahub-io/data-processing/blob/main/kubeflow-pipelines/docling-vlm/README.md[VLM Pipelines documentation].
