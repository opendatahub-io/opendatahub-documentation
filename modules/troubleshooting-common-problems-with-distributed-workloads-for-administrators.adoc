:_module-type: REFERENCE

[id="troubleshooting-common-problems-with-distributed-workloads-for-administrators_{context}"]
= Troubleshooting common problems with distributed training workloads for administrators

[role='_abstract']
If your users are experiencing errors in {productname-long} relating to distributed workloads, read this section to understand what could be causing the problem, and how to resolve the problem.

ifndef::upstream[]
If the problem is not documented here or in the release notes, contact {org-name} Support.
endif::[]

== A user's Ray cluster is in a suspended state

.Problem
The resource quota specified in the cluster queue configuration might be insufficient, or the resource flavor might not yet be created.

.Diagnosis
The user's Ray cluster head pod or worker pods remain in a suspended state. 
Check the status of the `Workload` resource that is created with the `RayCluster` resource.
The `status.conditions.message` field provides the reason for the suspended state, as shown in the following example:

[source,bash]
----
status:
 conditions:
   - lastTransitionTime: '2024-05-29T13:05:09Z'
     message: 'couldn''t assign flavors to pod set small-group-jobtest12: insufficient quota for nvidia.com/gpu in flavor default-flavor in ClusterQueue'

----

.Resolution
. Check whether the resource flavor is created, as follows:
ifdef::upstream,self-managed[]
.. In the {openshift-platform} console, select the user's project from the *Project* list. 
endif::[]
ifdef::cloud-service[]
..  In the OpenShift console, select the user's project from the *Project* list.
endif::[]
.. Click *Home -> Search*, and from the *Resources* list, select *ResourceFlavor*.
.. If necessary, create the resource flavor.
. Check the cluster queue configuration in the user's code, to ensure that the resources that they requested are within the limits defined for the project.
. If necessary, increase the resource quota. 

ifndef::upstream[]
For information about configuring resource flavors and quotas, see link:{rhoaidocshome}{default-format-url}/managing_openshift_ai/managing-workloads-with-kueue#configuring-quota-management-for-distributed-workloads_kueue[Configuring quota management for distributed workloads].
endif::[]
ifdef::upstream[]
For information about configuring resource flavors and quotas, see link:{odhdocshome}/managing-odh/#configuring-quota-management-for-distributed-workloads_managing-odh[Configuring quota management for distributed workloads].
endif::[]

== A user's Ray cluster is in a failed state

.Problem
The user might have insufficient resources.

.Diagnosis
The user's Ray cluster head pod or worker pods are not running.
When a Ray cluster is created, it initially enters a `failed` state. 
This failed state usually resolves after the reconciliation process completes and the Ray cluster pods are running.

.Resolution
If the failed state persists, complete the following steps:

ifdef::upstream,self-managed[]
. In the {openshift-platform} console, select the user's project from the *Project* list. 
endif::[]
ifdef::cloud-service[]
. In the OpenShift console, select the user's project from the *Project* list.
endif::[]
. Click *Workloads -> Pods*.
. Click the user's pod name to open the pod details page.
. Click the *Events* tab, and review the pod events to identify the cause of the problem.
. Check the status of the `Workload` resource that is created with the `RayCluster` resource.
The `status.conditions.message` field provides the reason for the failed state.

== A user receives a "failed to call webhook" error message for the CodeFlare Operator

.Problem
After the user runs the `cluster.apply()` command, the following error is shown:

[source,bash]
----
ApiException: (500)
Reason: Internal Server Error
HTTP response body: {"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"Internal error occurred: failed calling webhook \"mraycluster.ray.openshift.ai\": failed to call webhook: Post \"https://codeflare-operator-webhook-service.redhat-ods-applications.svc:443/mutate-ray-io-v1-raycluster?timeout=10s\": no endpoints available for service \"codeflare-operator-webhook-service\"","reason":"InternalError","details":{"causes":[{"message":"failed calling webhook \"mraycluster.ray.openshift.ai\": failed to call webhook: Post \"https://codeflare-operator-webhook-service.redhat-ods-applications.svc:443/mutate-ray-io-v1-raycluster?timeout=10s\": no endpoints available for service \"codeflare-operator-webhook-service\""}]},"code":500}
----

.Diagnosis
The CodeFlare Operator pod might not be running.

.Resolution

ifdef::upstream,self-managed[]
. In the {openshift-platform} console, select the user's project from the *Project* list. 
endif::[]
ifdef::cloud-service[]
. In the OpenShift console, select the user's project from the *Project* list.
endif::[]
. Click *Workloads -> Pods*.
. Verify that the CodeFlare Operator pod is running.
If necessary, restart the CodeFlare Operator pod.

. Review the logs for the CodeFlare Operator pod to verify that the webhook server is serving, as shown in the following example:
+
[source,bash]
----
INFO	controller-runtime.webhook	  Serving webhook server	{"host": "", "port": 9443}
----


== A user's Ray cluster does not start

.Problem
After the user runs the `cluster.apply()` command, when they run either the `cluster.details()` command or the `cluster.status()` command, the Ray cluster status remains as `Starting` instead of changing to `Ready`.
No pods are created.

.Diagnosis
Check the status of the `Workload` resource that is created with the `RayCluster` resource.
The `status.conditions.message` field provides the reason for remaining in the `Starting` state.
Similarly, check the `status.conditions.message` field for the `RayCluster` resource. 

.Resolution

ifdef::upstream,self-managed[]
. In the {openshift-platform} console, select the user's project from the *Project* list. 
endif::[]
ifdef::cloud-service[]
. In the OpenShift console, select the user's project from the *Project* list.
endif::[]
. Click *Workloads -> Pods*.
. Verify that the KubeRay pod is running.
If necessary, restart the KubeRay pod.

. Review the logs for the KubeRay pod to identify errors.


== A user cannot create a Ray cluster or submit jobs

.Problem
After the user runs the `cluster.apply()` command, an error similar to the following text is shown:

[source,bash]
----
RuntimeError: Failed to get RayCluster CustomResourceDefinition: (403)
Reason: Forbidden
HTTP response body: {"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"rayclusters.ray.io is forbidden: User \"system:serviceaccount:regularuser-project:regularuser-workbench\" cannot list resource \"rayclusters\" in API group \"ray.io\" in the namespace \"regularuser-project\"","reason":"Forbidden","details":{"group":"ray.io","kind":"rayclusters"},"code":403}
----

.Diagnosis
The correct OpenShift login credentials are not specified in the `TokenAuthentication` section of the user's notebook code.

.Resolution
. Advise the user to identify and specify the correct OpenShift login credentials as follows:

ifdef::upstream,self-managed[]
.. In the {openshift-platform} console header, click your username and click *Copy login command*.
endif::[]
ifdef::cloud-service[]
.. In the OpenShift console header, click your username and click *Copy login command*.
endif::[]

.. In the new tab that opens, log in as the user whose credentials you want to use.
.. Click *Display Token*.
.. From the *Log in with this token* section, copy the `token` and `server` values.

.. Specify the copied `token` and `server` values in your notebook code as follows:
+
[source,bash,subs="+quotes"]
----
auth = TokenAuthentication(
    token = "_<token>_",
    server = "_<server>_",
    skip_tls=False
)
auth.login()
----

. Verify that the user has the correct permissions and is part of the {user-group} group.

[role='_additional-resources']
== Additional resources

ifdef::upstream[]
* link:{odhdocshome}/working-with-distributed-workloads/#troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads[Troubleshooting common problems with distributed workloads for users]
* link:{odhdocshome}/managing-odh/#troubleshooting-common-problems-with-Kueue_kueue[Troubleshooting common problems with Kueue]
endif::[]
ifndef::upstream[]
* link:{rhoaidocshome}{default-format-url}/working_with_distributed_workloads/troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads[Troubleshooting common problems with distributed workloads for users]
* link:{rhoaidocshome}{default-format-url}/managing_openshift_ai/managing-workloads-with-kueue#troubleshooting-common-problems-with-Kueue_kueue[Troubleshooting common problems with Kueue]
endif::[]
