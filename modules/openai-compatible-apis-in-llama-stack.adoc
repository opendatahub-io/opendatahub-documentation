:_module-type: REFERENCE
[id="openai-compatible-apis-in-Llama-Stack_{context}"]
= OpenAI-compatible APIs in Llama Stack

[role="_abstract"]
{productname-short} includes a Llama Stack component that exposes OpenAI-compatible APIs. These APIs enable you to reuse existing OpenAI SDKs, tools, and workflows directly within your {openshift-platform} environment, without changing your client code. This compatibility layer supports retrieval-augmented generation (RAG), inference, and embedding workloads by using the same endpoints, schemas, and authentication model as OpenAI.

This compatibility layer has the following capabilities:

* *Standardized endpoints*: REST API paths align with OpenAI specifications.  
* *Schema parity:* Request and response fields follow OpenAI data structures.  

[NOTE]
====
When connecting OpenAI SDKs or third-party tools to {productname-short}, you must update the client configuration to use your deployment's Llama Stack route as the `base_url`.

When you use OpenAI-compatible SDKs, the `base_url` **must include the `/v1` path suffix** so that requests are routed to the OpenAI-compatible API surface exposed by Llama Stack.
====

[IMPORTANT]
====
When you use OpenAI SDKs or send raw HTTP requests to Llama Stack, always include the `/v1` path suffix in the base URL.

For example:
`http://llama-stack-service:8321/v1`

Using the service endpoint without `/v1` results in request failures.
====

== Supported OpenAI-compatible APIs in {productname-short}

=== Chat Completions API
* *Endpoint:* `/v1/openai/v1/chat/completions`.  
* *Providers:* All inference back ends deployed through {productname-short}.
* *Support level:* Technology Preview. 

The Chat Completions API enables conversational, message-based interactions with models served by Llama Stack in {productname-short}.

---

=== Completions API
* *Endpoint:* `/v1/openai/v1/completions`.  
* *Providers:* All inference backends managed by {productname-short}.
* *Support level:* Technology Preview. 

The Completions API supports single-turn text generation and prompt completion.

---

=== Embeddings API
* *Endpoint:* `/v1/openai/v1/embeddings`.  
* *Providers:* All embedding models enabled in {productname-short}.

The Embeddings API generates numerical embeddings for text or documents that can be used in downstream semantic search or RAG applications.

---

=== Files API
* *Endpoint:* `/v1/openai/v1/files`.  
* *Providers:* File system-based file storage provider for managing files and documents stored locally in your cluster.
* *Support level:* Technology Preview. 

The Files API manages file uploads for use in embedding and retrieval workflows.

---

=== Vector Stores API
* *Endpoint:* `/v1/openai/v1/vector_stores/`.  
* *Providers:* Inline and remote vector store providers configured in {productname-short}.
* *Support level:* Technology Preview. 

The Vector Stores API manages the creation, configuration, and lifecycle of vector store resources in Llama Stack. Through this API, you can create new vector stores, list existing ones, delete unused stores, and query their metadata, all using OpenAI-compatible request and response formats.  

---

=== Vector Store Files API
* *Endpoint:* `/v1/openai/v1/vector_stores/{vector_store_id}/files`.  
* *Providers:* Local inline provider configured for file storage and retrieval.
* *Support level:* Developer Preview. 

The Vector Store Files API implements the OpenAI Vector Store Files interface and manages the association between document files and vector stores used for RAG workflows. 

---

=== Models API
* *Endpoint:* `/v1/openai/v1/models`.  
* *Providers:* All model-serving back ends configured within {productname-short}.
* *Support level:* Technology Preview. 

The Models API lists and retrieves available model resources from the Llama Stack deployment running on {productname-short}. By using the Models API, you can enumerate models, view their capabilities, and verify deployment status through a standardized OpenAI-compatible interface.

---

=== Responses API
* *Endpoint:* `/v1/openai/v1/responses`.  
* *Providers:* All agents, inference, and vector providers configured in {productname-short}.
* *Support level:* Developer Preview. 

The Responses API generates model outputs by combining inference, file search, and tool-calling capabilities through a single OpenAI-compatible endpoint. It is particularly useful for retrieval-augmented generation (RAG) workflows that rely on the `file_search` tool to retrieve context from vector stores.

[NOTE]
====
The Responses API is an experimental feature that is still under active development in {productname-short}. While the API is already functional and suitable for evaluation, some endpoints and parameters remain under implementation and might change in future releases. This API is provided for testing and feedback purposes only and is not recommended for production use.
====

---

[role="_additional-resources"]
.Additional resources

* link:https://llamastack.github.io/docs/providers/openai[OpenAI API compatibility in Llama Stack]
