:_module-type: REFERENCE
[id="openai-compatible-apis-in-Llama-Stack_{context}"]
= OpenAI-compatible APIs in Llama Stack

[role="_abstract"]
{productname-short} includes a Llama Stack component that exposes OpenAI-compatible APIs. These APIs enable you to reuse existing OpenAI SDKs, tools, and workflows directly within your {openshift-platform} environment without changing client-side logic. This compatibility layer supports inference, embeddings, and retrieval-augmented generation (RAG) workflows by using OpenAI-compatible endpoints, schemas, and authentication models.

This compatibility layer has the following capabilities:

* *Standardized endpoints*: REST API paths align with OpenAI specifications.  
* *Schema parity:* Request and response fields follow OpenAI data structures.  

[NOTE]
====
When connecting OpenAI SDKs or third-party tools to {productname-short}, you must configure the client to use your Llama Stack service route as the `base_url`. Do not use the public OpenAI service endpoint. All API requests must be sent to the Llama Stack endpoint running inside your {openshift-platform} cluster.
====

== Supported OpenAI-compatible APIs in {productname-short}

=== Chat Completions API
* *Endpoint:* `/v1/chat/completions`.
* *Providers:* All inference providers configured in {productname-short}.
* *Support level:* Technology Preview. 

The Chat Completions API enables conversational, message-based interactions with models served by Llama Stack in {productname-short}.

---

=== Completions API
* *Endpoint:*  `/v1/completions`.  
* *Providers:* All inference providers configured in {productname-short}.
* *Support level:* Technology Preview. 

The Completions API supports single-turn text generation and prompt completion.

---

=== Embeddings API
* *Endpoint:* `/v1/embeddings`.
* *Providers:* All embedding providers enabled in {productname-short}.

The Embeddings API generates numerical embeddings for text or documents that can be used in downstream semantic search or RAG applications.

---

=== Files API
* *Endpoint:* `/v1/files`.  
* *Providers:* File storage providers configured in {productname-short}.
* *Support level:* Technology Preview. 

The Files API manages file uploads for use in embedding and retrieval workflows.

---

=== Vector Stores API
* *Endpoint:* `/v1/vector_stores`.  
* *Providers:* Vector store providers configured in {productname-short}.
* *Support level:* Technology Preview. 

The Vector Stores API manages the creation, configuration, and lifecycle of vector store resources in Llama Stack. Through this API, you can create new vector stores, list existing ones, delete unused stores, and query their metadata, all using OpenAI-compatible request and response formats.  

---

=== Vector Store Files API
* *Endpoint:* `/v1/vector_stores/{vector_store_id}/files`.  
* *Providers:* File ingestion providers configured for vector store usage in {productname-short}.
* *Support level:* Developer Preview. 

The Vector Store Files API manages the association between uploaded files and vector stores used for retrieval-augmented generation (RAG). Through this API, you can attach previously uploaded files to a vector store and control how content is chunked and prepared for retrieval by using configurable chunking strategies.

---

=== Models API
* *Endpoint:* `/v1/models`.  
* *Providers:* All model-serving back ends configured within {productname-short}.
* *Support level:* Technology Preview. 

The Models API lists and retrieves available model resources from the Llama Stack deployment running on {productname-short}. By using the Models API, you can enumerate models, view their capabilities, and verify deployment status through a standardized OpenAI-compatible interface.

---

=== Responses API
* *Endpoint:* `/v1/responses`.  
* *Providers:* Inference, file, and vector store providers configured in {productname-short}.
* *Support level:* Developer Preview. 

The Responses API generates model outputs by combining inference and retrieval capabilities through a single OpenAI-compatible endpoint. It is commonly used for RAG workflows that rely on vector store-backed file search.

[NOTE]
====
The Responses API is under active development in {productname-short}. Some parameters and behaviors might change in future releases. This API is intended for evaluation and testing purposes and is not recommended for production use.
====

---

[role="_additional-resources"]
.Additional resources

* link:https://llamastack.github.io/docs/providers/openai[OpenAI API Compatibility]
