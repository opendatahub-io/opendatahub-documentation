:_module-type: REFERENCE

[id='tested-verified-runtimes_{context}']
= Tested and verified model-serving runtimes

[role='_abstract']

Tested and verified runtimes are community versions of model-serving runtimes that have been tested and verified against specific versions of {productname-short}. 

{org-name} tests the current version of a tested and verified runtime each time there is a new version of {productname-short}. If a new version of a tested and verified runtime is released in the middle of an {productname-short} release cycle, it will be tested and verified in an upcoming release.

[NOTE]
--
Tested and verified runtimes are not directly supported by {org-name}. You are responsible for ensuring that you are licensed to use any tested and verified runtimes that you add, and for correctly configuring and maintaining them.
--

ifndef::upstream[]
For more information, see link:https://access.redhat.com/articles/7089743[Tested and verified runtimes in {productname-short}].
endif::[]

.Model-serving runtimes

|===
| Name | Description | Exported model format 

| IBM Z Accelerated for NVIDIA Triton Inference Server | An open-source AI inference server that standardizes model deployment and execution, delivering streamlined, highâ€‘performance inference at scale | Python, ONNX-MLIR, Snap ML (C++), PyTorch
| NVIDIA Triton Inference Server | An open-source inference-serving software for fast and scalable AI in applications. | TensorRT, TensorFlow, PyTorch, ONNX, OpenVINO, Python, RAPIDS FIL, and more
| Seldon MLServer | An open-source inference server designed to simplify the deployment of machine learning models. | Scikit-Learn (sklearn), XGBoost, LightGBM, CatBoost, HuggingFace and MLflow

|===

.Deployment requirements

|===
| Name | Default protocol | Additional protocol | Model mesh support | Single node OpenShift support | Deployment mode

| IBM Z Accelerated for NVIDIA Triton Inference Server | gRPC | REST | No | Yes | Raw
| NVIDIA Triton Inference Server | gRPC | REST | Yes | Yes | Raw and serverless
| Seldon MLServer | gRPC | REST | No | Yes | Raw and serverless

|===


[NOTE]
--
The `alibi-detect` and `alibi-explain` libraries from Seldon are under the Business Source License 1.1 (BSL 1.1). These libraries are not tested, verified, or supported by {org-name} as part of the certified *Seldon MLServer* runtime. It is not recommended that you use these libraries in production environments with the runtime.
--

[role="_additional-resources"]
.Additional resources
ifdef::upstream[]
* link:{odhdocshome}/serving-models/#inference-endpoints_serving-large-models[Inference endpoints]
endif::[]

ifndef::upstream[]
* link:{rhoaidocshome}{default-format-url}/serving_models/serving-large-models_serving-large-models#inference-endpoints_serving-large-models[Inference endpoints]
endif::[]

* link:https://github.com/IBM/ibmz-accelerated-for-nvidia-triton-inference-server?tab=readme-ov-file#rest-apis-[Using the IBM Z Accelerated for NVIDIA Triton Inference Server Container Image]
