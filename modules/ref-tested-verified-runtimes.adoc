:_module-type: REFERENCE

[id='tested-verified-runtimes_{context}']
= Tested and verified model-serving runtimes

[role='_abstract']

Tested and verified runtimes are community versions of model-serving runtimes that have been tested and verified against specific versions of {productname-short}. 

{org-name} tests the current version of a tested and verified runtime each time there is a new version of {productname-short}. If a new version of a tested and verified runtime is released in the middle of an {productname-short} release cycle, it will be tested and verified in an upcoming release.

ifndef::upstream[]
For more information, see link:https://access.redhat.com/articles/7089743[Tested and verified runtimes in {productname-short}].
endif::[]

[NOTE]
--
Tested and verified runtimes are not directly supported by {org-name}. You are responsible for ensuring that you are licensed to use any tested and verified runtimes that you add, and for correctly configuring and maintaining them.
--

.Model-serving runtimes

|===
| Name | Description | Exported model format 

| NVIDIA Triton Inference Server | An open-source inference-serving software for fast and scalable AI in applications. | TensorRT, TensorFlow, PyTorch, ONNX, OpenVINO, Python, RAPIDS FIL, and more

|===

.Deployment requirements

|===
| Name | Default protocol | Additonal protocol | Model mesh support | Single node OpenShift support | Deployment mode

| NVIDIA Triton Inference Server | gRPC | REST | Yes | Yes | Raw and serverless

|===

[role="_additional-resources"]
.Additional resources
ifdef::upstream[]
* link:{odhdocshome}/serving-models/#inference-endpoints_serving-large-models[Inference endpoints]
endif::[]

ifndef::upstream[]
* link:{rhoaidocshome}{default-format-url}/serving_models/serving-large-models_serving-large-models#inference-endpoints_serving-large-models[Inference endpoints]
endif::[]

