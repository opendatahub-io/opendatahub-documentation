:_module-type: PROCEDURE

[id="configuring-ragas-remote-provider-for-production_{context}"]
= Configuring the RAGAS remote provider for production

[role='_abstract']
You can configure the RAGAS remote provider to run evaluations as distributed jobs using {productname-short} pipelines. The remote provider enables production-scale evaluations by running RAGAS in a separate Kubeflow Pipelines environment, providing resource isolation, improved scalability, and integration with CI/CD workflows.

.Prerequisites
* You have cluster administrator privileges for your {openshift-platform} cluster.

* You have installed the {openshift-cli} as described in the appropriate documentation for your cluster:
ifdef::upstream,self-managed[]
** link:https://docs.redhat.com/en/documentation/openshift_container_platform/{ocp-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli[Installing the OpenShift CLI^] for OpenShift Container Platform
** link:https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws/{rosa-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli[Installing the OpenShift CLI^] for {rosa-productname}
endif::[]
ifdef::cloud-service[]
** link:https://docs.redhat.com/en/documentation/openshift_dedicated/{osd-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli[Installing the OpenShift CLI^] for OpenShift Dedicated
** link:https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws_classic_architecture/{rosa-classic-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli[Installing the OpenShift CLI^] for {rosa-classic-productname}
endif::[]

* You have configured a data science pipeline server in your data science project.
ifdef::upstream[]
For more information, see link:{odhdocshome}/working-with-data-science-pipelines/#configuring-a-pipeline-server_ds-pipelines[Configuring a pipeline server].
endif::[]
ifndef::upstream[]
For more information, see link:{rhoaidocshome}{default-format-url}/working_with_data_science_pipelines/configuring-a-pipeline-server_ds-pipelines[Configuring a pipeline server].
endif::[]

* You have activated the Llama Stack Operator in {productname-short}.
ifdef::upstream[]
For more information, see link:{odhdocshome}/working-with-rag/#installing-the-llama-stack-operator_rag[Installing the Llama Stack Operator].
endif::[]
ifndef::upstream[]
For more information, see link:{rhoaidocshome}{default-format-url}/working_with_rag/installing-the-llama-stack-operator_rag[Installing the Llama Stack Operator].
endif::[]

* You have deployed a Llama model with KServe.
ifdef::upstream[]
For more information, see link:{odhdocshome}/working-with-rag/#deploying-a-llama-model-with-kserve_rag[Deploying a Llama model with KServe].
endif::[]
ifndef::upstream[]
For more information, see link:{rhoaidocshome}{default-format-url}/working_with_rag/deploying-a-rag-stack-in-a-data-science-project_rag#Deploying-a-llama-model-with-kserve_rag[Deploying a Llama model with KServe].
endif::[]

* You have configured S3-compatible object storage for storing evaluation results.
ifdef::upstream[]
For more information, see link:{odhdocshome}/working-on-data-science-projects/#adding-a-connection-to-your-data-science-project_projects[Adding a connection to your data science project].
endif::[]
ifndef::upstream[]
For more information, see link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects/using-connections_projects#adding-a-connection-to-your-data-science-project_projects[Adding a connection to your data science project].
endif::[]

* You have created a data science project.

.Procedure
. In a terminal window, if you are not already logged in to your OpenShift cluster, log in to the {openshift-cli} as shown in the following example:
+
[source,terminal]
----
$ oc login <openshift_cluster_url> -u <username> -p <password>
----

. Navigate to your data science project:
+
[source,terminal"]
----
$ oc project <project_name>
----

. Create a secret for storing S3 credentials:
+
[source,terminal]
----
$ oc create secret generic ragas-s3-credentials \
  --from-literal=AWS_ACCESS_KEY_ID=<your_access_key> \
  --from-literal=AWS_SECRET_ACCESS_KEY=<your_secret_key> \
  --from-literal=AWS_DEFAULT_REGION=<your_region>
----
+
[NOTE]
====
Replace the placeholder values with your actual S3 credentials. The RAGAS remote provider uses these credentials to store evaluation results in S3-compatible storage.
====

. Create a secret for the Kubeflow Pipelines API token:
.. Get your token by running the following command:
+
[source,terminal]
----
$ export KUBEFLOW_PIPELINES_TOKEN=$(oc whoami -t)
----
.. Create the secret by running the following command:
+
[source,terminal]
----
$ oc create secret generic kubeflow-pipelines-token \
  --from-literal=KUBEFLOW_PIPELINES_TOKEN="$PIPELINES_TOKEN"
----
+
[IMPORTANT]
====
The Llama Stack distribution service account does not have privileges to create pipeline runs. This secret provides the necessary authentication token for creating and managing pipeline runs.
====

. Create a secret for storing your inference model information:
+
[source,terminal]
----
$ export INFERENCE_MODEL="llama-3-2-3b"
$ export VLLM_URL="https://llama-32-3b-instruct-predictor:8443/v1"
$ export VLLM_TLS_VERIFY="false"  # Use "true" in production
$ export VLLM_API_TOKEN="<token_identifier>"

$ oc create secret generic llama-stack-inference-model-secret \
  --from-literal INFERENCE_MODEL="$INFERENCE_MODEL" \
  --from-literal VLLM_URL="$VLLM_URL" \
  --from-literal VLLM_TLS_VERIFY="$VLLM_TLS_VERIFY" \
  --from-literal VLLM_API_TOKEN="$VLLM_API_TOKEN"
----

. Get the Kubeflow Pipelines endpoint URL. This URL is used in a later step for creating a ConfigMap for the RAGAS remote provider configuration:
+
[source,terminal]
----
$ export KFP_ENDPOINT=$(oc get route -n <project_name> -l app=ds-pipeline-dspa -o jsonpath='{.items[0].spec.host}')

$ echo "Kubeflow Pipelines endpoint: https://$KFP_ENDPOINT"
----

. Create an external route for the Llama Stack server:
+
[NOTE]
====
The KUBEFLOW_LLAMA_STACK_URL must be an external route that is accessible from the Kubeflow Pipeline pods.
====
+
The route is automatically created when you deploy the LlamaStackDistribution. You can get the route URL with the following command:
+
[source,subs="+quotes"]
----
$ export LLAMA_STACK_URL=$(oc get route llama-stack-ragas-remote -o jsonpath='{.spec.host}')

$ echo "Llama Stack URL: https://$LLAMA_STACK_URL"
----

. Create a ConfigMap for the RAGAS remote provider configuration. For example, create a kubeflow-ragas-config.yaml file as follows:
+
.Example kubeflow-ragas-config.yaml
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: kubeflow-ragas-config
  namespace: <project_name>
data:
  EMBEDDING_MODEL: "all-MiniLM-L6-v2"
  KUBEFLOW_LLAMA_STACK_URL: "<external_llama_stack_url>"
  KUBEFLOW_PIPELINES_ENDPOINT: <https://<kfp_endpoint>
  KUBEFLOW_NAMESPACE: <project_name>
  KUBEFLOW_BASE_IMAGE: "quay.io/trustyai/llama-stack-ragas:latest"
  KUBEFLOW_RESULTS_S3_PREFIX: "s3://<bucket_name>/ragas-results"
  KUBEFLOW_S3_CREDENTIALS_SECRET_NAME: <ragas_s3_credentials>
----
+
* `EMBEDDING_MODEL`: used by RAGAS for semantic similarity calculations.
* `KUBEFLOW_LLAMA_STACK_URL`: the external route URL for the Llama Stack server. This must be accessible from the Kubeflow Pipeline pods.
* `KUBEFLOW_PIPELINES_ENDPOINT`: the Kubeflow Pipelines API endpoint URL.
* `KUBEFLOW_NAMESPACE`: the namespace where pipeline runs is be executed.
* `KUBEFLOW_BASE_IMAGE`: the container image used for RAGAS evaluation pipeline components.
* `KUBEFLOW_RESULTS_S3_PREFIX`: the S3 path prefix where evaluation results will be stored.
* `KUBEFLOW_S3_CREDENTIALS_SECRET_NAME`: name of the secret containing S3 credentials.

. Apply the ConfigMap:
+
[source,subs="+quotes"]
----
$ oc apply -f kubeflow-ragas-config.yaml
----

. Create a Llama Stack distribution configuration file with the RAGAS remote provider. For example, create a llama-stack-ragas-remote.yaml as follows:
+
.Example llama-stack-ragas-remote.yaml 
[source,yaml,subs="+quotes"]
----
apiVersion: llamastack.io/v1alpha1
kind: LlamaStackDistribution
metadata:
  name: llama-stack-ragas-remote
spec:
  replicas: 1
  server:
    containerSpec:
      resources:
        requests:
          cpu: 4
          memory: "12Gi"
        limits:
          cpu: 6
          memory: "14Gi"
      env:
        - name: INFERENCE_MODEL
          valueFrom:
            secretKeyRef:
              key: INFERENCE_MODEL
              name: llama-stack-inference-model-secret
              optional: true
        - name: VLLM_MAX_TOKENS
          value: "4096"
        - name: VLLM_URL
          valueFrom:
            secretKeyRef:
              key: VLLM_URL
              name: llama-stack-inference-model-secret
              optional: true
        - name: VLLM_TLS_VERIFY
          valueFrom:
            secretKeyRef:
              key: VLLM_TLS_VERIFY
              name: llama-stack-inference-model-secret
              optional: true
        - name: VLLM_API_TOKEN
          valueFrom:
            secretKeyRef:
              key: VLLM_API_TOKEN
              name: llama-stack-inference-model-secret
              optional: true
        - name: MILVUS_DB_PATH
          value: ~/milvus.db
        - name: FMS_ORCHESTRATOR_URL
          value: "http://localhost"
        - name: KUBEFLOW_PIPELINES_ENDPOINT
          valueFrom:
            configMapKeyRef:
              key: KUBEFLOW_PIPELINES_ENDPOINT
              name: kubeflow-ragas-config
              optional: true
        - name: KUBEFLOW_NAMESPACE
          valueFrom:
            configMapKeyRef:
              key: KUBEFLOW_NAMESPACE
              name: kubeflow-ragas-config
              optional: true
        - name: KUBEFLOW_BASE_IMAGE
          valueFrom:
            configMapKeyRef:
              key: KUBEFLOW_BASE_IMAGE
              name: kubeflow-ragas-config
              optional: true
        - name: KUBEFLOW_LLAMA_STACK_URL
          valueFrom:
            configMapKeyRef:
              key: KUBEFLOW_LLAMA_STACK_URL
              name: kubeflow-ragas-config
              optional: true
        - name: KUBEFLOW_RESULTS_S3_PREFIX
          valueFrom:
            configMapKeyRef:
              key: KUBEFLOW_RESULTS_S3_PREFIX
              name: kubeflow-ragas-config
              optional: true
        - name: KUBEFLOW_S3_CREDENTIALS_SECRET_NAME
          valueFrom:
            configMapKeyRef:
              key: KUBEFLOW_S3_CREDENTIALS_SECRET_NAME
              name: kubeflow-ragas-config
              optional: true
        - name: EMBEDDING_MODEL
          valueFrom:
            configMapKeyRef:
              key: EMBEDDING_MODEL
              name: kubeflow-ragas-config
              optional: true
        - name: KUBEFLOW_PIPELINES_TOKEN
          valueFrom:
            secretKeyRef:
              key: KUBEFLOW_PIPELINES_TOKEN
              name: kubeflow-pipelines-token
              optional: true
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              key: AWS_ACCESS_KEY_ID
              name: aws-credentials
              optional: true
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              key: AWS_SECRET_ACCESS_KEY
              name: aws-credentials
              optional: true
        - name: AWS_DEFAULT_REGION
          valueFrom:
            secretKeyRef:
              key: AWS_DEFAULT_REGION
              name: aws-credentials
              optional: true
      name: llama-stack
      port: 8321
    distribution:
      name: rh-dev
----

. Deploy the Llama Stack distribution:
+
[source,terminal]
----
$ oc apply -f llama-stack-ragas-remote.yaml
----

. Wait for the deployment to complete:
+
[source,terminal]
----
$ oc get pods -w
----
+
Wait until the `llama-stack-ragas-remote` pod status shows `Running`.

.Verification
. Verify that the Llama Stack server is running:
+
[source,terminal]
----
$ oc get pods -l app=llama-stack-ragas-remote
----
+
You should see output similar to the following:
+
[source,terminal]
----
NAME                                       READY   STATUS    RESTARTS   AGE
llama-stack-ragas-remote-7b8d9c5f-xyz12    1/1     Running   0          2m
----

. Verify that the remote provider can create pipeline runs:
+
[source,python]
----
from llama_stack_client import LlamaStackClient

client = LlamaStackClient(base_url="https://__<llama-stack-route-url>__")

# List available eval providers
providers = client.providers.list()
print("Available eval providers:")
for provider in providers.eval:
    print(f"  - {provider.provider_id} ({provider.provider_type})")

# Expected output should include:
#   - trustyai_ragas_remote (remote::trustyai_ragas)
----

. Run a test evaluation to verify the pipeline integration:
+
[source,python]
----
from datasets import Dataset

# Create a small test dataset
test_data = {
    "question": ["What is 2+2?"],
    "answer": ["The sum of 2 and 2 is 4."],
    "contexts": [["Basic arithmetic: 2 + 2 = 4"]],
    "ground_truth": ["4"]
}

dataset = Dataset.from_dict(test_data)

# Register and run evaluation
client.datasets.register(
    dataset_id="ragas-test",
    dataset_def=dataset,
    provider_id="huggingface"
)

response = client.eval.evaluate_rows(
    task_id="ragas-test-benchmark",
    input_rows=dataset,
    scoring_functions=["faithfulness", "answer_relevancy"]
)

print(f"Pipeline run created: {response.job_id}")
----

. Check that the pipeline run appears in the {productname-short} dashboard:
+
.. Navigate to *Data Science Projects* in the {productname-short} dashboard.
.. Click your project name.
.. Click the *Pipelines* tab.
.. Click *Runs*.
.. Verify that a new pipeline run for the RAGAS evaluation appears in the list.

.Next steps
ifdef::upstream[]
* link:{odhdocshome}/working-with-rag/#evaluating-rag-system-quality-with-ragas_rag[Evaluating RAG system quality with RAGAS metrics]
