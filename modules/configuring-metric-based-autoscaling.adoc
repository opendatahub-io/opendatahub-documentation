:_module-type: PROCEDURE

[id="configuring-metrics-based-autoscaling_{context}"]
= Configuring metrics-based autoscaling

[role="_abstract"]

ifndef::upstream[]
[IMPORTANT]
====
ifdef::self-managed[]
Metrics-based autoscaling is currently available in {productname-long} {vernum} as a Technology Preview feature.
endif::[]
ifdef::cloud-service[]
Metrics-based autoscaling is currently available in {productname-long} as a Technology Preview feature.
endif::[]
Technology Preview features are not supported with {org-name} production service level agreements (SLAs) and might not be functionally complete.
{org-name} does not recommend using them in production.
These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.

For more information about the support scope of {org-name} Technology Preview features, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].
====
endif::[]

Knative-based autoscaling is not available in KServe RawDeployment mode. However, you can enable metrics-based autoscaling for an inference service in this mode. Metrics-based autoscaling helps you efficiently manage accelerator resources, lower operational costs, and ensure that your inference services meet performance requirements.

To set up autoscaling for your inference service in KServe RawDeployment mode, install and configure the OpenShift Custom Metrics Autoscaler (CMA), which is based on Kubernetes Event-driven Autoscaling (KEDA). You can then use various model runtime metrics available in OpenShift Monitoring to trigger autoscaling of your inference service, such as KVCache utilization, Time to First Token (TTFT), and Concurrency. 

.Prerequisites
* You have cluster administrator privileges for your {openshift-platform} cluster.
* You have installed the CMA operator on your cluster. For more information, see link:https://docs.redhat.com/en/documentation/openshift_container_platform/{ocp-latest-version}/html/nodes/automatically-scaling-pods-with-the-custom-metrics-autoscaler-operator#nodes-cma-autoscaling-custom-install[Installing the custom metrics autoscaler].
* You have installed the cert-manager Operator in {openshift-platform} by using the web console as described in link:https://docs.redhat.com/en/documentation/openshift_container_platform/{ocp-latest-version}/html/security_and_compliance/cert-manager-operator-for-red-hat-openshift#installing-the-cert-manager-operator-for-red-hat-openshift[Installing the cert-manager Operator for Red Hat OpenShift].
+
[NOTE]
====
* You must configure the `KedaController` resource after installing the CMA operator. 
* The `odh-controller` automatically creates the `TriggerAuthentication`, `ServiceAccount`, `Role`, `RoleBinding`, and `Secret` resources to allow CMA access to OpenShift Monitoring metrics. 
====
* You have enabled User Workload Monitoring (UWM) for your cluster. For more information, see link:https://docs.redhat.com/en/documentation/openshift_container_platform/{ocp-latest-version}/html/monitoring/configuring-user-workload-monitoring[Configuring user workload monitoring].
* You have deployed a model on the model serving platform.

.Procedure

. Log in to the {openshift-platform} console as a cluster administrator.
. In the *Administrator* perspective, click *Home* -> *Search*.
. Select the project where you have deployed your model.
. From the *Resources* dropdown menu, select *InferenceService*.
. Click the `InferenceService` for your deployed model and then click *YAML*.
. Under `spec.predictor`, define a metric-based autoscaling policy similar to the following example:
+
[source,yaml]
----
kind: InferenceService
metadata:
  name: my-inference-service
  namespace: my-namespace
  annotations:
    serving.kserve.io/autoscalerClass: keda
spec:
  predictor:
    minReplicas: 1
    maxReplicas: 5
    autoscaling:
      metrics:
        - type: External
          external:
            metric:
              backend: "prometheus"
              serverAddress: "https://thanos-querier.openshift-monitoring.svc:9092"
              query: vllm:num_requests_waiting
          authenticationRef:
            name: inference-prometheus-auth
          authModes: bearer
          target:
            type: Value
            value: 2
----
+
The example configuration sets up the inference service to autoscale between 1 and 5 replicas based on the number of requests waiting to be processed, as indicated by the `vllm:num_requests_waiting` metric.
. Click *Save*.

.Verification

* Confirm that the KEDA `ScaledObject` resource is created:
+
[source, console]
----
oc get scaledobject -n <namespace>
---- 

//[role="_additional-resources"]
//.Additional resources
