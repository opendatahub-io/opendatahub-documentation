:_module-type: CONCEPT
[id="deployment-strategies-for-resource-optimization_{context}"]
= Deployment strategies for resource optimization

[role="_abstract"]
To optimize resource usage and manage downtime during model rollouts, you can configure the deployment strategy for your inference services. Choosing the appropriate strategy depends on your cluster's available quotas, especially hardware accelerators such as GPUs, and your tolerance for service interruptions.

There are two primary deployment strategies available for model serving:

Rolling update::
This strategy ensures zero downtime and continuous availability of the model. New inference service pods start while the existing pods are running. Traffic is switched to the new pods only after they are fully ready, and then the old pods are terminated.
+
However, rolling updates require increased resources like CPU, memory, and GPUs during the update process. Plan for approximately 200% of the pod requests as headroom during the transition because parallel instances exist briefly.

Recreate::
This strategy prioritizes resource conservation over availability. All existing inference service pods are terminated before the new pods attempt to launch.
+
However, this method requires a period of downtime. The model endpoint is unavailable and returns errors between the termination of the old pod and the readiness of the new pod.

== Choosing a deployment strategy

Choose the deployment strategy that best fits your availability requirements and resource quotas. The following table compares the rolling update and recreate strategies.

[cols="1,2,2,2", options="header"]
|===
|Strategy
|Description
|Resource impact
|Recommended scenarios

|*Rolling update*
|Replaces pods gradually to ensure zero downtime. Traffic switches to new pods only after they are fully ready.
|*High:* Requires approximately 200% of the request resources to host parallel instances during the transition.
a|
* *Production workloads:* Environments where the model must remain accessible without interruption.
* *High-quota clusters:* Namespaces with sufficient headroom to accommodate parallel instances.

|*Recreate*
|Terminates the old pod before starting the new one. Service is unavailable during the transition.
|*Low:* Consumption does not exceed 100%. Prevents _Insufficient Resources_ errors.
a|
* *Resource-constrained environments:* Projects using scarce hardware, such as high-end GPUs, where double allocation is not possible.
* *Development and staging:* Environments where downtime does not impact business operations.
* *Batch processing:* Workflows where immediate availability is not critical.
* *Maintenance windows:* Periods where service unavailability is expected.
|===


[IMPORTANT]
====
The *Recreate* strategy severs the connection to the old pod immediately. Ensure that your traffic routing gateway and client applications can handle a temporary gap in service before applying this strategy.
====

[NOTE]
====
The *Recreate* deployment strategy is available for all runtimes except *Distributed inference with llm-d*. If you select the *Distributed inference with llm-d* runtime, the deployment strategy options are not displayed and the system defaults to the Recreate strategy.
====
