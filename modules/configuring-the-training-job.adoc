:_module-type: PROCEDURE

[id="configuring-the-training-job_{context}"]
= Configuring the training job

[role='_abstract']
Before you can use a training job to tune a model, you must configure the training job. 
The example training job in this section is based on the IBM and Hugging Face tuning example provided in link:https://github.com/foundation-model-stack/fms-hf-tuning/tree/main/examples/prompt_tuning_twitter_complaints[GitHub]. 


.Prerequisites
ifdef::upstream,self-managed[]
* You have logged in to {openshift-platform}.
endif::[]
ifdef::cloud-service[]
* You have logged in to OpenShift.
endif::[]

ifndef::upstream[]
* You have access to a data science cluster that is configured to run distributed workloads as described in link:{rhoaidocshome}{default-format-url}/managing_openshift_ai/managing_distributed_workloads[Managing distributed workloads].
endif::[]
ifdef::upstream[]
* You have access to a data science cluster that is configured to run distributed workloads as described in link:{odhdocshome}/managing-openshift-ai/#managing_distributed_workloads[Managing distributed workloads].
endif::[]

ifndef::upstream[]
* You have created a data science project. 
For information about how to create a project, see link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects/using-data-science-projects_projects#creating-a-data-science-project_projects[Creating a data science project].
endif::[]
ifdef::upstream[]
* You have created a data science project. 
For information about how to create a project, see link:{odhdocshome}/working-on-data-science-projects/#creating-a-data-science-project_projects[Creating a data science project].
endif::[]

* You have Admin access for the data science project.
** If you created the project, you automatically have Admin access. 
** If you did not create the project, your cluster administrator must give you Admin access.

* You have access to a model.
* You have access to data that you can use to train the model.

.Procedure
. In a terminal window, if you are not already logged in to your OpenShift cluster, log in to the OpenShift CLI as shown in the following example:
+
[source,subs="+quotes"]
----
$ oc login __<openshift_cluster_url>__ -u __<username>__ -p __<password>__
----

. Configure a training job, as follows:
.. Create a YAML file named `config_trainingjob.yaml`.
.. Add the `ConfigMap` object definition as follows:
+
.Example training-job configuration
[source]
----
kind: ConfigMap
apiVersion: v1
metadata:
  name: training-config
  namespace: kfto
data:
  config.json: |
    {
      "model_name_or_path": "bigscience/bloom-560m",
      "training_data_path": "/data/input/twitter_complaints.json",
      "output_dir": "/data/output/tuning/bloom-twitter",
      "save_model_dir": "/mnt/output/model",
      "num_train_epochs": 10.0,
      "per_device_train_batch_size": 4,
      "per_device_eval_batch_size": 4,
      "gradient_accumulation_steps": 4,
      "save_strategy": "no",
      "learning_rate": 1e-05,
      "weight_decay": 0.0,
      "lr_scheduler_type": "cosine",
      "include_tokens_per_second": true,
      "response_template": "\n### Label:",
      "dataset_text_field": "output",
      "padding_free": ["huggingface"],
      "multipack": [16],
      "use_flash_attn": false
    }

----

.. Optional: To fine-tune with Low Rank Adaptation (LoRA), update the `config.json` section as follows:

... Set the `peft_method` parameter to `"lora"`.
... Add the `lora_r`, `lora_alpha`, `lora_dropout`, `bias`, and `target_modules` parameters.
+
.Example LoRA configuration
[source]
----
      ...
      "peft_method": "lora",
      "lora_r": 8,
      "lora_alpha": 8,
      "lora_dropout": 0.1,
      "bias": "none",
      "target_modules": ["all-linear"]
    }

----

.. Optional: To fine-tune with Quantized Low Rank Adaptation (QLoRA), update the `config.json` section as follows:

... Set the `use_flash_attn` parameter to `"true"`.
... Set the `peft_method` parameter to `"lora"`.
... Add the LoRA parameters: `lora_r`, `lora_alpha`, `lora_dropout`, `bias`, and `target_modules`.
... Add the QLoRA mandatory parameters: `auto_gptq`, `torch_dtype`, and `fp16`.
... If required, add the QLoRA optional parameters: `fused_lora` and `fast_kernels`.
+
.Example QLoRA configuration
[source]
----
      ...
      "use_flash_attn": true,
      "peft_method": "lora",
      "lora_r": 8,
      "lora_alpha": 8,
      "lora_dropout": 0.1,
      "bias": "none",
      "target_modules": ["all-linear"],
      "auto_gptq": ["triton_v2"],
      "torch_dtype": float16,
      "fp16": true,
      "fused_lora": ["auto_gptq", true],
      "fast_kernels": [true, true, true]
    }

----

.. Edit the metadata of the training-job configuration as shown in the following table.
+
.Training-job configuration metadata
[cols="16,84"]
|===
|Parameter | Value

|`name`
|Name of the training-job configuration

|`namespace`
|Name of your project
|===

.. Edit the parameters of the training-job configuration as shown in the following table.
+
.Training-job configuration parameters
[cols="30,70"]
|===
|Parameter | Value

|`model_name_or_path`
|Name of the pre-trained model or the path to the model in the training-job container; in this example, the model name is taken from the Hugging Face web page

|`training_data_path`
|Path to the training data that you set in the `training_data.yaml` ConfigMap

|`output_dir`
|Output directory for the model

|`save_model_dir`
|Directory where the tuned model is saved

|`num_train_epochs`
|Number of epochs for training; in this example, the training job is set to run 10 times

|`per_device_train_batch_size`
|Batch size, the number of data set examples to process together; in this example, the training job processes 4 examples at a time

|`per_device_eval_batch_size`
|Batch size, the number of data set examples to process together per GPU or TPU core or CPU; in this example, the training job processes 4 examples at a time

|`gradient_accumulation_steps`
|Number of gradient accumulation steps

|`save_strategy`
|How often model checkpoints can be saved; the default value is `"epoch"` (save model checkpoint every epoch), other possible values are `"steps"` (save model checkpoint for every training step) and `"no"` (do not save model checkpoints)

|`save_total_limit`
|Number of model checkpoints to save; omit if `save_strategy` is set to `"no"` (no model checkpoints saved)

|`learning_rate`
|Learning rate for the training

|`weight_decay`
|Weight decay to apply

|`lr_scheduler_type`
|Optional: Scheduler type to use; the default value is `"linear"`, other possible values are `"cosine"`, `"cosine_with_restarts"`, `"polynomial"`, `"constant"`, and `"constant_with_warmup"`

|`include_tokens_per_second`
|Optional: Whether or not to compute the number of tokens per second per device for training speed metrics

|`response_template`
|Template formatting for the response

|`dataset_text_field`
|Dataset field for training output, as set in the `training_data.yaml` config map

|`padding_free`
|Whether to use a technique to process multiple examples in a single batch without adding padding tokens that waste compute resources; if used, this parameter must be set to `["huggingface"]`

|`multipack`
|Whether to use a technique for multi-GPU training to balance the number of tokens processed in each device, to minimize waiting time; you can experiment with different values to find the optimum value for your training job

|`use_flash_attn`
|Whether to use flash attention

|`peft_method`
|Tuning method: for full fine-tuning, omit this parameter; for LoRA and QLoRA, set to `"lora"`; for prompt tuning, set to `"pt"`

|`lora_r`
|LoRA: Rank of the low-rank decomposition

|`lora_alpha`
|LoRA: Scale the low-rank matrices to control their influence on the model's adaptations

|`lora_dropout`
|LoRA: Dropout rate applied to the LoRA layers, a regularization technique to prevent overfitting

|`bias`
|LoRA: Whether to adapt bias terms in the model; setting the bias to `"none"` indicates that no bias terms will be adapted

|`target_modules`
|LoRA: Names of the modules to apply LoRA to; to include all linear layers, set to "all_linear"; optional parameter for some models

|`auto_gptq`
|QLoRA: Sets 4-bit GPTQ-LoRA with AutoGPTQ; when used, this parameter must be set to `["triton_v2"]` 

|`torch_dtype`
|QLoRA: Tensor datatype; when used, this parameter must be set to `float16`

|`fp16`
|QLoRA: Whether to use half-precision floating-point format; when used, this parameter must be set to `true` 

|`fused_lora`
|QLoRA: Whether to use fused LoRA for more efficient LoRA training; if used, this parameter must be set to `["auto_gptq", true]`

|`fast_kernels`
|QLoRA: Whether to use fast cross-entropy, rope, rms loss kernels; if used, this parameter must be set to `[true, true, true]`

|===

.. Save your changes in the `config_trainingjob.yaml` file.
.. Apply the configuration to create the `training-config` object:
+
[source]
----
$ oc apply -f config_trainingjob.yaml
----

. Create the training data.
+
[NOTE]
====
The training data in this simple example is for demonstration purposes only, and is not suitable for production use.
The usual method for providing training data is to use persistent volumes. 
====
.. Create a YAML file named `training_data.yaml`.
.. Add the following `ConfigMap` object definition:
+
[source]
----
kind: ConfigMap
apiVersion: v1
metadata:
  name: twitter-complaints
  namespace: kfto
data:
  twitter_complaints.json: |
    [
        {"Tweet text":"@HMRCcustomers No this is my first job","ID":0,"Label":2,"text_label":"no complaint","output":"### Text: @HMRCcustomers No this is my first job\n\n### Label: no complaint"},
        {"Tweet text":"@KristaMariePark Thank you for your interest! If you decide to cancel, you can call Customer Care at 1-800-NYTIMES.","ID":1,"Label":2,"text_label":"no complaint","output":"### Text: @KristaMariePark Thank you for your interest! If you decide to cancel, you can call Customer Care at 1-800-NYTIMES.\n\n### Label: no complaint"},
        {"Tweet text":"@EE On Rosneath Arial having good upload and download speeds but terrible latency 200ms. Why is this.","ID":3,"Label":1,"text_label":"complaint","output":"### Text: @EE On Rosneath Arial having good upload and download speeds but terrible latency 200ms. Why is this.\n\n### Label: complaint"},
        {"Tweet text":"Couples wallpaper, so cute. :) #BrothersAtHome","ID":4,"Label":2,"text_label":"no complaint","output":"### Text: Couples wallpaper, so cute. :) #BrothersAtHome\n\n### Label: no complaint"},
        {"Tweet text":"@mckelldogs This might just be me, but-- eyedrops? Artificial tears are so useful when you're sleep-deprived and sp… https:\/\/t.co\/WRtNsokblG","ID":5,"Label":2,"text_label":"no complaint","output":"### Text: @mckelldogs This might just be me, but-- eyedrops? Artificial tears are so useful when you're sleep-deprived and sp… https:\/\/t.co\/WRtNsokblG\n\n### Label: no complaint"},
        {"Tweet text":"@Yelp can we get the exact calculations for a business rating (for example if its 4 stars but actually 4.2) or do we use a 3rd party site?","ID":6,"Label":2,"text_label":"no complaint","output":"### Text: @Yelp can we get the exact calculations for a business rating (for example if its 4 stars but actually 4.2) or do we use a 3rd party site?\n\n### Label: no complaint"},
        {"Tweet text":"@nationalgridus I have no water and the bill is current and paid. Can you do something about this?","ID":7,"Label":1,"text_label":"complaint","output":"### Text: @nationalgridus I have no water and the bill is current and paid. Can you do something about this?\n\n### Label: complaint"},
        {"Tweet text":"@JenniferTilly Merry Christmas to as well. You get more stunning every year ��","ID":9,"Label":2,"text_label":"no complaint","output":"### Text: @JenniferTilly Merry Christmas to as well. You get more stunning every year ��\n\n### Label: no complaint"}
    ]

----
.. Replace the example namespace value `kfto` with the name of your project.
.. Replace the example training data with your training data.
.. Save your changes in the `training_data.yaml` file.
.. Apply the configuration to create the training data:
+
[source]
----
$ oc apply -f training_data.yaml
----

. Create a persistent volume claim (PVC), as follows:
.. Create a YAML file named `trainedmodelpvc.yaml`.
.. Add the following `PersistentVolumeClaim` object definition:
+
[source]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: trained-model
  namespace: kfto
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Gi

----
.. Replace the example namespace value `kfto` with the name of your project, and update the other parameters to suit your environment.
To calculate the `storage` value, multiply the model size by the number of epochs, and add a little extra as a buffer.
.. Save your changes in the `trainedmodelpvc.yaml` file.
.. Apply the configuration to create a Persistent Volume Claim (PVC) for the training job:
+
[source]
----
$ oc apply -f trainedmodelpvc.yaml
----





.Verification
ifdef::upstream,self-managed[]
. In the {openshift-platform} console, select your project from the *Project* list. 
endif::[]
ifdef::cloud-service[]
. In the OpenShift console, select your project from the *Project* list.
endif::[]
. Click *ConfigMaps* and verify that the `training-config` and `twitter-complaints` ConfigMaps are listed. 
. Click *Search*. From the *Resources* list, select *PersistentVolumeClaim* and verify that the `trained-model` PVC is listed.


////
[role='_additional-resources']
.Additional resources
<Do we want to link to additional resources?>


* link:https://url[link text]
////
