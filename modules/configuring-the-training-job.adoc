:_module-type: PROCEDURE

[id="configuring-the-training-job_{context}"]
= Configuring the training job

[role='_abstract']
Before you can use a training job to tune a model, you must configure the training job. 
The example training job in this section is based on the IBM and Hugging Face tuning example provided in link:https://github.com/foundation-model-stack/fms-hf-tuning/tree/main/examples/prompt_tuning_twitter_complaints[GitHub]. 


.Prerequisites
ifdef::upstream,self-managed[]
* You have logged in to {openshift-platform}.
endif::[]
ifdef::cloud-service[]
* You have logged in to OpenShift.
endif::[]

ifndef::upstream[]
* You have access to a data science cluster that is configured to run distributed workloads as described in link:{rhoaidocshome}{default-format-url}/working_with_distributed_workloads/configuring-distributed-workloads_distributed-workloads[Configuring distributed workloads].
endif::[]
ifdef::upstream[]
* You have access to a data science cluster that is configured to run distributed workloads as described in link:{odhdocshome}/working-with-distributed-workloads/#configuring-distributed-workloads_distributed-workloads[Configuring distributed workloads].
endif::[]

ifndef::upstream[]
* You have created a data science project. 
For information about how to create a project, see link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects/using-data-science-projects_projects#creating-a-data-science-project_projects[Creating a data science project].
endif::[]
ifdef::upstream[]
* You have created a data science project. 
For information about how to create a project, see link:{odhdocshome}/working-on-data-science-projects/#creating-a-data-science-project_projects[Creating a data science project].
endif::[]

* You have Admin access for the data science project.
** If you created the project, you automatically have Admin access. 
** If you did not create the project, your cluster administrator must give you Admin access.

* You have access to a model.
* You have access to data that you can use to train the model.

.Procedure
. In a terminal window, if you are not already logged in to your OpenShift cluster, log in to the OpenShift CLI as shown in the following example:
+
[source,subs="+quotes"]
----
$ oc login __<openshift_cluster_url>__ -u __<username>__ -p __<password>__
----

. Configure a training job, as follows:
.. Create a YAML file named `config_trainingjob.yaml`.
.. Add the `ConfigMap` object definition as follows:
+
[source]
----
kind: ConfigMap
apiVersion: v1
metadata:
  name: training-config
  namespace: kfto
data:
  config.json: |
    {
      "model_name_or_path": "bigscience/bloom-560m",
      "training_data_path": "/data/input/twitter_complaints.json",
      "output_dir": "/data/output/tuning/bloom-twitter",
      "num_train_epochs": 10.0,
      "per_device_train_batch_size": 4,
      "gradient_accumulation_steps": 4,
      "learning_rate": 1e-05,
      "response_template": "\n### Label:",
      "dataset_text_field": "output",
      "use_flash_attn": false
    }

----

.. Optional: To fine-tune with Low Rank Adaptation (LoRA), update the `config.json` section to add the `lora_r`, `lora_alpha`, `lora_dropout`, `bias`, and `target_modules` parameters, as shown in the following example:
+
[source]
----
      ...
      "use_flash_attn": false,
      "lora_r": 8,
      "lora_alpha": 8,
      "lora_dropout": 0.1,
      "bias": "none",
      "target_modules": ["q_proj", "v_proj"]
    }

----

.. Edit the metadata of the training-job configuration as shown in the following table.
+
.Training-job configuration metadata
[cols="16,84"]
|===
|Parameter | Value

|`name`
|Name of the training job

|`namespace`
|Name of your project
|===

.. Edit the parameters of the training-job configuration as shown in the following table.
+
.Training-job configuration parameters
[cols="30,70"]
|===
|Parameter | Value

|`model_name_or_path`
|Name of the pre-trained model or the path to the model in the training-job container; in this example, the model name is taken from the Hugging Face web page

|`training_data_path`
|Path to the training data that you set in the `training_data.yaml` ConfigMap

|`output_dir`
|Output directory for the model

|`num_train_epochs`
|Number of epochs for training; in this example, the training job is set to run 10 times

|`per_device_train_batch_size`
|Batch size, the number of data set examples to process together; in this example, the training job processes 4 examples at a time

|`gradient_accumulation_steps`
|Number of gradient accumulation steps

|`learning_rate`
|Learning rate for the training

|`response_template`
|Template formatting for the response

|`dataset_text_field`
|Dataset field for training output, as set in the `training_data.yaml` config map

|`use_flash_attn`
|Whether to use flash attention

|`lora_r`
|LoRA: Rank of the low-rank decomposition

|`lora_alpha`
|LoRA: Scale the low-rank matrices to control their influence on the model's adaptations

|`lora_dropout`
|LoRA: Dropout rate applied to the LoRA layers, a regularization technique to prevent overfitting

|`bias`
|LoRA: Whether to adapt bias terms in the model; setting the bias to `"none"` indicates that no bias terms will be adapted

|`target_modules`
|LoRA: Names of the modules to apply LoRA to; includes `"q_proj"` and `"v_proj"` by default

|===

.. Save your changes in the `config_trainingjob.yaml` file.
.. Apply the configuration to create the `training-config` object:
+
[source]
----
$ oc apply -f config_trainingjob.yaml
----

. Create the training data.
+
[NOTE]
====
The training data in this simple example is for demonstration purposes only, and is not suitable for production use.
The usual method for providing training data is to use persistent volumes. 
====
.. Create a YAML file named `training_data.yaml`.
.. Add the following `ConfigMap` object definition:
+
[source]
----
kind: ConfigMap
apiVersion: v1
metadata:
  name: twitter-complaints
  namespace: kfto
data:
  twitter_complaints.json: |
    [
        {"Tweet text":"@HMRCcustomers No this is my first job","ID":0,"Label":2,"text_label":"no complaint","output":"### Text: @HMRCcustomers No this is my first job\n\n### Label: no complaint"},
        {"Tweet text":"@KristaMariePark Thank you for your interest! If you decide to cancel, you can call Customer Care at 1-800-NYTIMES.","ID":1,"Label":2,"text_label":"no complaint","output":"### Text: @KristaMariePark Thank you for your interest! If you decide to cancel, you can call Customer Care at 1-800-NYTIMES.\n\n### Label: no complaint"},
        {"Tweet text":"@EE On Rosneath Arial having good upload and download speeds but terrible latency 200ms. Why is this.","ID":3,"Label":1,"text_label":"complaint","output":"### Text: @EE On Rosneath Arial having good upload and download speeds but terrible latency 200ms. Why is this.\n\n### Label: complaint"},
        {"Tweet text":"Couples wallpaper, so cute. :) #BrothersAtHome","ID":4,"Label":2,"text_label":"no complaint","output":"### Text: Couples wallpaper, so cute. :) #BrothersAtHome\n\n### Label: no complaint"},
        {"Tweet text":"@mckelldogs This might just be me, but-- eyedrops? Artificial tears are so useful when you're sleep-deprived and sp… https:\/\/t.co\/WRtNsokblG","ID":5,"Label":2,"text_label":"no complaint","output":"### Text: @mckelldogs This might just be me, but-- eyedrops? Artificial tears are so useful when you're sleep-deprived and sp… https:\/\/t.co\/WRtNsokblG\n\n### Label: no complaint"},
        {"Tweet text":"@Yelp can we get the exact calculations for a business rating (for example if its 4 stars but actually 4.2) or do we use a 3rd party site?","ID":6,"Label":2,"text_label":"no complaint","output":"### Text: @Yelp can we get the exact calculations for a business rating (for example if its 4 stars but actually 4.2) or do we use a 3rd party site?\n\n### Label: no complaint"},
        {"Tweet text":"@nationalgridus I have no water and the bill is current and paid. Can you do something about this?","ID":7,"Label":1,"text_label":"complaint","output":"### Text: @nationalgridus I have no water and the bill is current and paid. Can you do something about this?\n\n### Label: complaint"},
        {"Tweet text":"@JenniferTilly Merry Christmas to as well. You get more stunning every year ��","ID":9,"Label":2,"text_label":"no complaint","output":"### Text: @JenniferTilly Merry Christmas to as well. You get more stunning every year ��\n\n### Label: no complaint"}
    ]

----
.. Replace the example namespace value `kfto` with the name of your project.
.. Replace the example training data with your training data.
.. Save your changes in the `training_data.yaml` file.
.. Apply the configuration to create the training data:
+
[source]
----
$ oc apply -f training_data.yaml
----

. Create a persistent volume claim (PVC), as follows:
.. Create a YAML file named `trainedmodelpvc.yaml`.
.. Add the following `PersistentVolumeClaim` object definition:
+
[source]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: trained-model
  namespace: kfto
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Gi

----
.. Replace the example namespace value `kfto` with the name of your project, and update the other parameters to suit your environment.
To calculate the `storage` value, multiply the model size by the number of epochs, and add a little extra as a buffer.
.. Save your changes in the `trainedmodelpvc.yaml` file.
.. Apply the configuration to create a Persistent Volume Claim (PVC) for the training job:
+
[source]
----
$ oc apply -f trainedmodelpvc.yaml
----





.Verification
ifdef::upstream,self-managed[]
. In the {openshift-platform} console, select your project from the *Project* list. 
endif::[]
ifdef::cloud-service[]
. In the OpenShift console, select your project from the *Project* list.
endif::[]
. Click *ConfigMaps* and verify that the `training-config` and `twitter-complaints` ConfigMaps are listed. 
. Click *Search*. From the *Resources* list, select *PersistentVolumeClaim* and verify that the `trained-model` PVC is listed.


////
[role='_additional-resources']
.Additional resources
<Do we want to link to additional resources?>


* link:https://url[link text]
////
