:_module-type: PROCEDURE

[id="configuring-quota-management-for-distributed-workloads_{context}"]
= Configuring quota management for distributed workloads

[role='_abstract']
Configure quotas for distributed workloads on a cluster, so that you can share resources between several data science projects.

.Prerequisites
ifdef::cloud-service[]
* You have cluster administrator privileges for your OpenShift cluster.
endif::[]
ifdef::self-managed[]
* You have cluster administrator privileges for your {openshift-platform} cluster.
endif::[]

ifdef::self-managed[]
* You have downloaded and installed the OpenShift command-line interface (CLI). See link:https://docs.openshift.com/container-platform/{ocp-latest-version}/cli_reference/openshift_cli/getting-started-cli.html#installing-openshift-cli[Installing the OpenShift CLI].
endif::[]
ifdef::cloud-service[]
* You have downloaded and installed the OpenShift command-line interface (CLI). See link:https://docs.openshift.com/dedicated/cli_reference/openshift_cli/getting-started-cli.html#installing-openshift-cli[Installing the OpenShift CLI] (Red Hat OpenShift Dedicated) or link:https://docs.openshift.com/rosa/cli_reference/openshift_cli/getting-started-cli.html#installing-openshift-cli[Installing the OpenShift CLI] (Red Hat OpenShift Service on AWS).
endif::[]

ifndef::upstream[]
* You have enabled the required distributed workloads components as described in link:{rhoaidocshome}{default-format-url}/working_with_distributed_workloads/configuring-distributed-workloads_distributed-workloads#configuring-the-distributed-workloads-components_distributed-workloads[Configuring the distributed workloads components].
endif::[]
ifdef::upstream[]
* You have enabled the required distributed workloads components as described in link:{odhdocshome}/working-with-distributed-workloads/#configuring-the-distributed-workloads-components_distributed-workloads[Configuring the distributed workloads components].
endif::[]

ifndef::upstream[]
* You have created a data science project that contains a workbench, and the workbench is running a default notebook image that contains the CodeFlare SDK, for example, the *Standard Data Science* notebook. For information about how to create a project, see link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects/working-on-data-science-projects_nb-server#creating-a-data-science-project_nb-server[Creating a data science project]. 
endif::[]
ifdef::upstream[]
* You have created a data science project that contains a workbench, and the workbench is running a default notebook image that contains the CodeFlare SDK, for example, the *Standard Data Science* notebook. For information about how to create a project, see link:{odhdocshome}/working-on-data-science-projects/#_using_data_science_projects[Creating a data science project]. 
endif::[]

* You have sufficient resources. In addition to the base {productname-short} resources, you need 1.6 vCPU and 2 GiB memory to deploy the distributed workloads infrastructure.

ifndef::upstream[]
* The resources are physically available in the cluster.
+
[NOTE]
====
In {productname-short} {vernum}, {org-name} supports only a single cluster queue per cluster (that is, homogenous clusters), and only empty resource flavors.
For more information about Kueue resources, see link:{rhoaidocshome}{default-format-url}/working_with_distributed_workloads/overview-of-distributed-workloads_distributed-workloads#overview-of-kueue-resources_distributed-workloads[Overview of Kueue resources].
====
endif::[]
ifdef::upstream[]
* The resources are physically available in the cluster.
+
[NOTE]
====
For more information about Kueue resources, see link:{odhdocshome}/working-with-distributed-workloads/#overview-of-kueue-resources_distributed-workloads[Overview of Kueue resources].
====
endif::[]

ifndef::upstream[]
* If you want to use graphics processing units (GPUs), you have enabled GPU support in {productname-short}.
See link:{rhoaidocshome}{default-format-url}/managing_resources/managing-cluster-resources_cluster-mgmt#enabling-gpu-support_cluster-mgmt[Enabling GPU support in {productname-short}].
+
[NOTE]
====
In {productname-short} {vernum}, {org-name} supports only NVIDIA GPU accelerators for distributed workloads.
====
endif::[]
ifdef::upstream[]
* If you want to use graphics processing units (GPUs), you have enabled GPU support.
This process includes installing the Node Feature Discovery Operator and the NVIDIA GPU Operator.
For more information, see https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/index.html[NVIDIA GPU Operator on {org-name} OpenShift Container Platform^] in the NVIDIA documentation.
endif::[]


.Procedure

. In a terminal window, if you are not already logged in to your OpenShift cluster as a cluster administrator, log in to the OpenShift CLI as shown in the following example:
+
[source,subs="+quotes"]
----
$ oc login __<openshift_cluster_url>__ -u __<admin_username>__ -p __<password>__
----

. Create an empty Kueue resource flavor, as follows:
.. Create a file called `default_flavor.yaml` and populate it with the following content:
+
.Empty Kueue resource flavor
[source,bash]
----
apiVersion: kueue.x-k8s.io/v1beta1
kind: ResourceFlavor
metadata:
  name: default-flavor
----
.. Apply the configuration to create the `default-flavor` object:
+
[source,bash]
----
$ oc apply -f default_flavor.yaml
----

. Create a cluster queue to manage the empty Kueue resource flavor, as follows:
.. Create a file called `cluster_queue.yaml` and populate it with the following content:
+
.Example cluster queue
[source,bash]
----
apiVersion: kueue.x-k8s.io/v1beta1
kind: ClusterQueue
metadata:
  name: "cluster-queue"
spec:
  namespaceSelector: {}  # match all.
  resourceGroups:
  - coveredResources: ["cpu", "memory", "nvidia.com/gpu"]
    flavors:
    - name: "default-flavor"
      resources:
      - name: "cpu"
        nominalQuota: 9
      - name: "memory"
        nominalQuota: 36Gi
      - name: "nvidia.com/gpu"
        nominalQuota: 5
----
+
.. Replace the example quota values (9 CPUs, 36 GiB memory, and 5 NVIDIA GPUs) with the appropriate values for your cluster queue.
The cluster queue will start a distributed workload only if the total required resources are within these quota limits.
+
You must specify a quota for each resource that the user can request, even if the requested value is 0, by updating the `spec.resourceGroups` section as follows:

* Include the resource name in the `coveredResources` list.
* Specify the resource `name` and `nominalQuota` in the `flavors.resources` section, even if the `nominalQuota` value is 0.

.. Apply the configuration to create the `cluster-queue` object:
+
[source,bash]
----
$ oc apply -f cluster_queue.yaml
----

. Create a local queue that points to your cluster queue, as follows:
.. Create a file called `local_queue.yaml` and populate it with the following content:
+
.Example local queue
[source,bash]
----
apiVersion: kueue.x-k8s.io/v1beta1
kind: LocalQueue
metadata:
  namespace: test
  name: local-queue-test
  annotations:
    kueue.x-k8s.io/default-queue: 'true'
spec:
  clusterQueue: cluster-queue
----
The `kueue.x-k8s.io/default-queue: 'true'` annotation defines this queue as the default queue.
Distributed workloads are submitted to this queue if no `local_queue` value is specified in the `ClusterConfiguration` section of the data science pipeline or Jupyter notebook or Microsoft Visual Studio Code file.
.. Update the `namespace` value to specify the same namespace as in the `ClusterConfiguration` section that creates the Ray cluster.
.. Optional: Update the `name` value accordingly.
.. Apply the configuration to create the local-queue object:
+
[source,bash]
----
$ oc apply -f local_queue.yaml
----
+
The cluster queue allocates the resources to run distributed workloads in the local queue.


.Verification
Check the status of the local queue in a project, as follows:

[source,subs="+quotes"]
----
$ oc get -n __<project-name>__ localqueues
----


[role='_additional-resources']
.Additional resources
* link:https://kueue.sigs.k8s.io/docs/concepts/[Kueue documentation]
