:_module-type: PROCEDURE

[id="configuring-quota-management-for-distributed-workloads_{context}"]
= Configuring quota management for distributed workloads

[role='_abstract']
Configure quotas for distributed workloads on a cluster, so that you can share resources between several data science projects.

.Prerequisites
* You have logged in to {openshift-platform} with the `cluster-admin` role.

ifdef::self-managed[]
* You have downloaded and installed the OpenShift command-line interface (CLI). See link:https://docs.redhat.com/en/documentation/openshift_container_platform/{ocp-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli[Installing the OpenShift CLI^].
endif::[]
ifdef::cloud-service[]
* You have downloaded and installed the OpenShift command-line interface (CLI). See link:https://docs.redhat.com/en/documentation/openshift_dedicated/{osd-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli[Installing the OpenShift CLI (OpenShift Dedicated)^] or link:https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws/{rosa-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli[Installing the OpenShift CLI (Red Hat OpenShift Service on AWS)^].
endif::[]

ifdef::upstream[]
* You have installed the required distributed workloads components as described in link:{odhdocshome}/installing-open-data-hub/#installing-the-distributed-workloads-components_install[Installing the distributed workloads components].
endif::[]

ifdef::self-managed[]
* You have installed the required distributed workloads components as described in link:{rhoaidocshome}{default-format-url}/installing_and_uninstalling_{url-productname-short}/installing-the-distributed-workloads-components_install[Installing the distributed workloads components] (for disconnected environments, see link:{rhoaidocshome}{default-format-url}/installing_and_uninstalling_{url-productname-short}_in_a_disconnected_environment/installing-the-distributed-workloads-components_install[Installing the distributed workloads components]).
endif::[]

ifdef::cloud-service[]
* You have installed the required distributed workloads components as described in link:{rhoaidocshome}{default-format-url}/installing_and_uninstalling_{url-productname-short}/installing-the-distributed-workloads-components_install[Installing the distributed workloads components].
endif::[]


ifndef::upstream[]
* You have created a data science project that contains a workbench, and the workbench is running a default notebook image that contains the CodeFlare SDK, for example, the *Standard Data Science* notebook. For information about how to create a project, see link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects/using-data-science-projects_projects#creating-a-data-science-project_projects[Creating a data science project]. 
endif::[]
ifdef::upstream[]
* You have created a data science project that contains a workbench, and the workbench is running a default notebook image that contains the CodeFlare SDK, for example, the *Standard Data Science* notebook. For information about how to create a project, see link:{odhdocshome}/working-on-data-science-projects/#creating-a-data-science-project_projects[Creating a data science project]. 
endif::[]

* You have sufficient resources. In addition to the base {productname-short} resources, you need 1.6 vCPU and 2 GiB memory to deploy the distributed workloads infrastructure.

ifndef::upstream[]
* The resources are physically available in the cluster.
For more information about Kueue resources, see link:{rhoaidocshome}{default-format-url}/managing_openshift_ai/managing-distributed-workloads_managing-rhoai#overview-of-kueue-resources_managing-rhoai[Overview of Kueue resources].
endif::[]

ifdef::upstream[]
* The resources are physically available in the cluster.
For more information about Kueue resources, see link:{odhdocshome}/managing-odh/#overview-of-kueue-resources_managing-odh[Overview of Kueue resources].
endif::[]

ifndef::upstream[]
* If you want to use graphics processing units (GPUs), you have enabled GPU support in {productname-short}.
If you use NVIDIA GPUs, see link:{rhoaidocshome}{default-format-url}/managing_openshift_ai/enabling_accelerators#enabling-nvidia-gpus_managing-rhoai[Enabling NVIDIA GPUs^]. 
If you use AMD GPUs, see link:{rhoaidocshome}{default-format-url}/managing_openshift_ai/enabling_accelerators#amd-gpu-integration_managing-rhoai[AMD GPU integration^].
+
ifdef::self-managed[]
[NOTE]
====
In {productname-short} {vernum}, {org-name} supports only NVIDIA GPU accelerators and AMD GPU accelerators for distributed workloads.
====
endif::[]
ifdef::cloud-service[]
[NOTE]
====
In {productname-short}, {org-name} supports only NVIDIA GPU accelerators and AMD GPU accelerators for distributed workloads.
====
endif::[]
endif::[]
ifdef::upstream[]
* If you want to use graphics processing units (GPUs), you have enabled GPU support.
This process includes installing the Node Feature Discovery Operator and the relevant GPU Operator.
For more information, see link:https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/index.html[NVIDIA GPU Operator on {org-name} OpenShift Container Platform^] for NVIDIA GPUs and link:https://instinct.docs.amd.com/projects/gpu-operator/en/latest/installation/openshift-olm.html[AMD GPU Operator on {org-name} OpenShift Container Platform^] for AMD GPUs.
endif::[]

.Procedure

. In a terminal window, if you are not already logged in to your OpenShift cluster as a cluster administrator, log in to the OpenShift CLI as shown in the following example:
+
[source,subs="+quotes"]
----
$ oc login __<openshift_cluster_url>__ -u __<admin_username>__ -p __<password>__
----

. Create an empty Kueue resource flavor, as follows:
.. Create a file called `default_flavor.yaml` and populate it with the following content:
+
.Empty Kueue resource flavor
[source,bash]
----
apiVersion: kueue.x-k8s.io/v1beta1
kind: ResourceFlavor
metadata:
  name: default-flavor
----
.. Apply the configuration to create the `default-flavor` object:
+
[source,bash]
----
$ oc apply -f default_flavor.yaml
----

. Create a cluster queue to manage the empty Kueue resource flavor, as follows:
.. Create a file called `cluster_queue.yaml` and populate it with the following content:
+
.Example cluster queue
[source,bash]
----
apiVersion: kueue.x-k8s.io/v1beta1
kind: ClusterQueue
metadata:
  name: "cluster-queue"
spec:
  namespaceSelector: {}  # match all.
  resourceGroups:
  - coveredResources: ["cpu", "memory", "nvidia.com/gpu"] 
    flavors:
    - name: "default-flavor"
      resources:
      - name: "cpu"
        nominalQuota: 9
      - name: "memory"
        nominalQuota: 36Gi
      - name: "nvidia.com/gpu" 
        nominalQuota: 5
----
+
.. Replace the example quota values (9 CPUs, 36 GiB memory, and 5 NVIDIA GPUs) with the appropriate values for your cluster queue.
If you use AMD GPUs, replace `nvidia.com/gpu` with `amd.com/gpu` in the example code.
The cluster queue will start a distributed workload only if the total required resources are within these quota limits.
+
You must specify a quota for each resource that the user can request, even if the requested value is 0, by updating the `spec.resourceGroups` section as follows:

* Include the resource name in the `coveredResources` list.
* Specify the resource `name` and `nominalQuota` in the `flavors.resources` section, even if the `nominalQuota` value is 0.

.. Apply the configuration to create the `cluster-queue` object:
+
[source,bash]
----
$ oc apply -f cluster_queue.yaml
----

. Create a local queue that points to your cluster queue, as follows:
.. Create a file called `local_queue.yaml` and populate it with the following content:
+
.Example local queue
[source,bash]
----
apiVersion: kueue.x-k8s.io/v1beta1
kind: LocalQueue
metadata:
  namespace: test
  name: local-queue-test
  annotations:
    kueue.x-k8s.io/default-queue: 'true'
spec:
  clusterQueue: cluster-queue
----
The `kueue.x-k8s.io/default-queue: 'true'` annotation defines this queue as the default queue.
Distributed workloads are submitted to this queue if no `local_queue` value is specified in the `ClusterConfiguration` section of the data science pipeline or Jupyter notebook or Microsoft Visual Studio Code file.
.. Update the `namespace` value to specify the same namespace as in the `ClusterConfiguration` section that creates the Ray cluster.
.. Optional: Update the `name` value accordingly.
.. Apply the configuration to create the local-queue object:
+
[source,bash]
----
$ oc apply -f local_queue.yaml
----
+
The cluster queue allocates the resources to run distributed workloads in the local queue.


.Verification
Check the status of the local queue in a project, as follows:

[source,subs="+quotes"]
----
$ oc get -n __<project-name>__ localqueues
----


[role='_additional-resources']
.Additional resources
* link:https://kueue.sigs.k8s.io/docs/concepts/[Kueue documentation]