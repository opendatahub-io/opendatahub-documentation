:_module-type: PROCEDURE

[id='configuring-a-pipeline-server_{context}']
= Configuring a pipeline server

[role='_abstract']
Before you can successfully create a pipeline in {productname-short}, you must configure a pipeline server. This includes configuring where your pipeline artifacts and data are stored.

[NOTE]
====
After the pipeline server is created, the `/metadata` and `/artifacts` folders are automatically created in the default `root` folder. Therefore, you are not required to specify any storage directories when configuring a data connection for your pipeline server.
====

.Prerequisites
* You have installed the OpenShift Pipelines operator.
* You have logged in to {productname-long}.
ifndef::upstream[]
* If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {oai-user-group} or {oai-admin-group} ) in OpenShift.
endif::[]
ifdef::upstream[]
* If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group} or {odh-admin-group}) in OpenShift.
endif::[]
* You have created a data science project that you can add a pipeline server to.
* You have an existing S3-compatible object storage bucket and you have configured write access to your S3 bucket on your storage account.

.Procedure
. From the {productname-short} dashboard, click *Data Science Projects*.
+
The *Data science projects* page opens.
. Click the name of the project that you want to configure a pipeline server for.
+
A project details page opens.
. In the *Pipelines* section, click *Configure a pipeline server*.
+
The *Configure pipeline server* dialog appears.
. In the *Object storage connection* section, provide values for the mandatory fields:
.. In the *Access key* field, enter the access key ID for the S3-compatible object storage provider.
.. In the *Secret key* field, enter the secret access key for the S3-compatible object storage account that you specified.
.. In the *Endpoint* field, enter the endpoint of your S3-compatible object storage bucket.
.. In the *Bucket* field, enter the name of your S3-compatible object storage bucket.
+
[IMPORTANT]
====
If you specify incorrect data connection settings, you cannot update these settings on the same pipeline server. Therefore, you must delete the pipeline server and configure another one.
====

. In the *Database* section, click *Show advanced database options* to specify the database to store your pipeline data and select one of the following sets of actions:
* Select *Use default database stored on your cluster* to deploy a MariaDB database in your project.
* Select *Connect to external MySQL database* to add a new connection to an external database that your pipeline server can access.
... In the *Host* field, enter the database's host name.
... In the *Port* field, enter the database's port.
... In the *Username* field, enter the default user name that is connected to the database.
... In the *Password* field, enter the password for the default user account.
... In the *Database* field, enter the database name.
+
[NOTE]
====
The external database connection is secured by default. To specify an external database connection that is not secured with TLS, set the value of `database.customExtraParams.tls` to `false` in the `DataSciencePipelinesApplication` custom resource as shown here:

[source]
----
spec:
  database:
    customExtraParams: |
      {"tls": "false"}    
    externalDB:
       host: mysql:3306
       ... 
---- 
====

. Click *Configure*.

.Verification
* The pipeline server that you configured is displayed in the *Pipelines* section on the project details page.
* The *Import pipeline* button is available in the *Pipelines* section on the project details page.

//[role="_additional-resources"]
//.Additional resources
