:_module-type: PROCEDURE

[id='configuring-a-pipeline-server_{context}']
= Configuring a pipeline server

[role='_abstract']
Before you can successfully create a pipeline in {productname-short}, you must configure a pipeline server. This task includes configuring where your pipeline artifacts and data are stored.

[NOTE]
====
You are not required to specify any storage directories when configuring a connection for your pipeline server. When you import a pipeline, the `/pipelines` folder is created in the `root` folder of the bucket, containing a YAML file for the pipeline. If you upload a new version of the same pipeline, a new YAML file with a different ID is added to the `/pipelines` folder.  

When you run a pipeline, the artifacts are stored in the `/pipeline-name` folder in the `root` folder of the bucket.
====

ifdef::upstream[]
[IMPORTANT]
====
If you use an external MySQL database and upgrade to {productname-short} 2.10.0 or later, the database is migrated to data science pipelines 2.0 format, making it incompatible with earlier versions of {productname-short}.
====
endif::[]
ifndef::upstream[]
ifdef::self-managed[]
[IMPORTANT]
====
If you use an external MySQL database and upgrade to {productname-short} 2.9 or later, the database is migrated to data science pipelines 2.0 format, making it incompatible with earlier versions of {productname-short}.
====
endif::[]
ifdef::cloud-service[]
[IMPORTANT]
====
If you use an external MySQL database and upgrade to {productname-short} with data science pipelines 2.0, the database is migrated to data science pipelines 2.0 format, making it incompatible with earlier versions of {productname-short}.
====
endif::[]
endif::[]

.Prerequisites
* You have logged in to {productname-long}.
ifndef::upstream[]
* If you are using {productname-short} groups, you are part of the user group or admin group (for example, {oai-user-group} or {oai-admin-group} ) in OpenShift.
endif::[]
ifdef::upstream[]
* If you are using {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group} or {odh-admin-group}) in OpenShift.
endif::[]
* You have created a data science project that you can add a pipeline server to.
* You have an existing S3-compatible object storage bucket and you have configured write access to your S3 bucket on your storage account.
* If you are configuring a pipeline server for production pipeline workloads, you have an existing external MySQL or MariaDB database.
* If you are configuring a pipeline server with an external MySQL database, your database must use at least MySQL version 5.x. However, {org-name} recommends that you use MySQL version 8.x. 
* If you are configuring a pipeline server with a MariaDB database, your database must use MariaDB version 10.3 or later. However, {org-name} recommends that you use at least MariaDB version 10.5.

.Procedure
. From the {productname-short} dashboard, click *Data Science Projects*.
+
The *Data Science Projects* page opens.
. Click the name of the project that you want to configure a pipeline server for.
+
A project details page opens.
. Click the *Pipelines* tab.
. Click *Configure pipeline server*.
+
The *Configure pipeline server* dialog appears.
. In the *Object storage connection* section, provide values for the mandatory fields:
.. In the *Access key* field, enter the access key ID for the S3-compatible object storage provider.
.. In the *Secret key* field, enter the secret access key for the S3-compatible object storage account that you specified.
.. In the *Endpoint* field, enter the endpoint of your S3-compatible object storage bucket.
.. In the *Region* field, enter the default region of your S3-compatible object storage account.
.. In the *Bucket* field, enter the name of your S3-compatible object storage bucket.
+
[IMPORTANT]
====
If you specify incorrect connection settings, you cannot update these settings on the same pipeline server. Therefore, you must delete the pipeline server and configure another one.

If you want to use an existing artifact that was not generated by a task in a pipeline, you can use the link:https://kubeflow-pipelines.readthedocs.io/en/latest/source/dsl.html#kfp.dsl.importer[kfp.dsl.importer component] to import the artifact from its URI. You can only import these artifacts to the S3-compatible object storage bucket that you define in the *Bucket* field in your pipeline server configuration. For more information about the `kfp.dsl.importer` component, see link:https://www.kubeflow.org/docs/components/pipelines/user-guides/components/importer-component/[Special Case: Importer Components].
====

. In the *Database* section, click *Show advanced database options* to specify the database to store your pipeline data and select one of the following sets of actions:
* Select *Use default database stored on your cluster* to deploy a MariaDB database in your project.
+
[IMPORTANT]
====
The *Use default database stored on your cluster* option is intended for development and testing purposes only. For production pipeline workloads, select the *Connect to external MySQL database* option to use an external MySQL or MariaDB database.
====
* Select *Connect to external MySQL database* to add a new connection to an external MySQL or MariaDB database that your pipeline server can access.
... In the *Host* field, enter the database's host name.
... In the *Port* field, enter the database's port.
... In the *Username* field, enter the default user name that is connected to the database.
... In the *Password* field, enter the password for the default user account.
... In the *Database* field, enter the database name.
. Click *Configure pipeline server*.

.Verification
On the *Pipelines* tab for the project:

* The *Import pipeline* button is available.
* When you click the action menu (*&#8942;*) and then click *View pipeline server configuration*, the pipeline server details are displayed.


//[role="_additional-resources"]
//.Additional resources
