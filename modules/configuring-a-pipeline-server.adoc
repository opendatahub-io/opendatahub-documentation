:_module-type: PROCEDURE

[id='configuring-a-pipeline-server_{context}']
= Configuring a pipeline server

[role='_abstract']
Before you can successfully create a pipeline in {productname-short}, you must configure a pipeline server. This task includes configuring where your pipeline artifacts and data are stored.

[NOTE]
====
You are not required to specify any storage directories when configuring a connection for your pipeline server. When you import a pipeline, the `/pipelines` folder is created in the `root` folder of the bucket, containing a YAML file for the pipeline. If you upload a new version of the same pipeline, a new YAML file with a different ID is added to the `/pipelines` folder.  

When you run a pipeline, the artifacts are stored in the `/pipeline-name` folder in the `root` folder of the bucket.
====

ifdef::upstream[]
[IMPORTANT]
====
If you use an external MySQL database and upgrade to {productname-short} 2.10.0 or later, the database is migrated to data science pipelines 2.0 format, making it incompatible with earlier versions of {productname-short}.
====
endif::[]
ifndef::upstream[]
ifdef::self-managed[]
[IMPORTANT]
====
If you use an external MySQL database and upgrade to {productname-short} 2.9 or later, the database is migrated to data science pipelines 2.0 format, making it incompatible with earlier versions of {productname-short}.
====
endif::[]
ifdef::cloud-service[]
[IMPORTANT]
====
If you use an external MySQL database and upgrade to {productname-short} with data science pipelines 2.0, the database is migrated to data science pipelines 2.0 format, making it incompatible with earlier versions of {productname-short}.
====
endif::[]
endif::[]

.Prerequisites
* You have logged in to {productname-long}.
* You have created a data science project that you can add a pipeline server to.
* You have an existing S3-compatible object storage bucket and you have configured write access to your S3 bucket on your storage account.
* If you are configuring a pipeline server for production pipeline workloads, you have an existing external MySQL or MariaDB database.
* If you are configuring a pipeline server with an external MySQL database, your database must use at least MySQL version 5.x. However, {org-name} recommends that you use MySQL version 8.x. 
+
[NOTE]
====
The `mysql_native_password` authentication plugin is required for the ML Metadata component to successfully connect to your database. `mysql_native_password` is disabled by default in MySQL 8.4 and later. If your database uses MySQL 8.4 or later, you must update your MySQL deployment to enable the `mysql_native_password` plugin. 

For more information about enabling the `mysql_native_password` plugin, see link:https://dev.mysql.com/doc/refman/8.4/en/native-pluggable-authentication.html[Native Pluggable Authentication] in the MySQL documentation.
====
* If you are configuring a pipeline server with a MariaDB database, your database must use MariaDB version 10.3 or later. However, {org-name} recommends that you use at least MariaDB version 10.5.

.Procedure
. From the {productname-short} dashboard, click *Data science projects*.
+
The *Data science projects* page opens.
. Click the name of the project that you want to configure a pipeline server for.
+
A project details page opens.
. Click the *Pipelines* tab.
. Click *Configure pipeline server*.
+
The *Configure pipeline server* dialog appears.
. In the *Object storage connection* section, provide values for the mandatory fields:
.. In the *Access key* field, enter the access key ID for the S3-compatible object storage provider.
.. In the *Secret key* field, enter the secret access key for the S3-compatible object storage account that you specified.
.. In the *Endpoint* field, enter the endpoint of your S3-compatible object storage bucket.
.. In the *Region* field, enter the default region of your S3-compatible object storage account.
.. In the *Bucket* field, enter the name of your S3-compatible object storage bucket.
+
[IMPORTANT]
====
If you specify incorrect connection settings, you cannot update these settings on the same pipeline server. Therefore, you must delete the pipeline server and configure another one.

If you want to use an existing artifact that was not generated by a task in a pipeline, you can use the link:https://kubeflow-pipelines.readthedocs.io/en/latest/source/dsl.html#kfp.dsl.importer[kfp.dsl.importer component] to import the artifact from its URI. You can only import these artifacts to the S3-compatible object storage bucket that you define in the *Bucket* field in your pipeline server configuration. For more information about the `kfp.dsl.importer` component, see link:https://www.kubeflow.org/docs/components/pipelines/user-guides/components/importer-component/[Special Case: Importer Components].
====

. Click *Advanced settings* to display the *Database*, *Pipeline definition storage*, and *Pipeline caching* sections. 
. In the *Database* section, choose one of the following options to specify where to store your pipeline metadata and run information:
* Select *Default database on the cluster* to deploy a MariaDB database in your project.
+
[IMPORTANT]
====
The *Default database on the cluster* option is intended for development and testing purposes only. For production pipeline workloads, select the *External MySQL database* option to use an external MySQL or MariaDB database.
====
* Select *External MySQL database* to add a new connection to an external MySQL or MariaDB database that your pipeline server can access.
... In the *Host* field, enter the database hostname.
... In the *Port* field, enter the database port.
... In the *Username* field, enter the default user name that is connected to the database.
... In the *Password* field, enter the password for the default user account.
... In the *Database* field, enter the database name.

. Optional: By default, pipeline definitions are stored as Kubernetes resources, enabling version control, GitOps workflows, and integration with OpenShift GitOps or similar tools. To store pipeline definitions in the internal database instead, in the *Pipeline definition storage* section, clear the *Store pipeline definitions in Kubernetes* checkbox.

. Optional: By default, caching is configurable at both the pipeline and task levels. To disable caching for all pipelines and tasks in the pipeline server and override any pipeline-level and task-level caching settings, in the *Pipeline caching* section, clear the *Allow caching to be configured per pipeline and task* checkbox.

. Click *Configure pipeline server*.

.Verification
On the *Pipelines* tab for the project:

* The *Import pipeline* button is available.
* When you click the action menu (*&#8942;*) and then click *Manage pipeline server configuration*, the pipeline server details are displayed.


[role='_additional-resources']
.Additional resources
ifndef::upstream[]
* link:{rhoaidocshome}{default-format-url}/working_with_data_science_pipelines/managing-data-science-pipelines_ds-pipelines#overview-of-data-science-pipelines-caching_ds-pipelines[Overview of data science pipelines caching]
endif::[]
ifdef::upstream[]
* link:{odhdocshome}/working-with-data-science-pipelines/#overview-of-data-science-pipelines-caching_ds-pipelines[Overview of data science pipelines caching]
endif::[]
