:_module-type: PROCEDURE

[id='configuring-a-pipeline-server_{context}']
= Configuring a pipeline server

[role='_abstract']
Before you can successfully create a pipeline in {productname-short}, you must configure a pipeline server. This task includes configuring where your pipeline artifacts and data are stored.

[NOTE]
====
You are not required to specify any storage directories when configuring a data connection for your pipeline server. When you import a pipeline, the `/pipelines` folder is created in the `root` folder of the bucket, containing a YAML file for the pipeline. If you upload a new version of the same pipeline, a new YAML file with a different ID is added to the `/pipelines` folder.  

When you run a pipeline, the artifacts are stored in the `/pipeline-name` folder in the `root` folder of the bucket.
====

ifdef::upstream[]
[IMPORTANT]
====
If you use an external MySQL database and upgrade to {productname-short} 2.10.0, the database is migrated to data science pipelines 2.0 format, making it incompatible with earlier versions of {productname-short}.
====
endif::[]
ifndef::upstream[]
ifdef::self-managed[]
[IMPORTANT]
====
If you use an external MySQL database and upgrade to {productname-short} 2.10 or later, the database is migrated to data science pipelines 2.0 format, making it incompatible with earlier versions of {productname-short}.
====
endif::[]
ifdef::cloud-service[]
[IMPORTANT]
====
If you use an external MySQL database and upgrade to {productname-short} with data science pipelines 2.0, the database is migrated to data science pipelines 2.0 format, making it incompatible with earlier versions of {productname-short}.
====
endif::[]
endif::[]

.Prerequisites
* You have logged in to {productname-long}.
ifndef::upstream[]
* If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {oai-user-group} or {oai-admin-group} ) in OpenShift.
endif::[]
ifdef::upstream[]
* If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group} or {odh-admin-group}) in OpenShift.
endif::[]
* You have created a data science project that you can add a pipeline server to.
* You have an existing S3-compatible object storage bucket and you have configured write access to your S3 bucket on your storage account.
* If you are configuring a pipeline server with an external MySQL database, your database must use at least MySQL version 5.x. However, {org-name} recommends that you use MySQL version 8.x. 
* If you are configuring a pipeline server with a MariaDB database, your database must use MariaDB version 10.3 or later. However, {org-name} recommends that you use at least MariaDB version 10.5.

.Procedure
. From the {productname-short} dashboard, click *Data Science Projects*.
+
The *Data Science Projects* page opens.
. Click the name of the project that you want to configure a pipeline server for.
+
A project details page opens.
. Click the *Pipelines* tab.
. Click *Configure pipeline server*.
+
The *Configure pipeline server* dialog appears.
. In the *Object storage connection* section, provide values for the mandatory fields:
.. In the *Access key* field, enter the access key ID for the S3-compatible object storage provider.
.. In the *Secret key* field, enter the secret access key for the S3-compatible object storage account that you specified.
.. In the *Endpoint* field, enter the endpoint of your S3-compatible object storage bucket.
.. In the *Region* field, enter the default region of your S3-compatible object storage account.
.. In the *Bucket* field, enter the name of your S3-compatible object storage bucket.
+
[IMPORTANT]
====
If you specify incorrect data connection settings, you cannot update these settings on the same pipeline server. Therefore, you must delete the pipeline server and configure another one.
====

. In the *Database* section, click *Show advanced database options* to specify the database to store your pipeline data and select one of the following sets of actions:
* Select *Use default database stored on your cluster* to deploy a MariaDB database in your project.
* Select *Connect to external MySQL database* to add a new connection to an external database that your pipeline server can access.
... In the *Host* field, enter the database's host name.
... In the *Port* field, enter the database's port.
... In the *Username* field, enter the default user name that is connected to the database.
... In the *Password* field, enter the password for the default user account.
... In the *Database* field, enter the database name.
. Click *Configure pipeline server*.

.Verification
In the *Pipelines* tab for the project:

* The *Import pipeline* button is available.
* When you click the action menu (*&#8942;*) and then click *View pipeline server configuration*, the pipeline server details are displayed.


//[role="_additional-resources"]
//.Additional resources
