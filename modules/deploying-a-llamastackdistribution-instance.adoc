:_module-type: PROCEDURE

[id="deploying-a-llamastackdistribution-instance_{context}"]
= Deploying a LlamaStackDistribution instance

[role="_abstract"]
You can deploy Llama Stack with retrieval-augmented generation (RAG) by pairing it with a vLLM-served Llama 3.2 model. This module provides the following deployment examples of the `LlamaStackDistribution` custom resource (CR):

* *Example A:* Inline Milvus (embedded, single-node, remote embeddings)
* *Example B:* Remote Milvus (external service, inline embeddings served with the sentence-transformers library)
* *Example C:* Inline FAISS (embedded, single node, inline embeddings served with the sentence-transformers library)
* *Example D:* Remote PostgreSQL with pgvector (external service, remote embeddings)

ifdef::self-managed[]
ifdef::disconnected[]
If your cluster cannot pull images directly from public registries, first mirror the image to your local registry. For more information, see link:https://docs.redhat.com/en/documentation/openshift_container_platform/{ocp-latest-version}/html/disconnected_environments/mirroring-in-disconnected-environments#mirroring-images-disconnected-install[Mirroring images for disconnected installation] in the OpenShift documentation.
endif::[]
endif::[]

.Prerequisites
* You have installed {openshift-platform} {ocp-minimum-version} or newer.
ifndef::upstream[]
* You have enabled GPU support in {productname-short}. This includes installing the Node Feature Discovery Operator and NVIDIA GPU Operator. For more information, see link:https://docs.redhat.com/en/documentation/openshift_container_platform/{ocp-latest-version}/html/specialized_hardware_and_driver_enablement/psap-node-feature-discovery-operator#installing-the-node-feature-discovery-operator_psap-node-feature-discovery-operator[Installing the Node Feature Discovery Operator^] and link:{rhoaidocshome}{default-format-url}/managing_openshift_ai/enabling-accelerators#enabling-nvidia-gpus_managing-rhoai[Enabling NVIDIA GPUs^].
endif::[]
ifdef::upstream[]
* You have enabled GPU support. This includes installing the Node Feature Discovery and NVIDIA GPU Operators. For more information, see link:https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/index.html[NVIDIA GPU Operator on {org-name} OpenShift Container Platform^] in the NVIDIA documentation.
endif::[]
* You have cluster administrator privileges for your {openshift-platform} cluster.
* You are logged in to {productname-long}.
* You have activated the Llama Stack Operator in {productname-short}.
* You have deployed an inference model with vLLM (for example, *llama-3.2-3b-instruct*) and selected *Make deployed models available through an external route* and *Require token authentication* during model deployment. In addition, in *Add custom runtime arguments*, you have added *--enable-auto-tool-choice*.
* You have the correct inference model identifier, for example, `llama-3-2-3b`.
* You have the model endpoint URL ending with `/v1`, for example, `https://llama-32-3b-instruct-predictor:8443/v1`.
* You have the API token required to access the model endpoint.
* You have installed the PostgreSQL Operator version 14 or later and configured a PostgreSQL database for Llama Stack metadata storage. For more information, see the documentation for "Deploying a Llama Stack server".
* You have installed the {openshift-cli} as described in the appropriate documentation for your cluster:
ifdef::upstream,self-managed[]
** link:https://docs.redhat.com/en/documentation/openshift_container_platform/{ocp-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli[Installing the OpenShift CLI^] for OpenShift Container Platform
** link:https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws/{rosa-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli[Installing the OpenShift CLI^] for {rosa-productname}
endif::[]
ifdef::cloud-service[]
** link:https://docs.redhat.com/en/documentation/openshift_dedicated/{osd-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli[Installing the OpenShift CLI^] for OpenShift Dedicated
** link:https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws_classic_architecture/{rosa-classic-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli[Installing the OpenShift CLI^] for {rosa-classic-productname}
endif::[]

.Procedure

. Open a new terminal window and log in to your {openshift-platform} cluster from the CLI:
+
In the upper-right corner of the OpenShift web console, click your user name and select *Copy login command*. After you have logged in, click *Display token*. Copy the *Log in with this token* command and paste it in the {openshift-cli}.
+
[source,subs="+quotes"]
----
$ oc login --token=__<token>__ --server=__<openshift_cluster_url>__
----

. Create a secret that contains the inference model and the remote embeddings environment variables:
+
[source,terminal]
----
# Remote LLM
export INFERENCE_MODEL="llama-3-2-3b"
export VLLM_URL="https://llama-32-3b-instruct-predictor:8443/v1"
export VLLM_TLS_VERIFY="false"   # Use "true" in production
export VLLM_API_TOKEN="<token identifier>"
export VLLM_MAX_TOKENS=16384

# Remote embedding configuration
export EMBEDDING_MODEL="nomic-embed-text-v1-5"
export EMBEDDING_PROVIDER_MODEL_ID="nomic-embed-text-v1-5"
export VLLM_EMBEDDING_URL="<embedding-endpoint>/v1"
export VLLM_EMBEDDING_API_TOKEN="<embedding-token>"
export VLLM_EMBEDDING_MAX_TOKENS=8192
export VLLM_EMBEDDING_TLS_VERIFY="true"

oc create secret generic llama-stack-secret -n <project-name> \
  --from-literal=INFERENCE_MODEL="$INFERENCE_MODEL" \
  --from-literal=VLLM_URL="$VLLM_URL" \
  --from-literal=VLLM_TLS_VERIFY="$VLLM_TLS_VERIFY" \
  --from-literal=VLLM_API_TOKEN="$VLLM_API_TOKEN" \
  --from-literal=VLLM_MAX_TOKENS="$VLLM_MAX_TOKENS" \
  --from-literal=EMBEDDING_MODEL="$EMBEDDING_MODEL" \
  --from-literal=EMBEDDING_PROVIDER_MODEL_ID="$EMBEDDING_PROVIDER_MODEL_ID" \
  --from-literal=VLLM_EMBEDDING_URL="$VLLM_EMBEDDING_URL" \
  --from-literal=VLLM_EMBEDDING_TLS_VERIFY="$VLLM_EMBEDDING_TLS_VERIFY" \
  --from-literal=VLLM_EMBEDDING_API_TOKEN="$VLLM_EMBEDDING_API_TOKEN" \
  --from-literal=VLLM_EMBEDDING_MAX_TOKENS="$VLLM_EMBEDDING_MAX_TOKENS"
----

. Choose **one** of the following deployment examples:

[IMPORTANT]
====
To enable inline embeddings in a disconnected environment, add the following parameters to your `LlamaStackDistribution` custom resource:

[source,yaml]
----
# Enable inline embeddings with sentence-transformers
- name: ENABLE_SENTENCE_TRANSFORMERS
  value: "true"
- name: EMBEDDING_PROVIDER
  value: "sentence-transformers"

# Additional required configuration for disconnected environments
- name: SENTENCE_TRANSFORMERS_HOME
  value: /opt/app-root/src/.cache/huggingface/hub
- name: HF_HUB_OFFLINE
  value: "1"
- name: TRANSFORMERS_OFFLINE
  value: "1"
- name: HF_DATASETS_OFFLINE
  value: "1"
----

The built-in Llama Stack tool `websearch` is not available in the {org-name} Llama Stack Distribution in disconnected environments. In addition, the built-in Llama Stack tool `wolfram_alpha` is not available in the {org-name} Llama Stack Distribution in all clusters.
====

== Example A: LlamaStackDistribution with *Inline Milvus*

Use this example for development or small datasets where an embedded, single-node Milvus is sufficient. This example uses remote embeddings.

. In the OpenShift web console, select *Administrator* → *Quick Create* (image:images/quick-create-icon.png[]) → *Import YAML*, and create a CR similar to the following:
+
[source,yaml]
----
apiVersion: llamastack.io/v1alpha1
kind: LlamaStackDistribution
metadata:
  name: lsd-llama-milvus-inline
spec:
  replicas: 1
  server:
    containerSpec:
      resources:
        requests:
          cpu: "250m"
          memory: "500Mi"
        limits:
          cpu: 4
          memory: "12Gi"
      env:
        # PostgreSQL metadata store (required in {productname-short} 3.2)
        - name: POSTGRES_HOST
          value: <postgres-host>
        - name: POSTGRES_PORT
          value: "5432"
        - name: POSTGRES_DB
          value: <postgres-database>
        - name: POSTGRES_USER
          value: <postgres-username>
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: <postgres-secret-name>
              key: <postgres-password-key>

        # Remote LLM configuration
        - name: INFERENCE_MODEL
          valueFrom:
            secretKeyRef:
              name: llama-stack-secret
              key: INFERENCE_MODEL
        - name: VLLM_URL
          valueFrom:
            secretKeyRef:
              name: llama-stack-secret
              key: VLLM_URL
        - name: VLLM_TLS_VERIFY
          valueFrom:
            secretKeyRef:
              name: llama-stack-secret
              key: VLLM_TLS_VERIFY
        - name: VLLM_API_TOKEN
          valueFrom:
            secretKeyRef:
              name: llama-stack-secret
              key: VLLM_API_TOKEN
        - name: VLLM_MAX_TOKENS
          valueFrom:
            secretKeyRef:
              name: llama-stack-secret
              key: VLLM_MAX_TOKENS

        # Remote embedding configuration
        - name: EMBEDDING_MODEL
          valueFrom:
            secretKeyRef:
              name: llama-stack-secret
              key: EMBEDDING_MODEL
        - name: EMBEDDING_PROVIDER_MODEL_ID
          valueFrom:
            secretKeyRef:
              name: llama-stack-secret
              key: EMBEDDING_PROVIDER_MODEL_ID
        - name: VLLM_EMBEDDING_URL
          valueFrom:
            secretKeyRef:
              name: llama-stack-secret
              key: VLLM_EMBEDDING_URL
        - name: VLLM_EMBEDDING_TLS_VERIFY
          valueFrom:
            secretKeyRef:
              name: llama-stack-secret
              key: VLLM_EMBEDDING_TLS_VERIFY
        - name: VLLM_EMBEDDING_API_TOKEN
          valueFrom:
            secretKeyRef:
              name: llama-stack-secret
              key: VLLM_EMBEDDING_API_TOKEN
        - name: VLLM_EMBEDDING_MAX_TOKENS
          valueFrom:
            secretKeyRef:
              name: llama-stack-secret
              key: VLLM_EMBEDDING_MAX_TOKENS

        - name: FMS_ORCHESTRATOR_URL
          value: "http://localhost"
      name: llama-stack
      port: 8321
    distribution:
      name: rh-dev
    storage:
      size: 5Gi
----
+
[NOTE]
====
The `rh-dev` value is an internal image reference. When you create the `LlamaStackDistribution` custom resource, the {productname-short} Operator automatically resolves `rh-dev` to the container image in the appropriate registry. This internal image reference allows the underlying image to update without requiring changes to your custom resource.
====

== Example B: LlamaStackDistribution with *Remote Milvus*

Use this example for production-grade or large datasets with an external Milvus service. This example uses inline embeddings served with the sentence-transformers library.

. Create the Milvus connection secret:
+
[source,terminal]
----
# Required: gRPC endpoint on port 19530
export MILVUS_ENDPOINT="tcp://milvus-service:19530"
export MILVUS_TOKEN="<milvus-root-or-user-token>"
export MILVUS_CONSISTENCY_LEVEL="Bounded"   # Optional; choose per your deployment

oc create secret generic milvus-secret \
  --from-literal=MILVUS_ENDPOINT="$MILVUS_ENDPOINT" \
  --from-literal=MILVUS_TOKEN="$MILVUS_TOKEN" \
  --from-literal=MILVUS_CONSISTENCY_LEVEL="$MILVUS_CONSISTENCY_LEVEL"
----
+
[IMPORTANT]
====
Use the **gRPC port `19530`** for `MILVUS_ENDPOINT`. Ports such as `9091` are typically used for health checks and are not valid for client traffic.
====

. In the OpenShift web console, select *Administrator* → *Quick Create* (image:images/quick-create-icon.png[]) → *Import YAML*, and create a CR similar to the following:
+
[source,yaml]
----
apiVersion: llamastack.io/v1alpha1
kind: LlamaStackDistribution
metadata:
  name: lsd-llama-milvus-remote
spec:
  replicas: 1
  server:
    containerSpec:
      resources:
        requests:
          cpu: "250m"
          memory: "500Mi"
        limits:
          cpu: 4
          memory: "12Gi"
      env:
        # PostgreSQL metadata store (required in {productname-short} 3.2)
        - name: POSTGRES_HOST
          value: <postgres-host>
        - name: POSTGRES_PORT
          value: "5432"
        - name: POSTGRES_DB
          value: <postgres-database>
        - name: POSTGRES_USER
          value: <postgres-username>
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: <postgres-secret-name>
              key: <postgres-password-key>

        # Inline embeddings (sentence-transformers)
        - name: ENABLE_SENTENCE_TRANSFORMERS
          value: "true"
        - name: EMBEDDING_PROVIDER
          value: "sentence-transformers"

        # Remote LLM configuration
        - name: INFERENCE_MODEL
          valueFrom:
            secretKeyRef:
              name: llama-stack-secret
              key: INFERENCE_MODEL
        - name: VLLM_MAX_TOKENS
          value: "4096"
        - name: VLLM_URL
          valueFrom:
            secretKeyRef:
              name: llama-stack-secret
              key: VLLM_URL
        - name: VLLM_TLS_VERIFY
          valueFrom:
            secretKeyRef:
              name: llama-stack-secret
              key: VLLM_TLS_VERIFY
        - name: VLLM_API_TOKEN
          valueFrom:
            secretKeyRef:
              name: llama-stack-secret
              key: VLLM_API_TOKEN

        # Remote Milvus configuration from secret
        - name: MILVUS_ENDPOINT
          valueFrom:
            secretKeyRef:
              name: milvus-secret
              key: MILVUS_ENDPOINT
        - name: MILVUS_TOKEN
          valueFrom:
            secretKeyRef:
              name: milvus-secret
              key: MILVUS_TOKEN
        - name: MILVUS_CONSISTENCY_LEVEL
          valueFrom:
            secretKeyRef:
              name: milvus-secret
              key: MILVUS_CONSISTENCY_LEVEL
      name: llama-stack
      port: 8321
    distribution:
      name: rh-dev
----

== Example C: LlamaStackDistribution with *Inline FAISS*

Use this example to enable the inline FAISS vector store. This example uses inline embeddings served with the sentence-transformers library.

. In the OpenShift web console, select *Administrator* → *Quick Create* (image:images/quick-create-icon.png[]) → *Import YAML*, and create a CR similar to the following:
+
[source,yaml]
----
apiVersion: llamastack.io/v1alpha1
kind: LlamaStackDistribution
metadata:
  name: lsd-llama-faiss-inline
spec:
  replicas: 1
  server:
    containerSpec:
      resources:
        requests:
          cpu: "250m"
          memory: "500Mi"
        limits:
          cpu: "8"
          memory: "12Gi"
      env:
        # PostgreSQL metadata store (required in {productname-short} 3.2)
        - name: POSTGRES_HOST
          value: <postgres-host>
        - name: POSTGRES_PORT
          value: "5432"
        - name: POSTGRES_DB
          value: <postgres-database>
        - name: POSTGRES_USER
          value: <postgres-username>
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: <postgres-secret-name>
              key: <postgres-password-key>

        # Inline embeddings (sentence-transformers)
        - name: ENABLE_SENTENCE_TRANSFORMERS
          value: "true"
        - name: EMBEDDING_PROVIDER
          value: "sentence-transformers"

        # Remote LLM configuration
        - name: INFERENCE_MODEL
          valueFrom:
            secretKeyRef:
              name: llama-stack-secret
              key: INFERENCE_MODEL
        - name: VLLM_URL
          valueFrom:
            secretKeyRef:
              name: llama-stack-secret
              key: VLLM_URL
        - name: VLLM_TLS_VERIFY
          valueFrom:
            secretKeyRef:
              name: llama-stack-secret
              key: VLLM_TLS_VERIFY
        - name: VLLM_API_TOKEN
          valueFrom:
            secretKeyRef:
              name: llama-stack-secret
              key: VLLM_API_TOKEN

        # Enable inline FAISS
        - name: ENABLE_FAISS
          value: "faiss"

        - name: FMS_ORCHESTRATOR_URL
          value: "http://localhost"
      name: llama-stack
      port: 8321
    distribution:
      name: rh-dev
----

== Example D: LlamaStackDistribution with *Remote PostgreSQL with pgvector*

Use this example when you want to use a PostgreSQL database with the pgvector extension as the vector store backend. This configuration enables the pgvector provider and reads connection values from a secret. This example uses remote embeddings.

. Create the pgvector connection secret:
+
[source,terminal]
----
export PGVECTOR_HOST="<pgvector-hostname>"
export PGVECTOR_PORT="5432"
export PGVECTOR_DB="<pgvector-database>"
export PGVECTOR_USER="<pgvector-username>"
export PGVECTOR_PASSWORD="<pgvector-password>"

oc create secret generic pgvector-connection -n <project-name> \
  --from-literal=PGVECTOR_HOST="$PGVECTOR_HOST" \
  --from-literal=PGVECTOR_PORT="$PGVECTOR_PORT" \
  --from-literal=PGVECTOR_DB="$PGVECTOR_DB" \
  --from-literal=PGVECTOR_USER="$PGVECTOR_USER" \
  --from-literal=PGVECTOR_PASSWORD="$PGVECTOR_PASSWORD"
----

. In the OpenShift web console, select *Administrator* → *Quick Create* (image:images/quick-create-icon.png[]) → *Import YAML*, and create a custom resource similar to the following:
+
[source,yaml]
----
apiVersion: llamastack.io/v1alpha1
kind: LlamaStackDistribution
metadata:
  name: lsd-llama-pgvector-remote
spec:
  replicas: 1
  server:
    containerSpec:
      resources:
        requests:
          cpu: "250m"
          memory: "500Mi"
        limits:
          cpu: 4
          memory: "12Gi"
      env:
        # PostgreSQL metadata store (required in {productname-short} 3.2)
        - name: POSTGRES_HOST
          value: <postgres-host>
        - name: POSTGRES_PORT
          value: "5432"
        - name: POSTGRES_DB
          value: <postgres-database>
        - name: POSTGRES_USER
          value: <postgres-username>
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: <postgres-secret-name>
              key: <postgres-password-key>

        # Remote LLM configuration
        - name: INFERENCE_MODEL
          valueFrom:
            secretKeyRef:
              name: llama-stack-secret
              key: INFERENCE_MODEL
        - name: VLLM_URL
          valueFrom:
            secretKeyRef:
              name: llama-stack-secret
              key: VLLM_URL
        - name: VLLM_TLS_VERIFY
          valueFrom:
            secretKeyRef:
              name: llama-stack-secret
              key: VLLM_TLS_VERIFY
        - name: VLLM_API_TOKEN
          valueFrom:
            secretKeyRef:
              name: llama-stack-secret
              key: VLLM_API_TOKEN
        - name: VLLM_MAX_TOKENS
          valueFrom:
            secretKeyRef:
              name: llama-stack-secret
              key: VLLM_MAX_TOKENS

        # Remote embedding configuration
        - name: EMBEDDING_MODEL
          valueFrom:
            secretKeyRef:
              name: llama-stack-secret
              key: EMBEDDING_MODEL
        - name: EMBEDDING_PROVIDER_MODEL_ID
          valueFrom:
            secretKeyRef:
              name: llama-stack-secret
              key: EMBEDDING_PROVIDER_MODEL_ID
        - name: VLLM_EMBEDDING_URL
          valueFrom:
            secretKeyRef:
              name: llama-stack-secret
              key: VLLM_EMBEDDING_URL
        - name: VLLM_EMBEDDING_TLS_VERIFY
          valueFrom:
            secretKeyRef:
              name: llama-stack-secret
              key: VLLM_EMBEDDING_TLS_VERIFY
        - name: VLLM_EMBEDDING_API_TOKEN
          valueFrom:
            secretKeyRef:
              name: llama-stack-secret
              key: VLLM_EMBEDDING_API_TOKEN
        - name: VLLM_EMBEDDING_MAX_TOKENS
          valueFrom:
            secretKeyRef:
              name: llama-stack-secret
              key: VLLM_EMBEDDING_MAX_TOKENS

        # Enable and configure pgvector provider
        - name: ENABLE_PGVECTOR
          value: "true"
        - name: PGVECTOR_HOST
          valueFrom:
            secretKeyRef:
              name: pgvector-connection
              key: PGVECTOR_HOST
        - name: PGVECTOR_PORT
          valueFrom:
            secretKeyRef:
              name: pgvector-connection
              key: PGVECTOR_PORT
        - name: PGVECTOR_DB
          valueFrom:
            secretKeyRef:
              name: pgvector-connection
              key: PGVECTOR_DB
        - name: PGVECTOR_USER
          valueFrom:
            secretKeyRef:
              name: pgvector-connection
              key: PGVECTOR_USER
        - name: PGVECTOR_PASSWORD
          valueFrom:
            secretKeyRef:
              name: pgvector-connection
              key: PGVECTOR_PASSWORD

        - name: FMS_ORCHESTRATOR_URL
          value: "http://localhost"
      name: llama-stack
      port: 8321
    distribution:
      name: rh-dev
----

. Click *Create*.

.Verification
* In the left-hand navigation, click *Workloads* → *Pods* and verify that the Llama Stack pod is running in the correct namespace.
* To verify that the Llama Stack server is running, click the pod name and select the *Logs* tab. Look for output similar to the following:
+
[source,log]
----
INFO     2025-05-15 11:23:52,750 __main__:498 server: Listening on ['::', '0.0.0.0']:8321
INFO:     Started server process [1]
INFO:     Waiting for application startup.
INFO     2025-05-15 11:23:52,765 __main__:151 server: Starting up
INFO:     Application startup complete.
INFO:     Uvicorn running on http://['::', '0.0.0.0']:8321 (Press CTRL+C to quit)
----

[TIP]
====
If you switch between vector store configurations, delete the existing pod to ensure the new environment variables and backing store are picked up cleanly.
====
