:_module-type: PROCEDURE
//:disconnected:
//:upstream:
//:self-managed:

[id='enabling-nvidia-gpus_{context}']
= Enabling NVIDIA GPUs

[role='_abstract']
Before you can use NVIDIA GPUs in {productname-short}, you must install the NVIDIA GPU Operator. 

//the following note applies to self-managed connected only
ifdef::self-managed[]
ifndef::disconnected[]
[IMPORTANT]
====
If you are using {productname-short} in a disconnected self-managed environment, see link:{rhoaidocshome}{default-format-url}/installing_and_uninstalling_{url-productname-short}_in_a_disconnected_environment/enabling-accelerators_install#enabling-accelerators_install[Enabling accelerators] instead.
====
endif::[]
endif::[]

//the following note applies to cloud service only
ifdef::cloud-service[]
[IMPORTANT]
====
The NVIDIA GPU add-on is no longer supported. Instead, enable GPUs by installing the NVIDIA GPU Operator. If your deployment has a previously-installed NVIDIA GPU add-on, before you install the NVIDIA GPU Operator, use Red Hat OpenShift Cluster Manager to uninstall the NVIDIA GPU add-on from your cluster.
====
endif::[]


.Prerequisites
ifdef::upstream,self-managed[]
* You have logged in to your {openshift-platform} cluster.
* You have the `cluster-admin` role in your {openshift-platform} cluster.
* You have installed an NVIDIA GPU and confirmed that it is detected in your environment.
endif::[]
ifdef::cloud-service[]
* You have logged in to your OpenShift cluster.
* You have the `cluster-admin` role in your OpenShift cluster.
* You have installed an NVIDIA GPU and confirmed that it is detected in your environment.
endif::[]

.Procedure
//the following step applies to cloud service, self-managed connected, and upstream
ifndef::disconnected[]
. To enable GPU support on an OpenShift cluster, follow the instructions here: link:https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/index.html[NVIDIA GPU Operator on {org-name} OpenShift Container Platform^] in the NVIDIA documentation.
endif::[]
//the following step applies to self-managed disconnected only
ifdef::disconnected[]
. To enable GPU support on an OpenShift cluster in a disconnected or airgapped environment, follow the instructions here: link:https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/mirror-gpu-ocp-disconnected.html[Deploy GPU Operators in a disconnected or airgapped environment^] in the NVIDIA documentation.
endif::[]
//the following steps apply to upstream and downstream: self-managed (connected and disconnected) and cloud service
+
[IMPORTANT] 
====
After you install the Node Feature Discovery (NFD) Operator, you must create an instance of NodeFeatureDiscovery. In addition, after you install the NVIDIA GPU Operator, you must create a ClusterPolicy and populate it with default values. 
====
. Delete the *migration-gpu-status* ConfigMap.
ifdef::upstream,self-managed[]
.. In the {openshift-platform} web console, switch to the *Administrator* perspective.
endif::[]
ifdef::cloud-service[]
.. In the OpenShift web console, switch to the *Administrator* perspective.
endif::[]
.. Set the *Project* to *All Projects* or *redhat-ods-applications* to ensure you can see the appropriate ConfigMap.
.. Search for the *migration-gpu-status* ConfigMap.
.. Click the action menu (&#8942;) and select *Delete ConfigMap* from the list.
+
The *Delete ConfigMap* dialog opens.
.. Inspect the dialog and confirm that you are deleting the correct ConfigMap.
.. Click *Delete*.
. Restart the dashboard replicaset.
.. In the {openshift-platform} web console, switch to the *Administrator* perspective.
.. Click *Workloads* -> *Deployments*.
.. Set the *Project* to *All Projects* or *`pass:attributes[{dbd-config-default-namespace}]`* to ensure you can see the appropriate deployment.
.. Search for the *rhods-dashboard* deployment.
.. Click the action menu (&#8942;)  and select *Restart Rollout* from the list.
.. Wait until the *Status* column indicates that all pods in the rollout have fully restarted.

.Verification
* The reset *migration-gpu-status* instance is no longer present on the *Instances* tab on the `HardwareProfile` custom resource definition (CRD) details page.
* From the *Administrator* perspective, go to the *Operators* -> *Installed Operators* page. Confirm that the following Operators appear:

** NVIDIA GPU
** Node Feature Discovery (NFD)
** Kernel Module Management (KMM)

* The GPU is correctly detected a few minutes after full installation of the Node Feature Discovery (NFD) and NVIDIA GPU Operators. The {openshift-cli} displays the appropriate output for the GPU worker node. For example: 
+
[source]
----
# Expected output when the GPU is detected properly
oc describe node <node name>
...
Capacity:
  cpu:                4
  ephemeral-storage:  313981932Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16076568Ki
  nvidia.com/gpu:     1
  pods:               250
Allocatable:
  cpu:                3920m
  ephemeral-storage:  288292006229
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             12828440Ki
  nvidia.com/gpu:     1
  pods:               250 
----

//the following note applies to downstream only: self-managed (connected and disconnected) and cloud service
ifndef::upstream[]
ifdef::self-managed[]
[NOTE]
====
In {productname-short}, {org-name} supports the use of accelerators within the same cluster only. 

Starting from {productname-long} 2.19, {org-name} supports remote direct memory access (RDMA) for NVIDIA GPUs only, enabling them to communicate directly with each other by using NVIDIA GPUDirect RDMA across either Ethernet or InfiniBand networks.
====
endif::[]
ifdef::cloud-service[]
[NOTE]
====
In {productname-short}, {org-name} supports the use of accelerators within the same cluster only. 

{org-name} supports remote direct memory access (RDMA) for NVIDIA GPUs only, enabling them to communicate directly with each other by using NVIDIA GPUDirect RDMA across either Ethernet or InfiniBand networks.
====
endif::[]
endif::[]

//the following step applies to downstream only: self-managed (connected and disconnected) and cloud service
ifndef::upstream[]
After installing the NVIDIA GPU Operator, create a hardware profile as described in link:{rhoaidocshome}{default-format-url}/working_with_accelerators/#working-with-hardware-profiles_accelerators[Working with hardware profiles].
endif::[]
//the following step applies to upstream only
ifdef::upstream[]
After installing the NVIDIA GPU Operator, create a hardware profile as described in link:{odhdocshome}/working-with-accelerators/#working-with-hardware-profiles_accelerators[Working with hardware profiles].
endif::[]