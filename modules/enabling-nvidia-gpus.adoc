:_module-type: PROCEDURE
//:disconnected:
//:upstream:
//:self-managed:

[id='enabling-nvidia-gpus_{context}']
= Enabling NVIDIA GPUs

[role='_abstract']
Before you can use NVIDIA GPUs in {productname-short}, you must install the NVIDIA GPU Operator. 

//the following note applies to self-managed connected only
ifdef::self-managed[]
ifndef::disconnected[]
[IMPORTANT]
====
If you are using {productname-short} in a disconnected self-managed environment, see link:{rhoaidocshome}{default-format-url}/installing_and_uninstalling_{url-productname-short}_in_a_disconnected_environment/enabling-accelerators_install#enabling-accelerators_install[Enabling accelerators] instead.
====
endif::[]
endif::[]

//the following note applies to cloud service only
ifdef::cloud-service[]
[IMPORTANT]
====
The NVIDIA GPU add-on is no longer supported. Instead, enable GPUs by installing the NVIDIA GPU Operator. If your deployment has a previously-installed NVIDIA GPU add-on, before you install the NVIDIA GPU Operator, use Red Hat OpenShift Cluster Manager to uninstall the NVIDIA GPU add-on from your cluster.
====
endif::[]


.Prerequisites
ifdef::upstream,self-managed[]
* You have logged in to your {openshift-platform} cluster.
* You have the `cluster-admin` role in your {openshift-platform} cluster.
* You have installed an NVIDIA GPU and confirmed that it is detected in your environment.
endif::[]
ifdef::cloud-service[]
* You have logged in to your OpenShift cluster.
* You have the `cluster-admin` role in your OpenShift cluster.
* You have installed an NVIDIA GPU and confirmed that it is detected in your environment.
endif::[]

.Procedure
//the following step applies to cloud service, self-managed connected, and upstream
ifndef::disconnected[]
. To enable GPU support on an OpenShift cluster, follow the instructions here: link:https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/index.html[NVIDIA GPU Operator on {org-name} OpenShift Container Platform^] in the NVIDIA documentation.
endif::[]
//the following step applies to self-managed disconnected only
ifdef::disconnected[]
. To enable GPU support on an OpenShift cluster in a disconnected or airgapped environment, follow the instructions here: link:https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/mirror-gpu-ocp-disconnected.html[Deploy GPU Operators in a disconnected or airgapped environment^] in the NVIDIA documentation.
endif::[]
//the following steps apply to upstream and downstream: self-managed (connected and disconnected) and cloud service
+
[IMPORTANT] 
====
After you install the Node Feature Discovery (NFD) Operator, you must create an instance of NodeFeatureDiscovery. In addition, after you install the NVIDIA GPU Operator, you must create a ClusterPolicy and populate it with default values. 
====
. Delete the *migration-gpu-status* ConfigMap.
ifdef::upstream,self-managed[]
.. In the {openshift-platform} web console, switch to the *Administrator* perspective.
endif::[]
ifdef::cloud-service[]
.. In the OpenShift web console, switch to the *Administrator* perspective.
endif::[]
.. Set the *Project* to *All Projects* or *redhat-ods-applications* to ensure you can see the appropriate ConfigMap.
.. Search for the *migration-gpu-status* ConfigMap.
.. Click the action menu (&#8942;) and select *Delete ConfigMap* from the list.
+
The *Delete ConfigMap* dialog opens.
.. Inspect the dialog and confirm that you are deleting the correct ConfigMap.
.. Click *Delete*.
. Restart the dashboard replicaset.
ifdef::upstream,self-managed[]
.. In the {openshift-platform} web console, switch to the *Administrator* perspective.
endif::[]
ifdef::cloud-service[]
.. In the OpenShift web console, switch to the *Administrator* perspective.
endif::[]
.. Click *Workloads* -> *Deployments*.
.. Set the *Project* to *All Projects* or *redhat-ods-applications* to ensure you can see the appropriate deployment.
.. Search for the *rhods-dashboard* deployment.
.. Click the action menu (&#8942;)  and select *Restart Rollout* from the list.
.. Wait until the *Status* column indicates that all pods in the rollout have fully restarted.

.Verification
* The reset *migration-gpu-status* instance is present on the *Instances* tab on the `AcceleratorProfile` custom resource definition (CRD) details page.
* From the *Administrator* perspective, go to the *Operators* -> *Installed Operators* page. Confirm that the following Operators appear:

** NVIDIA GPU
** Node Feature Discovery (NFD)
** Kernel Module Management (KMM)

* The GPU is correctly detected a few minutes after full installation of the Node Feature Discovery (NFD) and NVIDIA GPU Operators. The {openshift-platform} command line interface (CLI) displays the appropriate output for the GPU worker node. For example: 
+
[source]
----
# Expected output when the GPU is detected properly
oc describe node <node name>
...
Capacity:
  cpu:                4
  ephemeral-storage:  313981932Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16076568Ki
  nvidia.com/gpu:     1
  pods:               250
Allocatable:
  cpu:                3920m
  ephemeral-storage:  288292006229
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             12828440Ki
  nvidia.com/gpu:     1
  pods:               250 
----

//the following note applies to downstream only: self-managed (connected and disconnected) and cloud service
ifndef::upstream[]
ifdef::self-managed[]
[NOTE]
====
In {productname-short}, {org-name} supports the use of accelerators within the same cluster only. 

Starting from {productname-long} 2.19, {org-name} supports remote direct memory access (RDMA) for NVIDIA GPUs only, enabling them to communicate directly with each other by using NVIDIA GPUDirect RDMA across either Ethernet or InfiniBand networks.
====
endif::[]
ifdef::cloud-service[]
[NOTE]
====
In {productname-short}, {org-name} supports the use of accelerators within the same cluster only. 

{org-name} supports remote direct memory access (RDMA) for NVIDIA GPUs only, enabling them to communicate directly with each other by using NVIDIA GPUDirect RDMA across either Ethernet or InfiniBand networks.
====
endif::[]
endif::[]

//the following step applies to downstream only: self-managed (connected and disconnected) and cloud service
ifndef::upstream[]
After installing the NVIDIA GPU Operator, create an accelerator profile as described in link:{rhoaidocshome}{default-format-url}/working_with_accelerators/#working-with-accelerator-profiles_accelerators[Working with accelerator profiles].
endif::[]
//the following step applies to upstream only
ifdef::upstream[]
After installing the NVIDIA GPU Operator, create a hardware profile as described in link:{odhdocshome}/working-with-accelerators/[Working with accelerators].
endif::[]
+
[IMPORTANT]
====
By default, hardware profiles are hidden in the dashboard navigation menu and user interface, while accelerator profiles remain visible. In addition, user interface components associated with the deprecated accelerator profiles functionality are still displayed. To show the *Settings -> Hardware profiles* option in the dashboard navigation menu, and the user interface components associated with hardware profiles, set the `disableHardwareProfiles` value to `false` in the `OdhDashboardConfig` custom resource (CR) in {openshift-platform}. 
ifdef::upstream[]
For more information about setting dashboard configuration options, see link:{odhdocshome}/managing-resources/#customizing-the-dashboard[Customizing the dashboard].
endif::[]
ifndef::upstream[]
For more information about setting dashboard configuration options, see link:{rhoaidocshome}{default-format-url}/managing_resources/customizing-the-dashboard[Customizing the dashboard].
endif::[]
==== 