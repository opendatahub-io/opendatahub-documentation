:_module-type: CONCEPT

[id="using-accelerators-with-vllm_{context}"]
= Using accelerators with vllm

[role="_abstract"]
{productname-short} includes support for NVIDIA, AMD and Intel Gaudi accelerators. {productname-short} also includes preinstalled model-serving runtimes that provide accelerator support.

== NVIDIA GPUs

ifndef::upstream[]
You can serve models with NVIDIA graphics processing units (GPUs) by using the *vLLM ServingRuntime for KServe* runtime. To use the runtime, you must enable GPU support in {productname-short}. This includes installing and configuring the Node Feature Discovery operator on your cluster. For more information, see link:https://docs.redhat.com/en/documentation/openshift_container_platform/{ocp-latest-version}/html/specialized_hardware_and_driver_enablement/psap-node-feature-discovery-operator#installing-the-node-feature-discovery-operator_psap-node-feature-discovery-operator[Installing the Node Feature Discovery operator^] and link:{rhoaidocshome}{default-format-url}/managing_openshift_ai/enabling_accelerators#enabling-nvidia-gpus_managing-rhoai[Enabling NVIDIA GPUs^].
endif::[]

ifdef::upstream[]
You can serve models with NVIDIA graphics processing units (GPUs) by using the *vLLM ServingRuntime for KServe* runtime. To use the runtime, you must enable GPU support in {productname-short}. This includes installing the Node Feature Discovery and NVIDIA GPU Operators. For more information, see https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/index.html[NVIDIA GPU Operator on {org-name} OpenShift Container Platform^] in the NVIDIA documentation.
endif::[]


== Intel Gaudi accelerators

ifdef::upstream[]
You can serve models with Intel Gaudi accelerators by using the *vLLM ServingRuntime with Gaudi accelerators support for KServe* runtime. To use the runtime, you must enable hybrid processing support (HPU) support in {productname-short}. This includes installing the Intel Gaudi AI accelerator operator and configuring an accelerator profile. For more information, see link:https://docs.habana.ai/en/latest/Installation_Guide/Additional_Installation/Intel_Gaudi_Base_Operator/Environment_Setup.html[Setting up Gaudi for OpenShift^] and link:{odhdocshome}/working_with_accelerators/working-with-accelerator-profiles_accelerators#working-with-accelerator-profiles_accelerators[Working with accelerators^].

For more information on recommended vLLM parameters, environment variables, supported configurations and more, please see link:https://github.com/HabanaAI/vllm-fork/blob/habana_main/README_GAUDI.md[vLLM with Intel速 Gaudi速 AI Accelerators^].
endif::[]

ifndef::upstream[]
You can serve models with Intel Gaudi accelerators by using the *vLLM ServingRuntime with Gaudi accelerators support for KServe* runtime. To use the runtime, you must enable hybrid processing support (HPU) support in {productname-short}. This includes installing the Intel Gaudi AI accelerator operator and configuring an accelerator profile. For more information, see link:https://docs.habana.ai/en/latest/Installation_Guide/Additional_Installation/Intel_Gaudi_Base_Operator/Environment_Setup.html[Setting up Gaudi for OpenShift^] and link:{rhoaidocshome}{default-format-url}/working_with_accelerators/working-with-accelerator-profiles_accelerators#working-with-accelerator-profiles_accelerators[Working with accelerators^]. 

For information on recommended vLLM parameters, environment variables, supported configurations and more, please see link:https://github.com/HabanaAI/vllm-fork/blob/habana_main/README_GAUDI.md[vLLM with Intel速 Gaudi速 AI Accelerators^].
endif::[]

== AMD GPUs

ifdef::upstream[]
You can serve models with AMD GPUs by using the *vLLM ROCm ServingRuntime for KServe* runtime. To use the runtime, you must enable support for AMD graphic processing units (GPUs) in {productname-short}. This includes installing the AMD GPU operator and configuring an accelerator profile. For more information, see link:https://dcgpu.docs.amd.com/projects/gpu-operator/en/latest/installation/openshift-olm.html[Deploying the AMD GPU operator on OpenShift^] and link:{odhdocshome}/working_with_accelerators/working-with-accelerator-profiles_accelerators#working-with-accelerator-profiles_accelerators[Working with accelerators^].
endif::[]

ifndef::upstream[]
You can serve models with AMD GPUs by using the *vLLM ROCm ServingRuntime for KServe* runtime. To use the runtime, you must enable support for AMD graphic processing units (GPUs) in {productname-short}. This includes installing the AMD GPU operator and configuring an accelerator profile. For more information, see link:https://dcgpu.docs.amd.com/projects/gpu-operator/en/latest/installation/openshift-olm.html[Deploying the AMD GPU operator on OpenShift^] and link:{rhoaidocshome}{default-format-url}/working_with_accelerators/working-with-accelerator-profiles_accelerators#working-with-accelerator-profiles_accelerators[Working with accelerators^].
endif::[]

[role="_additional-resources"]
.Additional resources
ifndef::upstream[]
* link:{rhoaidocshome}{default-format-url}/serving_models/serving-large-models_serving-large-models#supported-model-serving-runtimes_serving-large-models[Supported model-serving runtimes^]
endif::[]
ifdef::upstream[]
* link:{odhdocshome}/serving-models/#ref-supported-runtimes_serving-large-models[Supported model-serving runtimes^]
endif::[]
