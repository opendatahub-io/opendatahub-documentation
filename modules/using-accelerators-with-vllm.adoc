:_module-type: CONCEPT

[id="using-accelerators-with-vllm_{context}"]
= Using accelerators with vLLM

[role="_abstract"]
{productname-short} includes support for NVIDIA, AMD and Intel Gaudi accelerators. {productname-short} also includes preinstalled model-serving runtimes that provide accelerator support.

== NVIDIA GPUs

ifndef::upstream[]
You can serve models with NVIDIA graphics processing units (GPUs) by using the *vLLM NVIDIA GPU ServingRuntime for KServe* runtime. To use the runtime, you must enable GPU support in {productname-short}. This includes installing and configuring the Node Feature Discovery operator on your cluster. For more information, see link:https://docs.redhat.com/en/documentation/openshift_container_platform/{ocp-latest-version}/html/specialized_hardware_and_driver_enablement/psap-node-feature-discovery-operator#installing-the-node-feature-discovery-operator_psap-node-feature-discovery-operator[Installing the Node Feature Discovery operator^] and link:{rhoaidocshome}{default-format-url}/managing_openshift_ai/enabling_accelerators#enabling-nvidia-gpus_managing-rhoai[Enabling NVIDIA GPUs^].
endif::[]

ifdef::upstream[]
You can serve models with NVIDIA graphics processing units (GPUs) by using the *vLLM NVIDIA GPU ServingRuntime for KServe* runtime. To use the runtime, you must enable GPU support in {productname-short}. This includes installing the Node Feature Discovery and NVIDIA GPU Operators. For more information, see link:https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/index.html[NVIDIA GPU Operator on {org-name} OpenShift Container Platform^] in the NVIDIA documentation.
endif::[]


== Intel Gaudi accelerators

ifdef::upstream[]
You can serve models with Intel Gaudi accelerators by using the *vLLM Intel Gaudi Accelerator ServingRuntime for KServe* runtime. To use the runtime, you must enable hybrid processing (HPU) support in {productname-short}. This includes installing the Intel Gaudi Base Operator and configuring a hardware profile. For more information, see link:https://docs.habana.ai/en/latest/Installation_Guide/Additional_Installation/OpenShift_Installation/index.html#openshift-installation[Setting up Gaudi for OpenShift^] and link:{odhdocshome}/working-with-accelerators/#working-with-hardware-profiles_accelerators[Working with hardware profiles^].

For information about recommended vLLM parameters, environment variables, supported configurations and more, see link:https://github.com/HabanaAI/vllm-fork/blob/habana_main/README_GAUDI.md[vLLM with Intel速 Gaudi速 AI Accelerators^].
endif::[]

ifndef::upstream[]
You can serve models with Intel Gaudi accelerators by using the *vLLM Intel Gaudi Accelerator ServingRuntime for KServe* runtime. To use the runtime, you must enable hybrid processing (HPU) support in {productname-short}. This includes installing the Intel Gaudi Base Operator and configuring a hardware profile. For more information, see link:https://docs.habana.ai/en/latest/Installation_Guide/Additional_Installation/OpenShift_Installation/index.html#openshift-installation[Setting up Gaudi for OpenShift^] and link:{rhoaidocshome}{default-format-url}/working_with_accelerators/working-with-hardware-profiles_accelerators[Working with hardware profiles^]. 

For information about recommended vLLM parameters, environment variables, supported configurations and more, see link:https://github.com/HabanaAI/vllm-fork/blob/habana_main/README_GAUDI.md[vLLM with Intel速 Gaudi速 AI Accelerators^].
endif::[]
[NOTE]
====
Warm-up is a model initialization and performance optimization step that is useful for reducing cold-start delays and first-inference latency. Depending on the model size, warm-up can lead to longer model loading times. 

While highly recommended in production environments to avoid performance limitations, you can choose to skip warm-up for non-production environments to reduce model loading times and accelerate model development and testing cycles.
ifndef::upstream[]
To skip warm-up, follow the steps described in link:{rhoaidocshome}{default-format-url}/serving_models/serving-large-models_serving-large-models#customizing-parameters-serving-runtime_serving-large-models[Customizing the parameters of a deployed model-serving runtime] to add the following environment variable in the *Configuration parameters* section of your model deployment:
[source]
----
`VLLM_SKIP_WARMUP="true"`
----
endif::[]
ifdef::upstream[]
To skip warm-up, follow the steps described in link:{odhdocshome}/serving-models/#deploying-models-using-the-single-model-serving-platform_serving-large-models[Customizing the parameters of a deployed model-serving runtime] to add the following environment variable in the *Configuration parameters* section of your model deployment:
[source]
----
`VLLM_SKIP_WARMUP="true"`
----
endif::[]
====

== AMD GPUs

ifdef::upstream[]
You can serve models with AMD GPUs by using the *vLLM AMD GPU ServingRuntime for KServe* runtime. To use the runtime, you must enable support for AMD graphic processing units (GPUs) in {productname-short}. This includes installing the AMD GPU operator and configuring a hardware profile. For more information, see link:https://instinct.docs.amd.com/projects/gpu-operator/en/latest/installation/openshift-olm.html[Deploying the AMD GPU operator on OpenShift^] and link:{odhdocshome}/working-with-accelerators/#working-with-hardware-profiles_accelerators[Working with hardware profiles^].
endif::[]

ifndef::upstream[]
You can serve models with AMD GPUs by using the *vLLM AMD GPU ServingRuntime for KServe* runtime. To use the runtime, you must enable support for AMD graphic processing units (GPUs) in {productname-short}. This includes installing the AMD GPU operator and configuring a hardware profile. For more information, see link:https://instinct.docs.amd.com/projects/gpu-operator/en/latest/installation/openshift-olm.html[Deploying the AMD GPU operator on OpenShift^] in the AMD documentation and link:{rhoaidocshome}{default-format-url}/working_with_accelerators/working-with-hardware-profiles_accelerators[Working with hardware profiles^].
endif::[]

[role="_additional-resources"]
.Additional resources
ifndef::upstream[]
* link:{rhoaidocshome}{default-format-url}/serving_models/serving-large-models_serving-large-models#supported-model-serving-runtimes_serving-large-models[Supported model-serving runtimes^]
endif::[]
ifdef::upstream[]
* link:{odhdocshome}/serving-models/#supported-model-serving-runtimes_serving-large-models[Supported model-serving runtimes^]
endif::[]
