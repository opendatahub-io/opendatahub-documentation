:_module-type: PROCEDURE

[id="enabling-authentication-and-authorization-for-llm-inference-service_{context}"]
= Managing authentication and authorization for an LLM inference service

[role='_abstract']
In {productname-short} 3.0 and later, authentication and authorization are automatically enabled for `LLMInferenceService` resources when {org-name} Connectivity Link is configured. You can use the `security.opendatahub.io/enable-auth: "true"` annotation to explicitly enable authentication, such as re-enabling it after it was previously disabled.

.Prerequisites

* You have configured {org-name} Connectivity Link for {llmd} as described in _Configuring authentication for {llmd} using {org-name} Connectivity Link_.
* You have created an `LLMInferenceService` resource as described in _Enabling {llmd}_.
* You have access to the OpenShift CLI (`oc`).

.Procedure

. By default, authentication is enabled automatically. To explicitly enable authentication or to re-enable it after disabling, annotate your `LLMInferenceService` resource:
+
[source,yaml]
----
apiVersion: serving.kserve.io/v1alpha1
kind: LLMInferenceService
metadata:
  name: sample-llm-inference-service
  annotations:
    security.opendatahub.io/enable-auth: "true"
spec:
  replicas: 2
  model:
    uri: hf://RedHatAI/Qwen3-8B-FP8-dynamic
    name: RedHatAI/Qwen3-8B-FP8-dynamic
  router:
    route: {}
    gateway: {}
    scheduler: {}
    template:
      containers:
      - name: main
        resources:
          limits:
            cpu: '4'
            memory: 32Gi
            nvidia.com/gpu: "1"
          requests:
            cpu: '2'
            memory: 16Gi
            nvidia.com/gpu: "1"
----
+
NOTE: In {productname-short} 3.0 and later, the `security.opendatahub.io/enable-auth: "true"` annotation is the default behavior when Connectivity Link is configured. You only need to add this annotation if you previously disabled authentication.

. Apply the configuration:
+
[source,bash]
----
oc apply -f <llm-inference-service-file>.yaml
----

. Verify that the inference service is protected by attempting to access it without authentication:
+
[source,bash]
----
curl -v https://<inference-endpoint-url>/v1/models
----
+
You should receive a `401 Unauthorized` response.

.Verification

* Confirm that the `LLMInferenceService` resource has the annotation:
+
[source,bash]
----
oc get llminferenceservice sample-llm-inference-service -o jsonpath='{.metadata.annotations.security\.opendatahub\.io/enable-auth}'
----
+
This command should return `true`.

[role='_additional-resources']
.Additional resources

* link:https://docs.redhat.com/en/documentation/red_hat_connectivity_link/1.2/html/configuring_and_deploying_gateway_policies_with_connectivity_link/app-developer-workflow_rhcl[Override your Gateway policies for auth and rate limiting^]
