:_module-type: CONCEPT

[id="about-the-NVIDIA-NIM-model-serving-platform_{context}"]
= About the NVIDIA NIM model serving platform

[role="_abstract"]

The *NVIDIA NIM model serving platform* allows you to deploy NVIDIA optimized models using NVIDIA NIM inference services.

NVIDIA NIM, part of NVIDIA AI Enterprise, is a set of easy-to-use microservices designed for secure, reliable deployment of high performance AI model inferencing across clouds, data centers and workstations.

ifndef::upstream[]
[IMPORTANT]
====
The NVIDIA NIM feature is currently available in {productname-long} as a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.

For more information about the support scope of Red Hat Technology Preview features, see link:https://access.redhat.com/support/offerings/techpreview[Technology Preview Features Support Scope]
====
endif::[]

[role="_additional-resources"]
.Additional resources
* link:https://docs.nvidia.com/nim/index.html[NVIDIA NIM]
