:_module-type: CONCEPT

[id="about-the-NVIDIA-NIM-model-serving-platform_{context}"]
= About the NVIDIA NIM model serving platform

[role="_abstract"]

You can deploy models using NVIDIA NIM inference services on the *NVIDIA NIM model serving platform*.

NVIDIA NIM, part of NVIDIA AI Enterprise, is a set of microservices designed for secure, reliable deployment of high performance AI model inferencing across clouds, data centers and workstations.

ifndef::upstream[]
[IMPORTANT]
====
The *NVIDIA NIM model serving platform* is currently available in {productname-long} as a Technology Preview feature only. Technology Preview features are not supported with {org-name} production service level agreements (SLAs) and might not be functionally complete. {org-name} does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.

For more information about the support scope of {org-name} Technology Preview features, see link:https://access.redhat.com/support/offerings/techpreview[Technology Preview Features Support Scope]
====
endif::[]

[role="_additional-resources"]
.Additional resources
* link:https://docs.nvidia.com/nim/index.html[NVIDIA NIM]
