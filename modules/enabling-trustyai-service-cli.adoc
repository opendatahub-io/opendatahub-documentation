:_module-type: PROCEDURE

[id='enabling-trustyai-service-cli_{context}']
= Enabling the TrustyAI Service for a data science project

ifndef::upstream[]
[IMPORTANT]
====
The TrustyAI features are for Technology Preview only. Technology Preview features are not supported with {org-name} production service level agreements (SLAs), might not be functionally complete, and {org-name} does not recommend using them for production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process. 			
For more information on {org-name} Technology Preview features, see link:https://access.redhat.com/support/offerings/techpreview/ [Technology Preview Features Scope]. 		
====
endif::[]

[role='_abstract']
When you install {productname-short}, the TrustyAI Operator is included with the other components in the {productname-short} namespace. The TrustyAI operator is responsible for deploying and managing TrustyAI services, as well as managing all other resources required by TrustyAI.

These managing tasks include configuring storage, creating the service monitors, and configuring the serving runtimes and routes.

The TrustyAI operator manages all enabled TrustyAI services across any number of projects on your cluster.

For each data science project (namespace) that contains models for which you want to monitor bias metrics, enable an instance of the TrustyAI service.

*NOTE:* You should enable only one instance of the TrustyAI service in a project. The presence of multiple instances in the same project can result in unexpected behavior.

.Prerequisites

* On the OpenShift cluster where {productname-short} is installed, you have enabled user workload monitoring as described in link:https://docs.openshift.com/container-platform/{ocp-latest-version}/monitoring/enabling-monitoring-for-user-defined-projects.html[Enabling monitoring for user-defined projects].

ifdef::upstream[]
* You have installed {productname-short} as described in link:https://opendatahub.io/docs/quick-installation-new-operator/[Quick Installation(v2)].
endif::[]

ifndef::upstream[]
* You have installed {productname-short} as described in link:{rhodsdocshome}{default-format-url}/installing_and_uninstalling_openshift_data_science_self-managed/installing-the-openshift-data-science-operator_operator-install[Installing and managing version 2 of the {productname-long} Operator].
endif::[]

* The `trustyai` component is set to *Managed* for the {productname-short} Operator.
+
To verify this setting, navigate to *Operators* -> *Installed Operators* -> *{productname-long} Operator* -> *Data Science Cluster*. Select the *default* instance and then click *YAML*. Scroll down to view the `spec:components` setting:
+
----
 trustyai:
      devFlags: {}
      managementState: Managed
----
+
*NOTE:* If the `trustyai` component is set to *Removed*, edit the YAML file to set it to *Managed*.

* You have logged in to {productname-short}.

ifndef::upstream[]
* If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, `rhods-users` or `rhods-admins`) in OpenShift.

* You have created a data science project, as described in link:{rhodsdocshome}{default-format-url}/working_on_data_science_projects/working-on-data-science-projects_nb-server#creating-a-data-science-project_nb-server[Creating a data science project], that contains (or will contain) the models that you want to monitor.  
endif::[]

ifdef::upstream[]
* If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, `odh-users` or `odh-admins`) in OpenShift.

* You have created a data science project, as described in link:{odhdocshome}/working-on-data-science-projects/#working-on-data-science-projects_nb-server[Creating a data science project], that contains (or will contain) the models that you want to monitor.  
endif::[]

.Procedure

Add an instance of the TrustyAI service to each data science project that contains models that you want to monitor. The TrustyAI service instance provides the URL that a developer uses to monitor and analyze any number of models deployed into a data science project.

To enable the TrustyAI service from the command line, follow these steps:

. Login to your cluster.
+
----
 oc login
----

. Navigate to the data science project that contains (or will contain) the models that you want to monitor. 
+
----
oc project <project-name>
----
+
For example:
+
----
oc project mydsproject
----

. Create a `TrustyAIService` custom resource (CR) file, for example `trustyai_crd.yaml`: 
+
----
apiVersion: trustyai.opendatahub.io.trustyai.opendatahub.io/v1alpha1
kind: TrustyAIService
metadata:
  name: trustyai-service
spec:
  storage:
	format: "PVC"
	folder: "/inputs"
	size: "1Gi"
  data:
	filename: "data.csv"
	format: "CSV"
  metrics:
	schedule: "5s"
	batchSize: 5000 # Optional, default is 5000
----
+ 
Here is a description of the fields:
+
`metadata.name`:: The name of the TrustyAI service instance.
`spec.storage.format`:: The storage format for the data. Currently, only persistent volume control (PVC) is supported.
`spec.storage.folder`:: The location within the PVC where the data will be stored.
`spec.storage.size`:: The size of the PVC to request.
`spec.data.filename`:: The suffix for the stored data files.
`spec.data.format`:: The format of the data. Currently, only comma-separated value (CSV) format is supported.
`spec.metrics.schedule`:: The interval at which the metrics are calculated. The default is 5s. The duration is specified with the ISO-8601 format. For example, `5s` for 5 seconds, `5m` for 5 minutes, and `5h` for 5 hours.
`spec.metrics.batchSize`:: The observationâ€™s historical window size to use for metrics calculation. The default is `5000` (that is, the metrics are calculated by using the latest 5000 inferences).

. Add the TrustyAI service's CR to your project:
+
----
oc apply -f trustyai_crd.yaml
----
+
This command should return output similar to the following:
+
----
trusty-service created
----


.Verification

To verify that you enabled the TrustyAI Service:

----
oc get pods | grep trustyai 
----

You should see a response similar to the following:

----
trustyai-service-5d45b5884f-96h5z             1/1     Running
----