:_module-type: CONCEPT

[id="understanding-the-spark-compute-engine-in-feature-store_{context}"]

== Understanding the Spark compute engine in Feature Store 

[role="_abstract"]
Use the Spark compute engine to run distributed batch materialization and historical retrieval operations. Batch materialization includes materialize and materialize-incremental operations. The engine processes large-scale data from offline stores, such as Snowflake, Google BigQuery, and Apache Spark SQL.

Use the Spark compute engine to read from various data sources and perform distributed or custom transformations. You can use the engine to perform these tasks:
• Read from various data sources, such as Apache Spark SQL, Google BigQuery, and Snowflake.
• Execute distributed feature transformations and aggregations.
• Run custom transformations by using Apache Spark SQL or user-defined functions (UDFs).