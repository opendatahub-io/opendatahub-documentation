:_module-type: PROCEDURE

[id="deploying-models-using-distributed-inference_{context}"]
= Deploying models by using {llmd}

[role='_abstract']

ifndef::upstream[]
[IMPORTANT]
====
ifdef::self-managed[]
{llmd} is currently available in {productname-long} {vernum} as a Technology Preview feature.
endif::[]
ifdef::cloud-service[]
{llmd} is currently available in {productname-long} as a Technology Preview feature.
endif::[]
Technology Preview features are not supported with {org-name} production service level agreements (SLAs) and might not be functionally complete.
{org-name} does not recommend using them in production.
These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.

For more information about the support scope of {org-name} Technology Preview features, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].
====
endif::[]

{llmd} is a Kubernetes-native, open-source framework designed for serving large language models (LLMs) at scale. You can use {llmd} to simplify the deployment of generative AI, focusing on high performance and cost-effectiveness across various hardware accelerators.

Key features of {llmd} include:

* Efficiently handles large models using optimizations such as prefix-cache aware routing and disaggregated serving.
* Integrates into a standard Kubernetes environment, where it leverages specialized components like the Envoy proxy to handle networking and routing, and high-performance libraries such as vLLM and NVIDIA Inference Transfer Library (NIXL).
* Tested recipes and well-known presets reduce the complexity of deploying inference at scale, so users can focus on building applications rather than managing infrastructure.

Serving models using {llmd} on {productname-long} consists of the following steps:

. Installing {productname-short}.
+
NOTE: Because KServe Serverless conflicts with the Gateway API used for {llmd}, KServe Serverless is not supported on the same cluster. Instead, use KServe RawDeployment.

. Enabling the single model serving platform.
. Enabling {llmd} on a Kubernetes cluster.
. Creating an LLMInferenceService Custom Resource (CR).
. Deploying a model.

This procedure describes how to create a custom resource (CR) for an `LLMInferenceService` resource. You replace the default `InferenceService` with the `LLMInferenceService`.

.Prerequisites

* You have enabled the single model-serving platform.
* You have access to an {openshift-platform} cluster running version 4.19.9 or later.
* {openshift-platform} Service Mesh v2 is not installed in the cluster.
* You have created a `GatewayClass` and a `Gateway` named `openshift-ai-inference` in the `openshift-ingress` namespace as described in link:https://docs.redhat.com/en/documentation/openshift_container_platform/latest/html/ingress_and_load_balancing/configuring-ingress-cluster-traffic#ingress-gateway-api[Gateway API with {openshift-platform} Container Platform Networking]. 
* You have installed the `LeaderWorkerSet` Operator in {openshift-platform}. For more information, see the {openshift-platform} documentation.

.Procedure

. Log in to the {openshift-platform} console as a cluster administrator.

. Create a data science cluster initialization (DSCI) and set the `serviceMesh.managementState` to `removed`, as shown in the following example:
+
[source]
----
serviceMesh:
  ...
  managementState: Removed
----

. Create a data science cluster (DSC) with the following information set in `kserve` and `serving`:
+
[source]
----
kserve:
  defaultDeploymentMode: RawDeployment
  managementState: Managed
  ...
  serving:
    ...
    managementState: Removed
    ...
----

. Create the `LLMInferenceService` CR with the following information:
+
--
[source]
----
apiVersion: serving.kserve.io/v1alpha1
kind: LLMInferenceService
metadata:
  name: sample-llm-inference-service
spec:
  replicas: 2
  model:
    uri: hf://RedHatAI/Qwen3-8B-FP8-dynamic
    name: RedHatAI/Qwen3-8B-FP8-dynamic
  router: 
    route: {}
    gateway: {}
    scheduler: {}
    template:
      containers:
      - name: main
        resources:
          limits:
            cpu: '4'
            memory: 32Gi
            nvidia.com/gpu: "1"
          requests:
            cpu: '2'
            memory: 16Gi
            nvidia.com/gpu: "1"
----

Customize the following parameters in the `spec` section of the inference service:

* `replicas` - Specify the number of replicas.
* `model` - Provide the URI to the model based on how the model is stored (`uri`) and the model name to use in chat completion requests (`name`).
** S3 bucket:  `s3://<bucket-name>/<object-key>`
** Persistent volume claim (PVC): `pvc://<claim-name>/<pvc-path>`
** OCI container image: `oci://<registry_host>/<org_or_username>/<repository_name><tag_or_digest>`
** HuggingFace: `hf://<model>/<optional-hash>`
* `router` - Provide an HTTPRoute and gateway, or leave blank to automatically create one.
--

. Save the file.
