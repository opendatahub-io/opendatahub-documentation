:_module-type: PROCEDURE
[id="setting-up-ragas-inline-provider_{context}"]
= Setting up the Ragas inline provider for development

[role='_abstract']
You can set up the Ragas inline provider to run evaluation workloads directly inside the Llama Stack server pod. The inline provider is intended for development and experimentation, where simplicity and rapid iteration are more important than scalability or isolation.

.Prerequisites
* You have cluster administrator privileges for your {openshift-platform} cluster.
* You have installed the {openshift-cli}.
* You have activated the Llama Stack Operator in {productname-short}.
ifdef::upstream[]
For more information, see link:{odhdocshome}/working-with-llama-stack/#activating-the-llama-stack-operator_rag[Activating the Llama Stack Operator].
endif::[]
ifndef::upstream[]
For more information, see link:{rhoaidocshome}{default-format-url}/working_with_llama_stack/activating-the-llama-stack-operator_rag[Activating the Llama Stack Operator].
endif::[]

* You have deployed a Llama model with KServe.
ifdef::upstream[]
For more information, see link:{odhdocshome}/working-with-llama-stack/deploying-a-rag-stack-in-a-project_rag#deploying-a-llama-model-with-kserve_rag[Deploying a Llama model with KServe].
endif::[]
ifndef::upstream[]
For more information, see link:{rhoaidocshome}{default-format-url}/working_with_llama_stack/deploying-a-rag-stack-in-a-project_rag#Deploying-a-llama-model-with-kserve_rag[Deploying a Llama model with KServe].
endif::[]

* You have created a project.

.Procedure
. Log in to your OpenShift cluster if you are not already logged in:
+
[source,terminal]
----
oc login <openshift_cluster_url>
----

. Switch to your project:
+
[source,terminal]
----
oc project <project_name>
----

. Create a `LlamaStackDistribution` that enables the Ragas inline provider by setting the required environment variables. For example, create a file named `llama-stack-ragas-inline.yaml`:
+
.Example `LlamaStackDistribution` with Ragas inline provider
[source,yaml]
----
apiVersion: llamastack.io/v1alpha1
kind: LlamaStackDistribution
metadata:
  name: llama-stack-ragas-inline
  namespace: <project_name>
spec:
  replicas: 1
  server:
    containerSpec:
      env:
        - name: INFERENCE_MODEL
          value: <model_name>
        - name: VLLM_URL
          value: <model_url>
        - name: VLLM_API_TOKEN
          value: <model_api_token>
        - name: VLLM_TLS_VERIFY
          value: "false"

        # Ragas inline configuration
        - name: ENABLE_RAGAS
          value: "true"
        - name: EMBEDDING_MODEL
          value: granite-embedding-125m
      name: llama-stack
      port: 8321
    distribution:
      name: rh-dev
----

. Deploy the Llama Stack distribution:
+
[source,terminal]
----
oc apply -f llama-stack-ragas-inline.yaml
----

. Monitor the deployment until the pod is running:
+
[source,terminal]
----
oc get pods -w
----

.Verification
* The `llama-stack-ragas-inline` pod reaches the `Running` state.
* The pod logs show the Llama Stack server starting successfully with Ragas enabled.
