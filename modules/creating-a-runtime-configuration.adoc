:_module-type: PROCEDURE

[id="creating-a-runtime-configuration_{context}"]
= Creating a runtime configuration

[role='_abstract']
If you create a workbench as part of a project, a default runtime configuration is created automatically. However, if you create a workbench from the *Start basic workbench* tile in the {productname-short} dashboard, you must create a runtime configuration before you can run your pipeline in JupyterLab. This enables you to specify connectivity information for your pipeline instance and S3-compatible cloud storage.

.Prerequisites
* You have logged in to {productname-long}.
* You have access to S3-compatible cloud storage.
* You have created a project that contains a workbench.
* You have created and configured a pipeline server within the project that contains your workbench.
* You have created and launched a workbench from a workbench image that contains the Elyra extension (Standard Data Science, TensorFlow, TrustyAI, ROCm-PyTorch, ROCm-TensorFlow, or PyTorch).
+
[IMPORTANT]
====
The Elyra pipeline editor is available in specific workbench images only. To use Elyra, the workbench must be based on a JupyterLab image. The Elyra extension is not available in code-server or RStudio IDEs. The workbench must also be derived from the Standard Data Science image. It is not available in Minimal Python or CUDA-based workbenches. All other supported JupyterLab-based workbench images have access to the Elyra extension.
====

.Procedure
. In the left sidebar of JupyterLab, click *Runtimes* (image:images/jupyter-runtimes-sidebar.png[The Runtimes icon]).
. Click the *Create new runtime configuration* button (image:images/jupyter-create-runtime.png[Create new runtime configuration]).
+
The *Add new Pipelines runtime configuration* page opens.
. Complete the relevant fields to define your runtime configuration.
.. In the *Display Name* field, enter a name for your runtime configuration.
.. Optional: In the *Description* field, enter a description to define your runtime configuration.
.. Optional: In the *Tags* field, click *Add Tag* to define a category for your pipeline instance. Enter a name for the tag and press Enter.
.. Define the credentials of your pipeline:
... In the *API endpoint* field, enter the API endpoint of your pipeline. Do not specify the pipelines namespace in this field.
+
[IMPORTANT]
====
With Elyra, you can now use a service-based URL instead of the route-based URL by including the port number. Using the service-based URL allows your pipeline to access the service directly.
====
//+
//[IMPORTANT]
//====
//To obtain the pipeline API endpoint, x.
//====
... In the *Public API endpoint* field, enter the public API endpoint of your pipeline.
+
[IMPORTANT]
====
You can obtain the pipeline API endpoint from the *Develop & train* -> *Pipelines* -> *Runs* page in the dashboard. Copy the relevant endpoint and enter it in the *Public API endpoint* field.
====
... Optional: In the *User namespace* field, enter the relevant user namespace to run pipelines.
... From the *Authentication Type* list, select the authentication type required to authenticate your pipeline.
+
[IMPORTANT]
====
If you created a workbench directly from the *Start basic workbench* tile on the dashboard, select `EXISTING_BEARER_TOKEN` from the *Authentication Type* list.
====
... In the *API endpoint username* field, enter the user name required for the authentication type.
... In the *API endpoint password or token*, enter the password or token required for the authentication type.
+
[IMPORTANT]
====
To obtain the pipeline API endpoint token, in the upper-right corner of the {openshift-platform} web console, click your user name and select *Copy login command*. After you have logged in, click *Display token* and copy the value of `--token=` from the *Log in with this token* command.
====
.. Define the connectivity information of your S3-compatible storage:
... In the *Cloud Object Storage Endpoint* field, enter the endpoint of your S3-compatible storage. For more information about Amazon s3 endpoints, see link:https://docs.aws.amazon.com/general/latest/gr/s3.html[Amazon Simple Storage Service endpoints and quotas].
... Optional: In the *Public Cloud Object Storage Endpoint* field, enter the URL of your S3-compatible storage.
... In the *Cloud Object Storage Bucket Name* field, enter the name of the bucket where your pipeline artifacts are stored. If the bucket name does not exist, it is created automatically.
... From the *Cloud Object Storage Authentication Type* list, select the authentication type required to access to your S3-compatible cloud storage. If you use AWS S3 buckets, select `KUBERNETES_SECRET` from the list.
... In the *Cloud Object Storage Credentials Secret* field, enter the secret that contains the storage user name and password. This secret is defined in the relevant user namespace, if applicable. In addition, it must be stored on the cluster that hosts your pipeline runtime.
... In the *Cloud Object Storage Username* field, enter the user name to connect to your S3-compatible cloud storage, if applicable. If you use AWS S3 buckets, enter your AWS Secret Access Key ID.
... In the *Cloud Object Storage Password* field, enter the password to connect to your S3-compatible cloud storage, if applicable. If you use AWS S3 buckets, enter your AWS Secret Access Key.
.. Click *Save & Close*.

.Verification
* The runtime configuration that you created is displayed on the *Runtimes* tab (image:images/jupyter-runtimes-sidebar.png[The Runtimes icon]) in the left sidebar of JupyterLab.

//[role='_additional-resources']
//.Additional resources//