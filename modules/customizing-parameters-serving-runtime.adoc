:_module-type: PROCEDURE

[id="customizing-parameters-serving-runtime_{context}"]
= Customizing the parameters of a deployed model-serving runtime

[role='_abstract']
You might need additional parameters beyond the default ones to deploy specific models or to enhance an existing model deployment. In such cases, you can modify the parameters of an existing runtime to suit your deployment needs.

NOTE: Customizing the parameters of a runtime only affects the selected model deployment.

.Prerequisites
* You have logged in to {productname-short} as a user with {productname-short} administrator privileges.
* You have deployed a model.

.Procedure
. From the {productname-short} dashboard, click *AI hub* -> *Deployments*. 
+
The *Deployments* page opens.
. Click *Stop* next to the name of the model you want to customize. 
. Click the action menu (â‹®) and select *Edit*.
+
The *Configuration parameters* section shows predefined serving runtime parameters, if any are available.
. Customize the runtime parameters in the *Configuration parameters* section:
.. Modify the values in *Additional serving runtime arguments* to define how the deployed model behaves.
.. Modify the values in *Additional environment variables* to define variables in the model's environment.
+
NOTE: Do not modify the port or model serving runtime arguments, because they require specific values to be set. Overwriting these parameters can cause the deployment to fail.
. After you are done customizing the runtime parameters, click *Redeploy* to save.
. Click *Start* to deploy the model with your changes.

.Verification
* Confirm that the deployed model is shown on the *Deployments* tab for the project, and on the *Deployments* page of the dashboard with a checkmark in the *Status* column.
* Confirm that the arguments and variables that you set appear in `spec.predictor.model.args` and `spec.predictor.model.env` by one of the following methods:
** Checking the InferenceService YAML from the {openshift-platform} Console.
** Using the following command in the {openshift-platform} CLI:
+
[source]
----
oc get -o json inferenceservice <inferenceservicename/modelname> -n <projectname>
----
+

// .Additional resources
// <Link to reference with info on parameters that can be customized>
