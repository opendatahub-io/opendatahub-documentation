:_module-type: PROCEDURE

[id='configuring-a-model-server-for-your-data-science-project_{context}']
= Configuring a model server for your data science project

[role='_abstract']
Before you can successfully deploy a data science model on {productname-short}, you must configure a model server. This includes configuring the number of replicas being deployed, the server size, the token authorization, and how the project is accessed.

.Prerequisites
* You have logged in to {productname-long}.
ifndef::upstream[]
* If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, `rhods-users` or `rhods-admin` ) in OpenShift.
endif::[]
ifdef::upstream[]
* If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, `odh-users` or `odh-admins`) in OpenShift.
endif::[]
* You have created a data science project that you can add a model server to.
ifndef::upstream[]
* If you want to use a custom model-serving runtime for your model server, you have added and enabled the runtime. See link:{rhodsdocshome}{default-format-url}/working_on_data_science_projects/working-on-data-science-projects_nb-server#adding-a-custom-model-serving-runtime_nb-server[Adding a custom model-serving runtime].
* If you want to use graphics processing units (GPUs) with your model server, you have enabled GPU support in {productname-short}. See {rhodsdocshome}{default-format-url}/managing_users_and_user_resources/enabling-gpu-support-in-openshift-data-science_user-mgmt[Enabling GPU support in {productname-short}]
endif::[]
ifdef::upstream[]
* If you want to use a custom model-serving runtime for your model server, you have added and enabled the runtime. See link:{odhdocshome}/working-on-data-science-projects/#adding-a-custom-model-serving-runtime_nb-server[Adding a custom model-serving runtime].
* If you want to use graphics processing units (GPUs) with your model server, you have enabled GPU support. This includes installing the Node Feature Discovery and GPU Operators. For more information, see https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/openshift/contents.html[NVIDIA GPU Operator on OpenShift] in the NVIDIA documentation.
endif::[]

.Procedure
. From the {productname-short} dashboard, click *Data Science Projects*.
+
The *Data science projects* page opens.
. Click the name of the project that you want to configure a model server for.
+
A project details page opens.
. In the *Models and model servers* section, click *Configure server*.
+
The *Configure model server* dialog opens.
. In the *Model sever name* field, enter a unique name for the model server.
. From the *Serving runtime* list, select a model-serving runtime that is installed and enabled in your {productname-short} deployment.
. In the *Number of model replicas to deploy* field, specify a value.
. From the *Model server size* list, select one of the following server sizes:
* Small
* Medium
* Large
* Custom
. Optional: If you selected *Custom* in the preceding step, configure the following settings in the *Model server size* section to customize your model server:
.. In the *CPUs requested* field, specify a number of CPUs to use with your model server. Use the list beside this field to specify the value in cores or millicores.
.. In the *CPU limit* field, specify the maximum number of CPUs to use with your model server. Use the list beside this field to specify the value in cores or millicores.
.. In the *Memory requested* field, specify the requested memory for the model server in gibibytes (Gi).
.. In the *Memory limit* field, specify the maximum memory limit for the model server in gibibytes (Gi).
. Optional: In the *Model server GPUs* field, specify a number of GPUs to use with your model server.
+
[IMPORTANT]
====
{productname-short} includes two versions of the OpenVINO Model Server (OVMS) runtime by default; a version that supports GPUs and one that does not. To use GPUs, from the *Serving runtime* list, you must select the version whose display name includes `Supports GPUs`.

If you are using a _custom_ model-serving runtime with your model server, you must ensure that your custom runtime supports GPUs and is appropriately configured to use them.
====

. Optional: In the *Model route* section, select the *Make deployed models available through an external route* check box to make your deployed models available to external clients.
. Optional: In the *Token authorization* section, select the *Require token authentication* check box to require token authentication for your model server. To finish configuring token authentication, perform the following actions:
.. In the *Service account name* field, enter a service account name for which the token will be generated. The generated token is created and displayed in the *Token secret* field when the model server is configured.
.. To add an additional service account, click *Add a service account* and enter another service account name.
. Click *Configure*.

.Verification
* The model server that you configured is displayed in the *Models and model servers* section of the project details page.

//[role="_additional-resources"]
//.Additional resources
