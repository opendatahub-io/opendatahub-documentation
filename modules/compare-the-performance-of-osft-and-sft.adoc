:_mod-docs-content-type: CONCEPT

[id='compare-the-performance-of-osft-and-sft_{context}']
=  Compare the performance of OSFT, SFT, and LoRA training algorithms

You can use the Orthogonal Subspace Fine-Tuning (OSFT), Supervised Fine-Tuning (SFT), and Low-Rank Adaptation (LoRA) algorithms in Training Hub.

Use SFT to fine-tune a model on supervised datasets with support for:

* Single-node and multi-node distributed training
* Configurable training parameters, for example, epochs, batch size, and learning rate.
* InstructLab-Training backend integration

Use OSFT to fine-tune a model while controlling how much of its existing behavior to preserve, with support for:

* Single-node and multi-node distributed training
* Configurable training parameters (for example, epochs, batch size, learning rate)
* RHAI Innovation Mini-Trainer backend integration

Use LoRA for parameter-efficient fine-tuning with significantly reduced memory requirements, with support for:

* Training low-rank adaptation matrices instead of full model weights
* Unsloth backend integration
* QLoRA variant for further memory reduction (Float4)

The link:https://github.com/Red-Hat-AI-Innovation-Team/training_hub/tree/main/examples/docs[*examples/docs*] directory contains information and examples for how to use each algorithm.

Here is a performance comparison of using OSFT, SFT, and LoRA in Training Hub.

*NOTE:* When scaling the usage of Liger Kernels for all methods, some amount of fixed overhead memory is added to all methods that do not use Liger Kernels.

* *Memory scaling:* OSFT adds additional memory overhead to the model storage due to its unique matrices, roughly about 1.25-1.5x that of the normal model storage in SFT. However, the rest of OSFT memory scales linearly with the unfreeze rank ratio (URR). The URR is a hyperparameter for OSFT that is a value between 0 and 1. It represents the fraction of the matrix rank that is unfrozen and updated during fine-tuning.
+
A rough comparison is: OSFT Memory ~ 3 x _r_ x SFT Memory, where _r_ is the URR unfreeze rank ratio, the fraction of the matrix being fine-tuned. At URR = 1/3, OSFT and SFT have similar memory usage.
+
In most post-training setups, URR values below 1/3 are sufficient for learning new tasks, making OSFT notably lighter in memory.
+
Like SFT, LoRA requires a fixed amount of overhead memory to store the base model, intermediate activations, and outputs. The rest of the memory needed for LoRA scales linearly based on the LoRA rank (`lora_r`) parameter. The `lora_r` value is an integer, ideally no more than the size of any of the model's weight dimensions, that determines how many rows should be used in each of LoRA's approximated matrices.
+
You should keep the `lora_r` value as low as possible. As `lora_r` approaches 0, the memory that LoRA uses should approach 1/4 * SFT. While it is difficult to precisely compare SFT and LoRA, LoRA's memory usage should begin to reach or exceed that of SFT's if the value of `lora_r` is more than 3/8 of the size of the hidden dimensionality. Note that the memory used by LoRA in Training Hub is further reduced by the fact that LoRA uses Float16 as its main datatype. QLoRA uses Float4 instead. Note that when using QLoRA, you must briefly place the Float16 model onto the GPU, which can bottleneck memory usage.

* *Training time:*  On datasets of equal size, OSFT typically takes about twice as long per phase. However, because OSFT does not require replay buffers from past tasks, unlike SFT, the total training time across multiple phases or tasks is lower with clear benefits as the number of tasks grows. Because OSFT supports continual learning without maintaining or reusing old data, it enables lighter, single-pass end-to-end runs.
