:_module-type: PROCEDURE

[id="sending-training-data-to-trustyai_{context}"]
= Sending training data to TrustyAI

[role='_abstract']
To use TrustyAI for bias monitoring or data drift detection, you must send training data for your model to TrustyAI.

.Prerequisites

* Your OpenShift cluster administrator added you as a user to the {openshift-platform} cluster and has installed the TrustyAI service for the project that contains the deployed models.

ifndef::upstream[]
* You authenticated the TrustyAI service as described in link:{rhoaidocshome}{default-format-url}/monitoring_data_science_models/setting-up-trustyai-for-your-project_monitor#authenticating-trustyai-service_monitor[Authenticating the TrustyAI service].
endif::[]
ifdef::upstream[]
* You authenticated the TrustyAI service as described in link:{odhdocshome}/monitoring-data-science-models/#authenticating-trustyai-service_monitor[Authenticating the TrustyAI service].
endif::[]

* You have uploaded model training data to TrustyAI.

* Your deployed model is registered with TrustyAI.

.Procedure

. Verify that the TrustyAI service has registered your deployed model:
.. In the {openshift-platform} console, go to *Workloads* â†’ *Pods*. 
.. From the project list, select the project that contains your deployed model.
.. Inspect the `InferenceService` for your deployed model. For example, run the following command:
+
[source, bash]
----
 oc describe inferenceservice my-model -n my-namespace
----
+

.. When inspecting the `InferenceService`, you should see the following field in the specification:
+
[source, bash]
----
Logger:
      # ...
      Mode:  all
      URL:   https://trustyai-service.my-namespace.svc.cluster.local
----

. Set the `TRUSTY_ROUTE` variable to the external route for the TrustyAI service pod:
+
----
TRUSTY_ROUTE=https://$(oc get route/trustyai-service --template={{.spec.host}})
----

ifdef::upstream[]
. Get the inference endpoints for the deployed model, as described in link:{odhdocshome}/deploying-models/#accessing-inference-endpoint-for-deployed-model_odh-user[Accessing the inference endpoint for a deployed model].
endif::[]

ifndef::upstream[]
. Get the inference endpoints for the deployed model, as described in link:{rhoaidocshome}{default-format-url}/deploying_models/making_inference_requests_to_deployed_models#accessing-inference-endpoint-for-deployed-model_rhoai-user[Accessing the inference endpoint for a deployed model].
endif::[]

. Send data to this endpoint. For more information, see the link:https://kserve.github.io/archive/0.8/modelserving/inference_api/#server-metadata-response-json-object[KServe v2 Inference Protocol documentation].

.Verification
Follow these steps to view cluster metrics and verify that TrustyAI is receiving data. 

. Log in to the {openshift-platform} web console.
. Switch to the *Developer* perspective.
. In the left menu, click *Observe*.
. On the *Metrics* page, click the *Select query* list and then select *Custom query*.
. In the *Expression* field, enter `trustyai_model_observations_total` and press Enter. Your model should be listed and reporting observed inferences.
. Optional: Select a time range from the list above the graph. For example, select *5m*.

////
. In the {openshift-cli}, get the route to the TrustyAI service: 
+
[source]
----
TRUSTY_ROUTE=https://$(oc get route/trustyai-service --template={{.spec.host}})
----

. Query the `/info` endpoint: 
[source]
----
curl -H "Authorization: Bearer $TOKEN" $TRUSTY_ROUTE/info | jq ".[0].data"
----
+
This outputs a JSON file containing the following information for each model:
* The names, data types, and positions of fields in the input and output.
* The observed values that these fields take.
* The total number of input-output pairs observed.
////