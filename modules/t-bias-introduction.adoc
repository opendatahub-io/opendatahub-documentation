:_module-type: CONCEPT

[id="t-bias-introduction_{context}"]
= Introduction

Ensuring that your machine learning models are fair and unbiased is essential for building trust with your users. Although you can assess fairness during model training, it is only in deployment that your models use real-world data. Even if your models are unbiased on training data, they can exhibit dangerous biases in real-world scenarios. Therefore, it is crucial to monitor your models for fairness during their real-world deployment.

In this tutorial, you learn how to monitor models for bias. You will use two example models to complete the following tasks:

* Deploy the models by using multi-model serving.
* Send training data to the models.
* Examine the metadata for the models.
* Check model fairness.
* Schedule and check fairness and identity metric requests.
* Simulate real-world data.

== About the example models

For this tutorial, your role is a DevOps engineer for a credit lending company. The company's data scientists have created two candidate neural network models to predict whether a borrower will default on a loan. Both models make predictions based on the following information from the borrower's application:

* Number of Children
* Total Income
* Number of Total Family Members
* Is Male-Identifying?
* Owns Car?
* Owns Realty?
* Is Partnered?
* Is Employed?
* Lives with Parents?
* Age (in days)
* Length of Employment (in days)

As the DevOps engineer, your task is to verify that the models are not biased against the `Is Male-Identifying?` gender field. To complete this task, you can monitor the models using the Statistical Parity Difference (SPD) metric, which reports whether there is a difference between how often male-identifying and non-male-identifying applicants are given favorable predictions (that is, they are predicted to pay off their loans). An ideal SPD metric is 0, meaning both groups are equally likely to receive a positive outcome. However, an SPD between -0.1 and 0.1 also indicates fairness, as it reflects only a +/-10% variation between the groups.