:_module-type: PROCEDURE

[id="adding-a-tested-and-verified-model-serving-runtime-for-the-multi-model-serving-platform_{context}"]
= Adding a tested and verified model-serving runtime for the multi-model serving platform

In addition to preinstalled and custom model-serving runtimes, you can also use {org-name} tested and verified model-serving runtimes such as the link:https://developer.nvidia.com/triton-inference-server[NVIDIA Triton Inference Server] to support your needs. For more information about {org-name} tested and verified runtimes, see link:https://access.redhat.com/articles/7089743[Tested and verified runtimes for {productname-long}^].

You can use the {productname-long} dashboard to add and enable the *NVIDIA Triton Inference Server* runtime and then choose the runtime when you create a new model server for the multi-model serving platform.
 
[role='_abstract']

.Prerequisites
* You have logged in to {productname-short} as a user with {productname-short} administrator privileges.
ifdef::upstream[]
* You are familiar with how to link:{odhdocshome}/deploying-models/#adding-a-model-server-for-the-multi-model-serving-platform_odh-user[add a model server to your project]. After you have added a tested and verified model-serving runtime, you must configure a new model server to use the runtime.
endif::[]
ifndef::upstream[]
* You are familiar with how to link:{rhoaidocshome}{default-format-url}/deploying_models/deploying-models_odh-user#adding-a-model-server-for-the-multi-model-serving-platform_rhoai-user[add a model server to your project]. After you have added a tested and verified model-serving runtime, you must configure a new model server to use the runtime.
endif::[]

.Procedure
. From the {productname-short} dashboard, click *Settings* -> *Serving runtimes*.
+
The *Serving runtimes* page opens and shows the model-serving runtimes that are already installed and enabled.

. To add a tested and verified runtime, click *Add serving runtime*.
. In the *Select the model serving platforms this runtime supports* list, select *Multi-model serving platform*.
+
NOTE: The multi-model serving platform supports only the REST protocol. Therefore, you cannot change the default value in the *Select the API protocol this runtime supports* list.
. Click *Start from scratch*.
. Enter or paste the following YAML code directly in the embedded editor.
+
[source]
----
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  annotations:
    enable-route: "true"
  name: modelmesh-triton
  labels:
    opendatahub.io/dashboard: "true"
spec:
  annotations:
    opendatahub.io/modelServingSupport: '["multi"x`x`]'
    prometheus.kserve.io/path: /metrics
    prometheus.kserve.io/port: "8002"
  builtInAdapter:
    env:
      - name: CONTAINER_MEM_REQ_BYTES
        value: "268435456"
      - name: USE_EMBEDDED_PULLER
        value: "true"
    memBufferBytes: 134217728
    modelLoadingTimeoutMillis: 90000
    runtimeManagementPort: 8001
    serverType: triton
  containers:
    - args:
        - -c
        - 'mkdir -p /models/_triton_models;  chmod 777
          /models/_triton_models;  exec
          tritonserver "--model-repository=/models/_triton_models" "--model-control-mode=explicit" "--strict-model-config=false" "--strict-readiness=false" "--allow-http=true" "--allow-grpc=true"  '
      command:
        - /bin/sh
      image: nvcr.io/nvidia/tritonserver@sha256:xxxxx
      name: triton
      resources:
        limits:
          cpu: "1"
          memory: 2Gi
        requests:
          cpu: "1"
          memory: 2Gi
  grpcDataEndpoint: port:8001
  grpcEndpoint: port:8085
  multiModel: true
  protocolVersions:
    - grpc-v2
    - v2
  supportedModelFormats:
    - autoSelect: true
      name: onnx
      version: "1"
    - autoSelect: true
      name: pytorch
      version: "1"
    - autoSelect: true
      name: tensorflow
      version: "1"
    - autoSelect: true
      name: tensorflow
      version: "2"
    - autoSelect: true
      name: tensorrt
      version: "7"
    - autoSelect: false
      name: xgboost
      version: "1"
    - autoSelect: true
      name: python
      version: "1"
----
. In the `metadata.name` field, make sure that the value of the runtime you are adding does not match a runtime that you have already added).

. Optional: To use a custom display name for the runtime that you are adding, add a `metadata.annotations.openshift.io/display-name` field and specify a value, as shown in the following example:
+
[source]
----
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: modelmesh-triton
  annotations:
    openshift.io/display-name: Triton ServingRuntime
----
+
NOTE: If you do not configure a custom display name for your runtime, {productname-short} shows the value of the `metadata.name` field.

. Click *Create*.
+
The *Serving runtimes* page opens and shows the updated list of runtimes that are installed. Observe that the runtime you added is automatically enabled.

. Optional: To edit the runtime, click the action menu (&#8942;) and select *Edit*.

.Verification
* The model-serving runtime that you added is shown in an enabled state on the *Serving runtimes* page.

[role='_additional-resources']
.Additional resources
ifndef::upstream[]
* To learn how to configure a model server that uses a model-serving runtime that you have added, see link:{rhoaidocshome}{default-format-url}/deploying_models/deploying-models_rhoai-user#adding-a-model-server-for-the-multi-model-serving-platform_rhoai-user[Adding a model server to your data science project].
endif::[]
ifdef::upstream[]
* To learn how to configure a model server that uses a model-serving runtime that you have added, see link:{odhdocshome}/deploying-models/#adding-a-model-server-for-the-multi-model-serving-platform_odh-user[Adding a model server to your data science project].
endif::[]
