:_module-type: CONCEPT

[id="about-the-single-model-serving-platform_{context}"]
= About the single-model serving platform

[role="_abstract"]
The single-model serving platform consists of the following components:

* link:https://github.com/opendatahub-io/kserve[KServe^]: A Kubernetes custom resource definition (CRD) that orchestrates model serving for all types of models. It includes model-serving runtimes that implement the loading of given types of model servers. KServe handles the lifecycle of the deployment object, storage access, and networking setup.

* link:https://docs.openshift.com/serverless/{os-latest-version}/about/about-serverless.html[{org-name} OpenShift Serverless^]: A cloud-native development model that allows for serverless deployments of models. OpenShift Serverless is based on the open source link:https://knative.dev/docs/[Knative^] project.

ifdef::self-managed[]
* link:https://docs.openshift.com/container-platform/{ocp-latest-version}/service_mesh/v2x/ossm-architecture.html[{org-name} OpenShift Service Mesh^]: A service mesh networking layer that manages traffic flows and enforces access policies. OpenShift Service Mesh is based on the open source link:https://istio.io/[Istio^] project.
endif::[]

ifdef::cloud-service[]
* link:https://docs.openshift.com/rosa/service_mesh/v2x/ossm-architecture.html[{org-name} OpenShift Service Mesh^]: Service mesh networking layer that manages traffic flows and enforces access policies. OpenShift Service Mesh is based on the open source link:https://istio.io/[Istio^] project.
endif::[]

To install the single-model serving platform, you have the following options:

Automated installation:: If you have not already created a `ServiceMeshControlPlane` or `KNativeServing` resource on your OpenShift cluster, you can configure the {productname-long} Operator to install KServe and its dependencies.

Manual installation:: If you have already created a `ServiceMeshControlPlane` or `KNativeServing` resource on your OpenShift cluster, you _cannot_ configure the {productname-long} Operator to install KServe and its dependencies. In this situation, you must install KServe manually.

When you have installed KServe, you can use the {productname-short} dashboard to deploy models using pre-installed or custom model-serving runtimes. 

{productname-short} includes the following pre-installed runtimes for KServe:

* *TGIS Standalone ServingRuntime for KServe*: A runtime for serving TGI-enabled models
* *Caikit-TGIS ServingRuntime for KServe*: A composite runtime for serving models in the Caikit format
* *Caikit Standalone ServingRuntime for KServe*: A runtime for serving models in the Caikit Embeddings format for embeddings tasks
* *OpenVINO Model Server*: A scalable, high-performance runtime for serving models that are optimized for Intel architectures
* *vLLM ServingRuntime for KServe*: A high-throughput and memory-efficient inference and serving runtime for large language models
 
ifdef::upstream[]
[NOTE]
==== 
* link:https://github.com/IBM/text-generation-inference[Text Generation Inference Server (TGIS)^] is based on an early fork of link:https://github.com/huggingface/text-generation-inference[Hugging Face TGI^]. Red Hat will continue to develop the standalone TGIS runtime to support TGI models. If a model does not work in the current version of {productname-short}, support might be added in a future version. In the meantime, you can also add your own, custom runtime to support a TGI model. For more information, see link:{odhdocshome}/serving-models/#adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models[Adding a custom model-serving runtime for the single-model serving platform].

* The composite Caikit-TGIS runtime is based on link:https://github.com/opendatahub-io/caikit[Caikit^] and link:https://github.com/IBM/text-generation-inference[Text Generation Inference Server (TGIS)^]. To use this runtime, you must convert your models to Caikit format. For an example, see link:https://github.com/opendatahub-io/caikit-tgis-serving/blob/main/demo/kserve/built-tip.md#bootstrap-process[Converting Hugging Face Hub models to Caikit format^] in the link:https://github.com/opendatahub-io/caikit-tgis-serving/tree/main[caikit-tgis-serving^] repository.
====
endif::[]

ifndef::upstream[]
[NOTE]
==== 
* link:https://github.com/IBM/text-generation-inference[Text Generation Inference Server (TGIS)^] is based on an early fork of link:https://github.com/huggingface/text-generation-inference[Hugging Face TGI^]. Red Hat will continue to develop the standalone TGIS runtime to support TGI models. If a model does not work in the current version of {productname-short}, support might be added in a future version. In the meantime, you can also add your own, custom runtime to support a TGI model. For more information, see link:{rhoaidocshome}{default-format-url}/serving_models/serving-large-models_serving-large-models#adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models[Adding a custom model-serving runtime for the single-model serving platform].

* The composite Caikit-TGIS runtime is based on link:https://github.com/opendatahub-io/caikit[Caikit^] and link:https://github.com/IBM/text-generation-inference[Text Generation Inference Server (TGIS)^]. To use this runtime, you must convert your models to Caikit format. For an example, see link:https://github.com/opendatahub-io/caikit-tgis-serving/blob/main/demo/kserve/built-tip.md#bootstrap-process[Converting Hugging Face Hub models to Caikit format^] in the link:https://github.com/opendatahub-io/caikit-tgis-serving/tree/main[caikit-tgis-serving^] repository.
====
endif::[]

You can also configure monitoring for the single-model serving platform and use Prometheus to scrape the available metrics.

// [role="_additional-resources"]
// .Additional resources
