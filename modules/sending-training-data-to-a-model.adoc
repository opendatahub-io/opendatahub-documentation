:_module-type: PROCEDURE

[id="sending-training-data-to-a-model_{context}"]
= Sending training data to a model

[role='_abstract']
You must send training data through your model so that TrustyAI can compute baseline fairness values.

.Prerequisites

* Your OpenShift cluster administrator has installed {productname-short} and enabled the TrustyAI service for the data science project where the model is deployed.

* You have logged in to {productname-long}.

.Procedure
ifdef::upstream[]
. Get the inference endpoints for the deployed model, as described in link:{odhdocshome}{default-format-url}/serving-models/serving-small-and-medium-sized-models_model-serving#viewing-a-deployed-model_model-serving[Viewing a deployed model].
endif::[]

ifndef::upstream[]
. Get the inference endpoints for the deployed model, as described in link:{rhoaidocshome}{default-format-url}/serving_models/serving-small-and-medium-sized-models_model-serving#viewing-a-deployed-model_model-serving[Viewing a deployed model].
endif::[]

. Send data to this endpoint. For more information, see the link:https://kserve.github.io/website/0.8/modelserving/inference_api/#server-metadata-response-json-object[KServe v2 Inference Protocol documentation].

.Verification
Follow these steps to view cluster metrics and verify that TrustyAI is receiving data. 

. Log in to the {openshift-platform} web console.
. Switch to the *Developer* perspective.
. In navigation menu, click *Observe* -> *Metrics*.
. On the *Metrics* page, select a value for *Time range* and for *Refresh interval*. For example, select 5 minutes for *Time range* and 15 seconds for *Refresh interval*.
. In the *Expression* field, enter `trustyai_model_observations_total` and then click *Run queries*. Your model should be listed and reporting observed inferences.

ifndef::upstream[]
.Next step
xref:assemblies/configuring-bias-metrics-for-a-model[Configuring bias metrics for a model]
endif::[]
