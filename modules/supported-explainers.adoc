:_module-type: REFERENCE
:stem: 

[id="supported-explainers_{context}"]
= Supported explainers

{productname-long} supports the following explainers:

*LIME*

_Local Interpretable Model-agnostic Explanations_ (LIME) footnote:1[Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin. "Why Should I Trust You?": Explaining the Predictions of Any Classifier." _Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data mining_, 2016. Pages 1135-1144.] is a saliency explanation method. LIME aims to explain a prediction &#119901; &#61; &#40;&#119909;, &#119910;&#41; (an input-output pair) generated by a black-box model &#119891; &#58;  &#8477;^&#119889;^ &#8594; &#8477;. The explanations come in the form of a "saliency" &#119908;~&#119894;~ attached to each feature &#119909;~&#119894;~ in the prediction &#119909;. LIME generates a local explanation &#958;&#40;&#119909;&#41; according to the following model:

image::images/explainer-lime.png[LIME model, scale=60, align="center"]

* &#120587;~&#119909;~ is a proximity function
* &#119866; is the family of interpretable models
* &#937;&#40;&#119892;&#41; is a measure of complexity of an explanation &#119892; &#8712; &#119866;
* &#119871;&#40;&#119891;, &#119892;, &#120587;~&#119909;~&#41; is a measure of how unfaithful &#119892; is in approximating &#119891; in the locality defined by &#120587;~&#119909;~ 

In the original paper, &#119866; is the class of linear models and &#120587;~&#119909;~ is an exponential kernel on a distance function &#119863; (for example, cosine distance). LIME converts samples &#119909;~&#119894;~ from the original domain into interpretable samples as binary vectors &#119909;&#8242; &#95; &#119894; &#8712; &#123;0,1&#125;. An encoded data set &#119864; is built by taking nonzero elements of &#119909;&#8242; &#95; &#119894;, recovering the original representation &#119911; &#8712; &#8477;^&#119889; and then computing &#119891;&#40;&#119911;&#41;. A weighted linear model &#119892; (with weights provided via &#120587;~&#119909;~) is then trained on the generated sparse data set &#119864; and the model weights &#119908; are used as feature weights for the final explanation &#958;&#40;&#119909;&#41;.

*SHAP*

_SHapley Additive exPlanations_ (SHAP), footnote:[Scott Lundberg, Su-In Lee. "A Unified Approach to Interpreting Model Predictions." _Advances in Neural Information Processing Systems_, 2017.] seeks to unify several common explanation methods, notably LIME footnote:1[] and DeepLIFT, footnote:[Avanti Shrikumar, Peyton Greenside, Anshul Kundaje. "Learning Important Features Through Propagating Activation Differences." _CoRR abs/1704.02685_, 2017.] under a common umbrella of additive feature attributions. These methods explain how an input &#119909; &#61; &#91;&#119909;~1~, &#119909;~2~, ..., &#119909;~&#119872;~&#93; affects the output of some model &#119891; by transforming &#119909; &#8712; &#8477;^&#119872;^ into simplified inputs &#119911;&#8242; &#8712; 0, 1^&#119872;^, such that &#119911;&#8242; &#95; &#119894; indicates the inclusion or exclusion of feature &#119894;. The simplified inputs are then passed to an explanatory model &#119892; that takes the following form:

image::images/explainer-shap.png[SHAP explanatory model, scale=60, align="center"]

In that form, each value &#120567;~&#119894;~ marks the contribution that feature &#119894; had on the output model (called the attribution), &#120567;~0~ marks the null output of the model; the model output when every feature is excluded. Therefore, this presents an easily interpretable explanation of the importance of each feature and a framework to permute the various input features to establish their collection contributions.

The final result of the algorithm are the Shapley values of each feature, which give an itemized "receipt" of all the contributing factors to the decision. For example, a SHAP explanation of a loan application might be as follows:

[%autowidth]
|===
|Feature | Shapley Value Ï†

|Null Output | 50%
|Income | +10%
|# Children | -15%
|Age | +22%
|Own Home? | -30%
|Acceptance% | 37%
|Deny | 63%
|===

From this, the applicant can see that the biggest contributor to their denial was their home ownership status, which reduced their acceptance probability by 30 percentage points. Meanwhile, their number of children was of particular benefit, increasing their probability by 22 percentage points.


