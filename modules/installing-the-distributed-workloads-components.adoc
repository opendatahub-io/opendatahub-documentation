:_module-type: PROCEDURE

[id="installing-the-distributed-workloads-components_{context}"]
= Installing the distributed workloads components

[role='_abstract']
To use the distributed workloads feature in {productname-short}, you must install several components.

.Prerequisites
ifdef::upstream,self-managed[]
* You have logged in to {openshift-platform} with the `cluster-admin` role and you can access the data science cluster.
endif::[]
ifdef::cloud-service[]
* You have logged in to OpenShift with the `cluster-admin` role and you can access the data science cluster.
endif::[]

* You have installed {productname-long}.

* You have installed the {rhbok-productname} Operator on your {openshift-platform} cluster, as described in the link:{rhbok-docs}[{rhbok-productname} documentation].


ifdef::cloud-service[]
* You have sufficient resources. In addition to the minimum {productname-short} resources described in link:{rhoaidocshome}{default-format-url}/installing_and_uninstalling_{url-productname-short}/installing-and-deploying-openshift-ai_install[Installing and deploying {productname-short}], you need 1.6 vCPU and 2 GiB memory to deploy the distributed workloads infrastructure.
endif::[]
ifdef::self-managed[]
* You have sufficient resources. In addition to the minimum {productname-short} resources described in link:{rhoaidocshome}{default-format-url}/installing_and_uninstalling_{url-productname-short}/installing-and-deploying-openshift-ai_install[Installing and deploying {productname-short}] (for disconnected environments, see link:{rhoaidocshome}{default-format-url}/installing_and_uninstalling_{url-productname-short}_in_a_disconnected_environment/deploying-openshift-ai-in-a-disconnected-environment_install[Deploying {productname-short} in a disconnected environment]), you need 1.6 vCPU and 2 GiB memory to deploy the distributed workloads infrastructure.
endif::[]
ifdef::upstream[]
* You have sufficient resources. In addition to the minimum {productname-short} resources described in link:{odhdocshome}/installing-open-data-hub/#installing-the-odh-operator-v2_installv2[Installing the {productname-short} Operator version 2], you need 1.6 vCPU and 2 GiB memory to deploy the distributed workloads infrastructure.
endif::[]

ifndef::upstream[]
* You have removed any previously installed instances of the CodeFlare Operator, as described in the Knowledgebase solution link:https://access.redhat.com/solutions/7043796[How to migrate from a separately installed CodeFlare Operator in your data science cluster].
endif::[]
ifdef::upstream[]
* You have removed any previously installed instances of the CodeFlare Operator.
endif::[]

ifndef::upstream[]
* If you want to use graphics processing units (GPUs), you have enabled GPU support in {productname-short}.
If you use NVIDIA GPUs, see link:{rhoaidocshome}{default-format-url}/managing_openshift_ai/enabling-accelerators#enabling-nvidia-gpus_managing-rhoai[Enabling NVIDIA GPUs].
If you use AMD GPUs, see link:{rhoaidocshome}{default-format-url}/managing_openshift_ai/enabling-accelerators#amd-gpu-integration_managing-rhoai[AMD GPU integration^].
+
ifdef::self-managed[]
[NOTE]
====
In {productname-short}, {org-name} supports the use of accelerators within the same cluster only. 

Starting from {productname-long} 2.19, {org-name} supports remote direct memory access (RDMA) for NVIDIA GPUs only, enabling them to communicate directly with each other by using NVIDIA GPUDirect RDMA across either Ethernet or InfiniBand networks.
====
endif::[]
ifdef::cloud-service[]
[NOTE]
====
In {productname-short}, {org-name} supports the use of accelerators within the same cluster only. 

{org-name} supports remote direct memory access (RDMA) for NVIDIA GPUs only, enabling them to communicate directly with each other by using NVIDIA GPUDirect RDMA across either Ethernet or InfiniBand networks.
====
endif::[]
endif::[]
ifdef::upstream[]
* If you want to use graphics processing units (GPUs), you have enabled GPU support.
This process includes installing the Node Feature Discovery Operator and the relevant GPU Operator.
For more information, see link:https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/index.html[NVIDIA GPU Operator on {org-name} OpenShift Container Platform^] in the NVIDIA documentation for NVIDIA GPUs and link:https://instinct.docs.amd.com/projects/gpu-operator/en/latest/installation/openshift-olm.html[AMD GPU Operator on {org-name} OpenShift Container Platform^] in the AMD documentation for AMD GPUs.
endif::[]

ifdef::cloud-service[]
* If you want to use self-signed certificates, you have added them to a central Certificate Authority (CA) bundle as described in link:{rhoaidocshome}{default-format-url}/installing_and_uninstalling_{url-productname-short}/working-with-certificates_certs[Working with certificates].
No additional configuration is necessary to use those certificates with distributed workloads.
The centrally configured self-signed certificates are automatically available in the workload pods at the following mount points:
** Cluster-wide CA bundle:
+
[source,bash]
----
/etc/pki/tls/certs/odh-trusted-ca-bundle.crt
/etc/ssl/certs/odh-trusted-ca-bundle.crt
----
** Custom CA bundle:
+
[source,bash]
----
/etc/pki/tls/certs/odh-ca-bundle.crt
/etc/ssl/certs/odh-ca-bundle.crt
----
endif::[]
ifdef::self-managed[]
* If you want to use self-signed certificates, you have added them to a central Certificate Authority (CA) bundle as described in link:{rhoaidocshome}{default-format-url}/installing_and_uninstalling_{url-productname-short}/working-with-certificates_certs[Working with certificates] (for disconnected environments, see link:{rhoaidocshome}{default-format-url}/installing_and_uninstalling_{url-productname-short}_in_a_disconnected_environment/working-with-certificates_certs[Working with certificates]).
No additional configuration is necessary to use those certificates with distributed workloads.
The centrally configured self-signed certificates are automatically available in the workload pods at the following mount points:
** Cluster-wide CA bundle:
+
[source,bash]
----
/etc/pki/tls/certs/odh-trusted-ca-bundle.crt
/etc/ssl/certs/odh-trusted-ca-bundle.crt
----
** Custom CA bundle:
+
[source,bash]
----
/etc/pki/tls/certs/odh-ca-bundle.crt
/etc/ssl/certs/odh-ca-bundle.crt
----
endif::[]
ifdef::upstream[]
* If you want to use self-signed certificates, you have added them to a central Certificate Authority (CA) bundle as described in link:{odhdocshome}/installing-open-data-hub/#understanding-certificates_certs[Understanding how {productname-short} handles certificates].
No additional configuration is necessary to use those certificates with distributed workloads.
The centrally configured self-signed certificates are automatically available in the workload pods at the following mount points:
** Cluster-wide CA bundle:
+
[source,bash]
----
/etc/pki/tls/certs/odh-trusted-ca-bundle.crt
/etc/ssl/certs/odh-trusted-ca-bundle.crt
----
** Custom CA bundle:
+
[source,bash]
----
/etc/pki/tls/certs/odh-ca-bundle.crt
/etc/ssl/certs/odh-ca-bundle.crt
----
endif::[]

.Procedure
ifdef::upstream,self-managed[]
. In the {openshift-platform} console, click *Operators* -> *Installed Operators*.
endif::[]
ifdef::cloud-service[]
. In the OpenShift console, click *Operators* -> *Installed Operators*.
endif::[]

ifdef::self-managed,cloud-service[]
. Search for the *Red Hat OpenShift AI* Operator, and then click the Operator name to open the Operator details page.
endif::[]
ifdef::upstream[]
. Search for the *Open Data Hub Operator*, and then click the Operator name to open the Operator details page.
endif::[]

. Click the *Data Science Cluster* tab.
. Click the default instance name (for example, *default-dsc*) to open the instance details page.
. Click the *YAML* tab to show the instance specifications.
. Enable the required distributed workloads components.
In the `spec.components` section, set the `managementState` field correctly for the required components:
+
* Set `kueue` to `Unmanaged` to allow the {rhbok-productname} Operator to manage Kueue.
* If you want to use the CodeFlare framework to tune models, set `codeflare` and `ray` to `Managed`.
* If you want to use the Kubeflow Training Operator to tune models, set `trainingoperator` to `Managed`.
* The list of required components depends on whether the distributed workload is run from a pipeline or workbench or both, as shown in the following table.

+

.Components required for distributed workloads
[cols="44,15,15,26"]
|===
|Component | Pipelines only | Workbenches only | Pipelines and workbenches

|`codeflare`
|`Managed`
|`Managed`
|`Managed`

|`dashboard`
|`Managed`
|`Managed`
|`Managed`

|`datasciencepipelines`
|`Managed`
|`Removed`
|`Managed`

|`kueue`
|`Unmanaged`
|`Unmanaged`
|`Unmanaged`

|`ray`
|`Managed`
|`Managed`
|`Managed`

|`trainingoperator`
|`Managed`
|`Managed`
|`Managed`

|`workbenches`
|`Removed`
|`Managed`
|`Managed`
|===

. Click *Save*.
After a short time, the components with a `Managed` state are ready.


.Verification
Check the status of the *codeflare-operator-manager*, *kubeflow-training-operator*, *kuberay-operator*, *kueue-controller-manager*, and *openshift-kueue-operator* pods, as follows:

. In the {openshift-platform} console, click *Workloads* -> *Deployments*.
. In the *Search by name* field, enter the following search strings:
+
* In the *pass:attributes[{dbd-config-default-namespace}]* project, search for *codeflare-operator-manager*, *kubeflow-training-operator*, and *kuberay-operator*.
* In the *openshift-kueue-operator* project, search for *kueue-controller-manager* and *openshift-kueue-operator*.

. In each case, check the status as follows:
.. Click the deployment name to open the deployment details page.
.. Click the *Pods* tab.
.. Check the pod status.
+
When the status of the pods is *Running*, the pods are ready to use.
.. To see more information about each pod, click the pod name to open the pod details page, and then click the *Logs* tab.

.Next Step
ifdef::upstream[]
Configure the distributed workloads feature as described in link:{odhdocshome}/managing-odh/#managing-distributed-workloads_managing-odh[Managing distributed workloads].
endif::[]
ifndef::upstream[]
Configure the distributed workloads feature as described in link:{rhoaidocshome}{default-format-url}/managing_openshift_ai/managing-distributed-workloads_managing-rhoai[Managing distributed workloads].
endif::[]