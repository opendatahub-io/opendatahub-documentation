:_module-type: PROCEDURE

[id="running-distributed-data-science-workloads-disconnected-env_{context}"]
= Running distributed data science workloads in a disconnected environment

[role='_abstract']
To run a distributed data science workload in a disconnected environment, you must be able to access a Ray cluster image, and the data sets and Python dependencies used by the workload, from the disconnected environment.

.Prerequisites
* You have logged in to {openshift-platform} with the `cluster-admin` role.
* You have access to the disconnected data science cluster.
* You have installed {productname-long} and created a mirror image as described in link:{rhoaidocshome}{default-format-url}/installing_and_uninstalling_openshift_ai_self-managed_in_a_disconnected_environment[Installing and uninstalling {productname-short} Self-Managed in a disconnected environment].
* You can access the following software from the disconnected cluster:
** A Ray cluster image
** An image that includes the `openssl` package, for the creation of TLS certificates when creating Ray clusters
** The data sets and models to be used by the workload
** The Python dependencies for the workload, either in a Ray image or in your own Python Package Index (PyPI) server that is available from the disconnected cluster
* You have logged in to {productname-long}.

ifndef::upstream[]
* You have created a data science project that contains a workbench, and the workbench is running a default notebook image that contains the CodeFlare SDK, for example, the *Standard Data Science* notebook. For information about how to create a project, see link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects/using-data-science-projects_projects#creating-a-data-science-project_projects[Creating a data science project].
endif::[]
ifdef::upstream[]
* You have created a data science project that contains a workbench, and the workbench is running a default notebook image that contains the CodeFlare SDK, for example, the *Standard Data Science* notebook. For information about how to create a project, see link:{odhdocshome}/working-on-data-science-projects/#_using_data_science_projects[Creating a data science project].
endif::[]

* You have Admin access for the data science project.
** If you created the project, you automatically have Admin access. 
** If you did not create the project, your cluster administrator must give you Admin access.


.Procedure
. Configure the disconnected data science cluster to run distributed workloads as described in link:{rhoaidocshome}{default-format-url}/working_with_distributed_workloads/configuring-distributed-workloads_distributed-workloads[Configuring distributed workloads].
. In the `ClusterConfiguration` section of the notebook or pipeline, ensure that the `image` value specifies a Ray cluster image that you can access from the disconnected environment:
* Notebooks use the Ray cluster image to create a Ray cluster when running the notebook.
* Pipelines use the Ray cluster image to create a Ray cluster during the pipeline run.
. In the CodeFlare Operator config map, ensure that the `kuberay:certGeneratorImage` value specifies an image that contains the `openssl` package, and that you can access the image from the disconnected environment.
The following example shows the default value provided by {productname-short}:
+
[source,bash]
----
kind: ConfigMap
apiVersion: v1
metadata:
  name: codeflare-operator-config
  namespace: redhat-ods-applications
  data:
  config.yaml: |
    kuberay:
      certGeneratorImage: "registry.redhat.io/ubi9@sha256:770cf07083e1c85ae69c25181a205b7cdef63c11b794c89b3b487d4670b4c328"
----
. If any of the Python packages required by the workload are not available in the Ray cluster, configure the Ray cluster to download the Python packages from a private PyPI server.
+
For example, set the `PIP_INDEX_URL` and `PIP_TRUSTED_HOST` environment variables for the Ray cluster, to specify the location of the Python dependencies, as shown in the following example:
+
----
PIP_INDEX_URL: https://pypi-notebook.apps.mylocation.com/simple
PIP_TRUSTED_HOST: pypi-notebook.apps.mylocation.com
----
where
* `PIP_INDEX_URL` specifies the base URL of your private PyPI server (the default value is https://pypi.org).
* `PIP_TRUSTED_HOST` configures Python to mark the specified host as trusted, regardless of whether that host has a valid SSL certificate or is using a secure channel.
. Run the distributed data science workload, as described in link:{rhoaidocshome}{default-format-url}/working_with_distributed_workloads/running-distributed-workloads_distributed-workloads#running-distributed-data-science-workloads-from-notebooks_distributed-workloads[Running distributed data science workloads from notebooks] or link:{rhoaidocshome}{default-format-url}/working_with_distributed_workloads/running-distributed-workloads_distributed-workloads#running-distributed-data-science-workloads-from-ds-pipelines_distributed-workloads[Running distributed data science workloads from data science pipelines].

.Verification
The notebook or pipeline run completes without errors:

* For notebooks, the output from the `cluster.status()` function or `cluster.details()` function indicates that the Ray cluster is `Active`.
* For pipeline runs, you can view the run details as described in link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects/working-with-data-science-pipelines_ds-pipelines#viewing-the-details-of-a-pipeline-run_ds-pipelines[Viewing the details of a pipeline run].


[role='_additional-resources']
.Additional resources

* link:{rhoaidocshome}{default-format-url}/installing_and_uninstalling_openshift_ai_self-managed_in_a_disconnected_environment[Installing and uninstalling {productname-long} in a disconnected environment]
* link:https://docs.ray.io/en/latest/cluster/getting-started.html[Ray Clusters documentation]
