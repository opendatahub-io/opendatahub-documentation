:_module-type: CONCEPT

ifdef::context[:parent-context: {context}]
[id='guardrails-about-nemo_{context}']

= About NeMo Guardrails
You can use link:https://docs.nvidia.com/nemo/guardrails/latest/index.html[NeMo Guardrails] to apply guardrails to your large language model (LLM) in {productname-long}. With NeMo Guardrails, you can add guardrails between the application code and the LLM. The TrustyAI Service Operator integrates NeMo Guardrails so that you can safeguard your LLM-based applications on {org-name} OpenShift.

The following table compares NeMo Guardrails and the TrustyAI implementation of the link:https://github.com/foundation-model-stack/fms-guardrails-orchestrator[FMS-Guardrails Orchestrator]:

.Comparison of Guardrail Architectures
[cols="1,2,2", options="header"]
|===
|Feature |FMS Guardrails Architecture |NeMo Guardrails Architecture

|Central Component
|Guardrails Orchestrator
|NeMo Guardrails Server

|Deployment Resource
|Guardrails CR
|NeMo-Guardrails CR

|Detection Mechanism
|Built-in detectors that are external to the Orchestrator
|Custom Python functions as internal detectors (using the @action decorator) that execute within the NeMo server pod and external, built-in detectors

|Operational Flow
|Orchestrator watches and calls detector services and detection flows are fixed in the ConfigMap
|NeMo server coordinates internal logic and external calls and Colang can be used for programmable detection flow

|Shared Operator
|Managed by the TrustyAI Operator
|Managed by the TrustyAI Operator

|Namespace Location
|Deployed within Model Namespaces
|Deployed within Model Namespaces

|Inference Path
|User → Orchestrator → vLLM Model
|User → NeMo Server → vLLM Model

|Language Stack
|Rust-based (Tokio)
|Python-based (FastAPI)
|===

