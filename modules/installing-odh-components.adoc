:_module-type: PROCEDURE

[id='installing-odh-components_{context}']
= Installing Open Data Hub components

[role='_abstract']
You can use the OpenShift web console to install specific components of {productname-short} on your cluster when the {productname-short} Operator is already installed on the cluster.

.Prerequisites
* You have installed the {productname-short} Operator.
* You can log in as a user with `cluster-admin` privileges.
* If you want to use the `trustyai` component, you must configure workload monitoring as described in link:{odhdocshome}/managing-and-monitoring-models/#_monitoring_models[Monitoring models on the model serving platform].
* If you want to use the `RAG` component, your infrastructure supports GPU-enabled instance types, for example, `g4dn.xlarge` on AWS.
* If you want to use `kserve`, you must have already installed the following Operator for the component. For information about installing an Operator, see link:https://docs.redhat.com/en/documentation/openshift_container_platform/{ocp-latest-version}/html/operators/administrator-tasks#olm-adding-operators-to-a-cluster[Adding Operators to a cluster].

* If you want to use the `kueue` component to manage workloads, you must install the {rhbok-productname} Operator before activating the Kueue integration. For more information, see link:{odhdocshome}/managing-odh/#configuring-workload-management-with-kueue_kueue[Configuring workload management with Kueue].

.Required Operators for components
[cols="3"]
|===
| Component | Required Operators | Catalog

| kserve
| Red Hat Connectivity Link, Red Hat OpenShift Service Mesh Operator
| Red Hat

| kueue
| {rhbok-productname} Operator
| Red Hat

| RAG (Llama Stack)
| Llama Stack Operator, Node Feature Discovery Operator, NVIDIA GPU Operator
| Red Hat
|===


.Procedure
. Log in to your {openshift-platform} cluster as a user with `cluster-admin` privileges. If you are performing a developer installation on link:http://try.openshift.com[try.openshift.com], you can log in as the `kubeadmin` user.
. Select *Operators* -> *Installed Operators*, and then click the *{productname-short} Operator*.
. On the *Operator details* page, click the *DSC Initialization* tab, and then click *Create DSCInitialization*.
. On the *Create DSCInitialization* page, configure by using the *YAML* view. 
//For general information about the supported components, see link:https://opendatahub.io/docs/tiered-components[Tiered Components].
.. If you are using a custom applications namespace, specify the namespace in the `spec.applicationsNamespace` field.
. Click *Create*.
. Wait until the status of the DSCInitialization is *Ready*.
. Click the *Data Science Cluster* tab, and then click *Create DataScienceCluster*.
. On the *Create DataScienceCluster* page, configure by using the *YAML* view. 
//For general information about the supported components, see link:https://opendatahub.io/docs/tiered-components[Tiered Components].
.. In the `spec.components` section, for each component shown, set the value of the `managementState` field to `Managed`, `Removed`, or `Unmanaged`.
.. If you are using a custom workbench namespace, specify the namespace in the `spec.workbenches.workbenchNamespace` field.
. Click *Create*.


.Verification
. Select *Home* -> *Projects*, and then select the *opendatahub* project.
. On the *Project details* page, click the *Workloads* tab and confirm that the {productname-short} core components are running. 
//For more information, see link:https://opendatahub.io/docs/tiered-components[Tiered Components].

*Note:* In the {productname-long} dashboard, users can view the list of the installed {productname-short} components, their corresponding source (upstream) components, and the versions of the installed components, as described in link:{odhdocshome}/installing-open-data-hub/#viewing-installed-components_get-started[Viewing installed {productname-short} components].

.Next steps
* Optional: link:{odhdocshome}/installing-open-data-hub/#configuring-pipelines-with-your-own-argo-workflows-instance_install[Configuring pipelines with your own Argo Workflows instance]
* Optional: link:{odhdocshome}/installing-open-data-hub/#installing-the-distributed-workloads-components_install[Installing the distributed workloads components]
* link:{odhdocshome}/installing-open-data-hub/#accessing-the-dashboard_installv2[Accessing the {productname-short} dashboard]

[role="_additional-resources"]
.Additional resources
* link:{odhdocshome}/installing-open-data-hub/#configuring-the-operator-logger_install[Configuring the {productname-short} Operator logger]
