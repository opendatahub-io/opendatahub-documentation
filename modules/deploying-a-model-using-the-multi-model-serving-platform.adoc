:_module-type: PROCEDURE

[id='deploying-a-model-using-the-multi-model-serving-platform_{context}']
= Deploying a model by using the multi-model serving platform

[role='_abstract']
You can deploy trained models on {productname-short} to enable you to test and implement them into intelligent applications. Deploying a model makes it available as a service that you can access by using an API. This enables you to return predictions based on data inputs.

When you have enabled the multi-model serving platform, you can deploy models on the platform.

.Prerequisites
* You have logged in to {productname-long}.
ifndef::upstream[]
* If you are using {productname-short} groups, you are part of the user group or admin group (for example, {oai-user-group}) in OpenShift.
endif::[]
ifdef::upstream[]
* If you are using {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group}) in OpenShift.
endif::[]
* You have enabled the multi-model serving platform.
* You have created a data science project and added a model server.
* You have access to S3-compatible object storage.
* For the model that you want to deploy, you know the associated folder path in your S3-compatible object storage bucket.

.Procedure
. In the left menu of the {productname-short} dashboard, click *Data Science Projects*.
+
The *Data Science Projects* page opens.
. Click the name of the project that you want to deploy a model in.
+
A project details page opens.
. Click the *Models* tab.
. Click *Deploy model*.
. Configure properties for deploying your model as follows:
.. In the *Model name* field, enter a unique name for the model that you are deploying.
.. From the *Model framework* list, select a framework for your model.
+
NOTE: The *Model framework* list shows only the frameworks that are supported by the model-serving runtime that you specified when you configured your model server.
.. To specify the location of the model you want to deploy from S3-compatible object storage, perform one of the following sets of actions:
+
--
* *To use an existing connection*
... Select *Existing connection*.
... From the *Name* list, select a connection that you previously defined.
... In the *Path* field, enter the folder path that contains the model in your specified data source.

* *To use a new connection*
... To define a new connection that your model can access, select *New connection*.
... In the *Name* field, enter a unique name for the connection.
... In the *Access key* field, enter the access key ID for the S3-compatible object storage provider.
... In the *Secret key* field, enter the secret access key for the S3-compatible object storage account that you specified.
... In the *Endpoint* field, enter the endpoint of your S3-compatible object storage bucket.
... In the *Region* field, enter the default region of your S3-compatible object storage account.
... In the *Bucket* field, enter the name of your S3-compatible object storage bucket.
... In the *Path* field, enter the folder path in your S3-compatible object storage that contains your data file.
--

.. Click *Deploy*.

.Verification
* Confirm that the deployed model is shown on the *Models* tab for the project, and on the *Model Serving* page of the dashboard with a checkmark in the *Status* column.

ifdef::upstream[]
[role='_additional-resources']
.Additional resources

* To learn how to monitor your model for bias, see link:{odhdocshome}/monitoring-data-science-models[Monitoring data science models].
endif::[]
