:_module-type: PROCEDURE

[id='deploying-a-model-using-the-multi-model-serving-platform_{context}']
= Deploying a model by using the multi-model serving platform

[role='_abstract']
You can deploy trained models on {productname-short} to enable you to test and implement them into intelligent applications. Deploying a model makes it available as a service that you can access by using an API. This enables you to return predictions based on data inputs.

When you have enabled the multi-model serving platform, you can deploy models on the platform.

.Prerequisites
* You have logged in to {productname-long}.
ifndef::upstream[]
* If you are using {productname-short} groups, you are part of the user group or admin group (for example, {oai-user-group}) in OpenShift.
endif::[]
ifdef::upstream[]
* If you are using {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group}) in OpenShift.
endif::[]
* You have enabled the multi-model serving platform.
* You have created a data science project and added a model server.
* You have access to S3-compatible object storage.
* For the model that you want to deploy, you know the associated folder path in your S3-compatible object storage bucket.

.Procedure
. In the left menu of the {productname-short} dashboard, click *Data Science Projects*.
+
The *Data Science Projects* page opens.
. Click the name of the project that you want to deploy a model in.
+
A project details page opens.
. Click the *Models* tab.
. Click *Deploy model*.
. Configure properties for deploying your model as follows:
.. In the *Model name* field, enter a unique name for the model that you are deploying.
.. From the *Model framework* list, select a framework for your model.
+
NOTE: The *Model framework* list shows only the frameworks that are supported by the model-serving runtime that you specified when you configured your model server.
.. To specify the location of the model you want to deploy from S3-compatible object storage, perform one of the following sets of actions:
+
--
* *To use an existing connection*
... Select *Existing connection*.
... From the *Name* list, select a connection that you previously defined.
... In the *Path* field, enter the folder path that contains the model in your specified data source.

* *To use a new connection* 
... To define a new connection that your model can access, select *New connection*.
--
+
. In the *Add connection* modal, select a *Connection type*. The *S3 compatible object storage* and *URI* options are pre-installed connection types. Additional options might be available if your {productname-short} administrator added them.
+
The *Add connection* form opens with fields specific to the connection type that you selected.
... Fill in the connection detail fields.

.. (Optional) Customize the runtime parameters in the *Configuration parameters* section:
... Modify the values in *Additional serving runtime arguments* to define how the deployed model behaves.
... Modify the values in *Additional environment variables* to define variables in the model's environment.

.. Click *Deploy*.

.Verification
* Confirm that the deployed model is shown on the *Models* tab for the project, and on the *Model Serving* page of the dashboard with a checkmark in the *Status* column.

ifdef::upstream[]
[role='_additional-resources']
.Additional resources

* To learn how to monitor your model for bias, see link:{odhdocshome}/monitoring-data-science-models[Monitoring data science models].
endif::[]
