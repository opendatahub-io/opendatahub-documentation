:_module-type: PROCEDURE

[id="configuring-distributed-workloads_{context}"]
= Configuring distributed workloads

[role='_abstract']
To configure the distributed workloads feature for your data scientists to use in {productname-short}, you must enable several components.

.Prerequisites
ifdef::upstream,self-managed[]
* You have logged in to {openshift-platform} with the `cluster-admin` role.
endif::[]
ifdef::cloud-service[]
* You have logged in to OpenShift with the `cluster-admin` role.
endif::[]
* You have sufficient resources. In addition to the base {productname-short} resources, you need 1.1 vCPU and 1.6 GB memory to deploy the distributed workloads infrastructure.
* You have access to a Ray cluster image. For information about how to create a Ray cluster, see the link:https://docs.ray.io/en/latest/cluster/getting-started.html[Ray Clusters documentation].
+
[NOTE]
====
Mutual TLS (mTLS) is enabled by default in the CodeFlare component in {productname-short}.
In the current {productname-short} version, `submissionMode=K8sJobMode` is not supported in the Ray job specification, so the KubeRay Operator cannot create a submitter Kubernetes Job to submit the Ray job.
Instead, the Ray job specification must be configured to set  `submissionMode=HTTPMode` only, so that the KubeRay Operator sends a request to the RayCluster to create a Ray job.
====


ifndef::upstream[]
* You have created the required Kueue resources as described in link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects/working-with-distributed-workloads_distributed-workloads#configuring-quota-management-for-distributed-workloads_distributed_workloads[Configuring quota management for distributed workloads].
* You have removed any previously installed instances of the CodeFlare Operator, as described in the Knowledgebase solution link:https://access.redhat.com/solutions/7043796[How to migrate from a separately installed CodeFlare Operator in your data science cluster].
* If you want to use graphics processing units (GPUs), you have enabled GPU support in {productname-short}.
See link:{rhoaidocshome}{default-format-url}/managing_resources/managing-cluster-resources_cluster-mgmt#enabling-gpu-support_cluster-mgmt[Enabling GPU support in {productname-short}].
endif::[]
ifdef::cloud-service[]
* If you want to use self-signed certificates, you have added them to a central Certificate Authority (CA) bundle as described in link:{rhoaidocshome}{default-format-url}/installing_and_uninstalling_{url-productname-short}/working-with-certificates_certs[Working with certificates].
No additional configuration is necessary to use those certificates with distributed workloads.
The centrally configured self-signed certificates are automatically available in the workload pods at the following mount points:
** Cluster-wide CA bundle:
+
[source,bash]
----
/etc/pki/tls/certs/odh-trusted-ca-bundle.crt
/etc/ssl/certs/odh-trusted-ca-bundle.crt
----
** Custom CA bundle:
+
[source,bash]
----
/etc/pki/tls/certs/odh-ca-bundle.crt
/etc/ssl/certs/odh-ca-bundle.crt
----
endif::[]
ifdef::self-managed[]
* If you want to use self-signed certificates, you have added them to a central Certificate Authority (CA) bundle as described in link:{rhoaidocshome}{default-format-url}/installing_and_uninstalling_{url-productname-short}/working-with-certificates_certs[Working with certificates] (for disconnected environments, see link:{rhoaidocshome}{default-format-url}/installing_and_uninstalling_{url-productname-short}_in_a_disconnected_environment/working-with-certificates_certs[Working with certificates]).
No additional configuration is necessary to use those certificates with distributed workloads.
The centrally configured self-signed certificates are automatically available in the workload pods at the following mount points:
** Cluster-wide CA bundle:
+
[source,bash]
----
/etc/pki/tls/certs/odh-trusted-ca-bundle.crt
/etc/ssl/certs/odh-trusted-ca-bundle.crt
----
** Custom CA bundle:
+
[source,bash]
----
/etc/pki/tls/certs/odh-ca-bundle.crt
/etc/ssl/certs/odh-ca-bundle.crt
----
endif::[]

ifdef::upstream[]
* You have created the required Kueue resources as described in link:{odhdocshome}/working-on-data-science-projects/#configuring-quota-management-for-distributed-workloads_distributed_workloads[Configuring quota management for distributed workloads].
* You have removed any previously installed instances of the CodeFlare Operator.
* If you want to use graphics processing units (GPUs), you have enabled GPU support.
This process includes installing the Node Feature Discovery Operator and the NVIDIA GPU Operator.
For more information, see https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/index.html[NVIDIA GPU Operator on {org-name} OpenShift Container Platform^] in the NVIDIA documentation.
* If you want to use self-signed certificates, you have added them to a central Certificate Authority (CA) bundle as described in link:{odhdocshome}/installing_open_data_hub/#understanding-certificates_certs[Understanding certificates in {productname-short}].
No additional configuration is necessary to use those certificates with distributed workloads.
The centrally configured self-signed certificates are automatically available in the workload pods at the following mount points:
** Cluster-wide CA bundle:
+
[source,bash]
----
/etc/pki/tls/certs/odh-trusted-ca-bundle.crt
/etc/ssl/certs/odh-trusted-ca-bundle.crt
----
** Custom CA bundle:
+
[source,bash]
----
/etc/pki/tls/certs/odh-ca-bundle.crt
/etc/ssl/certs/odh-ca-bundle.crt
----
endif::[]

.Procedure
ifdef::upstream,self-managed[]
. In the {openshift-platform} console, click *Operators* -> *Installed Operators*.
endif::[]
ifdef::cloud-service[]
. In the OpenShift console, click *Operators* -> *Installed Operators*.
endif::[]
ifdef::self-managed,cloud-service[]
. Search for the *Red Hat OpenShift AI* Operator, and then click the Operator name to open the Operator details page.
endif::[]
ifdef::upstream[]
. Search for the *Open Data Hub Operator*, and then click the Operator name to open the Operator details page.
endif::[]
. Click the *Data Science Cluster* tab.
. Click the default instance name to open the instance details page.
ifndef::upstream[]
+
[NOTE]
====
Starting from {productname-long} 2.4, the default instance name for new installations is *default-dsc*.
The default instance name for earlier installations, *rhods*, is preserved during upgrade.
====
endif::[]
. Click the *YAML* tab to show the instance specifications.
. In the `spec.components` section, ensure that the `managementState` field is set correctly for the required components depending on whether the distributed workload is run from a pipeline or notebook or both, as shown in the following table.
+
.Components required for distributed workloads
[cols="34,20,20,26"]
|===
|Component | Pipelines only | Notebooks only | Pipelines and notebooks

|`codeflare`
|`Managed`
|`Managed`
|`Managed`

|`dashboard`
|`Managed`
|`Managed`
|`Managed`

|`datasciencepipelines`
|`Managed`
|`Removed`
|`Managed`

|`kueue`
|`Managed`
|`Managed`
|`Managed`

|`ray`
|`Managed`
|`Managed`
|`Managed`

|`workbenches`
|`Removed`
|`Managed`
|`Managed`
|===

. Click *Save*.
After a short time, the components with a `Managed` state are ready.


.Verification
Check the status of the `codeflare-operator-manager` pod, as follows:

ifdef::cloud-service[]
. In the OpenShift console, from the *Project* list, select *redhat-ods-applications*.
endif::[]
ifdef::self-managed[]
. In the {openshift-platform} console, from the *Project* list, select *redhat-ods-applications*.
endif::[]
ifdef::upstream[]
. In the {openshift-platform} console, from the *Project* list, select *odh*.
endif::[]
. Click *Workloads* -> *Deployments*.
. Search for the *codeflare-operator-manager* deployment, and click the deployment name to open the deployment details page.
. Click the *Pods* tab.
When the status of the `codeflare-operator-manager-_<pod-id>_` pod is `Running`, the pod is ready to use.
To see more information about the pod, click the pod name to open the pod details page, and click the *Logs* tab.
