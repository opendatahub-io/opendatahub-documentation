:_module-type: PROCEDURE

[id="running-the-training-job_{context}"]
= Running the training job

[role='_abstract']
You can run a training job to tune a model. 
The example training job in this section is based on the IBM and Hugging Face tuning example provided link:https://github.com/foundation-model-stack/fms-hf-tuning/tree/main/examples/prompt_tuning_twitter_complaints[here]. 


.Prerequisites

ifndef::upstream[]
* You have access to a data science cluster that is configured to run distributed workloads as described in link:{rhoaidocshome}{default-format-url}/managing_openshift_ai/managing_distributed_workloads[Managing distributed workloads].
endif::[]
ifdef::upstream[]
* You have access to a data science cluster that is configured to run distributed workloads as described in link:{odhdocshome}/managing-openshift-ai/#managing_distributed_workloads[Managing distributed workloads].
endif::[]

ifndef::upstream[]
* You have created a data science project. 
For information about how to create a project, see link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects/using-data-science-projects_projects#creating-a-data-science-project_projects[Creating a data science project].
endif::[]
ifdef::upstream[]
* You have created a data science project. 
For information about how to create a project, see link:{odhdocshome}/working-on-data-science-projects/#creating-a-data-science-project_projects[Creating a data science project].
endif::[]

* You have Admin access for the data science project.
** If you created the project, you automatically have Admin access. 
** If you did not create the project, your cluster administrator must give you Admin access.

* You have access to a model.
* You have access to data that you can use to train the model.

ifndef::upstream[]
* You have configured the training job as described in link:{rhoaidocshome}{default-format-url}/working_with_distributed_workloads/tuning-a-model-by-using-the-training-operator_distributed-workloads#configuring-the-training-job_distributed-workloads[Configuring the training job].
endif::[]
ifdef::upstream[]
* You have configured the training job as described in link:{odhdocshome}/working-with-distributed-workloads/#configuring-the-training-job_distributed-workloads[Configuring the training job].
endif::[]


.Procedure
. In a terminal window, log in to the OpenShift CLI as shown in the following example:
+
[source,subs="+quotes"]
----
$ oc login __<openshift_cluster_url>__ -u __<username>__ -p __<password>__
----

. Create a PyTorch training job, as follows:
.. Create a YAML file named `pytorchjob.yaml`.
.. Add the following `PyTorchJob` object definition:
+
[source]
----
apiVersion: kubeflow.org/v1
kind: PyTorchJob
metadata:
  name: kfto-demo
  namespace: kfto
spec:
  pytorchReplicaSpecs:
    Master:
      replicas: 1
      restartPolicy: Never
      template:
        spec:
          containers:
            - env:
                - name: SFT_TRAINER_CONFIG_JSON_PATH
                  value: /etc/config/config.json
              image: 'quay.io/modh/fms-hf-tuning:release'
              imagePullPolicy: IfNotPresent
              name: pytorch
              volumeMounts:
                - mountPath: /etc/config
                  name: config-volume
                - mountPath: /data/input
                  name: dataset-volume
                - mountPath: /data/output
                  name: model-volume
          volumes:
            - configMap:
                items:
                  - key: config.json
                    path: config.json
                name: training-config
              name: config-volume
            - configMap:
                name: twitter-complaints
              name: dataset-volume
            - name: model-volume
              persistentVolumeClaim:
                claimName: trained-model
  runPolicy:
    suspend: false

----
.. Replace the example namespace value `kfto` with the name of your project, and update the other parameters to suit your environment.
.. Edit the parameters of the PyTorch training job, to provide the details for your training job and environment.
.. Save your changes in the `pytorchjob.yaml` file.
.. Apply the configuration to run the PyTorch training job:
+
[source]
----
$ oc apply -f pytorchjob.yaml
----




.Verification
ifdef::upstream,self-managed[]
. In the {openshift-platform} console, select your project from the *Project* list. 
endif::[]
ifdef::cloud-service[]
. In the OpenShift console, select your project from the *Project* list.
endif::[]
. Click *Workloads* -> *Pods* and verify that the *_<training-job-name>_-master-0* pod is listed. 


////
[role='_additional-resources']
.Additional resources
<Do we want to link to additional resources?>


* link:https://url[link text]
////
