:_module-type: PROCEDURE

[id="running-the-training-job_{context}"]
= Running the training job

[role='_abstract']
You can run a training job to tune a model. 
The example training job in this section is based on the IBM and Hugging Face tuning example provided link:https://github.com/foundation-model-stack/fms-hf-tuning/tree/main/examples/prompt_tuning_twitter_complaints[here]. 


.Prerequisites
ifdef::upstream,self-managed[]
* You have logged in to {openshift-platform} with the `cluster-admin` role.
endif::[]
ifdef::cloud-service[]
* You have logged in to OpenShift with the `cluster-admin` role.
endif::[]

ifndef::upstream[]
* You have access to a data science cluster that is configured to run distributed workloads as described in link:{rhoaidocshome}{default-format-url}/working_with_distributed_workloads/configuring-distributed-workloads_distributed-workloads[Configuring distributed workloads].
endif::[]
ifdef::upstream[]
* You have access to a data science cluster that is configured to run distributed workloads as described in link:{odhdocshome}/working-with-distributed-workloads/#configuring-distributed-workloads_distributed-workloads[Configuring distributed workloads].
endif::[]

ifndef::upstream[]
* You have created a data science project. 
For information about how to create a project, see link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects/using-data-science-projects_projects#creating-a-data-science-project_projects[Creating a data science project].
endif::[]
ifdef::upstream[]
* You have created a data science project. 
For information about how to create a project, see link:{odhdocshome}/working-on-data-science-projects/#creating-a-data-science-project_projects[Creating a data science project].
endif::[]

* You have Admin access for the data science project.
** If you created the project, you automatically have Admin access. 
** If you did not create the project, your cluster administrator must give you Admin access.

* You have access to a model.
* You have access to data to train the model.

ifndef::upstream[]
* You have created the training job as described in link:{rhoaidocshome}{default-format-url}/working_with_distributed_workloads/tuning-a-model-by-using-the-training-operator_distributed-workloads#creating-the-training-job_distributed-workloads[Creating the training job].
endif::[]
ifdef::upstream[]
* You have created the training job as described in link:{odhdocshome}/working-with-distributed-workloads/#creating-the-training-job_distributed-workloads[Creating the training job].
endif::[]


.Procedure
. In a terminal window, if you are not already logged in to your OpenShift cluster as a cluster administrator, log in to the OpenShift CLI as shown in the following example:
+
[source,subs="+quotes"]
----
$ oc login __<openshift_cluster_url>__ -u __<admin_username>__ -p __<password>__
----

. Create a PyTorch training job, as follows:
.. Create a YAML file named `pytorchjob.yaml`.
.. Add the following `PyTorchJob` object definition:
+
[source]
----
apiVersion: kubeflow.org/v1
kind: PyTorchJob
metadata:
  name: kfto-demo
  namespace: kfto
spec:
  pytorchReplicaSpecs:
    Master:
      replicas: 1
      restartPolicy: Never
      template:
        spec:
          containers:
            - env:
                - name: SFT_TRAINER_CONFIG_JSON_PATH
                  value: /etc/config/config.json
              image: 'quay.io/modh/fms-hf-tuning:release'
              imagePullPolicy: IfNotPresent
              name: pytorch
              volumeMounts:
                - mountPath: /etc/config
                  name: config-volume
                - mountPath: /data/input
                  name: dataset-volume
                - mountPath: /data/output
                  name: model-volume
          volumes:
            - configMap:
                items:
                  - key: config.json
                    path: config.json
                name: my-config-trainingjob
              name: config-volume
            - configMap:
                name: twitter-complaints
              name: dataset-volume
            - name: model-volume
              persistentVolumeClaim:
                claimName: trained-model
  runPolicy:
    suspend: false

----
.. Replace the example namespace value `kfto` with the name of your project, and update the other parameters to suit your environment.
.. Edit the parameters of the PyTorch training job, to provide the details for your training job and environment.
.. Save your changes in the `pytorchjob.yaml` file.
.. Apply the configuration to run the PyTorch training job, as follows:
+
[source]
----
$ oc apply -f pytorchjob.yaml
----




.Verification
ifdef::upstream,self-managed[]
. In the {openshift-platform} console, select your project from the *Project* list. 
endif::[]
ifdef::cloud-service[]
. In the OpenShift console, select your project from the *Project* list.
endif::[]
. Click *Workloads* -> *Pods* and verify that the *_<project-name>_-demo-master-0* pod is listed. 


////
[role='_additional-resources']
.Additional resources
<Do we want to link to additional resources?>


* link:https://url[link text]
////
