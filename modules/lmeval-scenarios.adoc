:_module-type: PROCEDURE

ifdef::context[:parent-context: {context}]
[id="lmeval-scenarios_{context}"]
= LM-Eval scenarios

[role='_abstract']

The following procedures outline example scenarios that can be useful for an LM-Eval setup.

== Configuring the LM-Eval environment

If the `LMEvalJob` needs to access a model on HuggingFace with the access token, you can set up the `HF_TOKEN` as one of the environment variables for the `lm-eval` container.


.Prerequisites
* You have logged in to {productname-long}.

* Your cluster administrator has installed {productname-short} and enabled the TrustyAI service for the data science project where the models are deployed.

.Procedure

. To start an evaluation job for a `huggingface` model, use the OpenShift CLI to apply the following YAML file to your data science project:
+
[source]
----
apiVersion: trustyai.opendatahub.io/v1alpha1
kind: LMEvalJob
metadata:
  name: evaljob-sample
spec:
  model: hf
  modelArgs:
  - name: pretrained
    value: huggingfacespace/model
  taskList:
    taskNames:
    - unfair_tos/
  logSamples: true
  pod:
    container:
      env:
      - name: HF_TOKEN
        value: "My HuggingFace token"
----
+ 
For example: 
[source,subs="+quotes"]
+ 
---- 
$ oc apply -f <yaml_file> -n <project_name> 
----

. (Optional) You can also create a secret to store the token, then refer the key from the `secretKeyRef` object using the following reference syntax:
+
[source]
----

env:
  - name: HF_TOKEN
    valueFrom:
      secretKeyRef:
        name: my-secret
        key: hf-token

----

== Using a custom Unitxt card

You can run evaluations using custom Unitxt cards. To do this, include the custom Unitxt card in JSON format within the `LMEvalJob` YAML.

.Prerequisites
* You have logged in to {productname-long}.

* Your cluster administrator has installed {productname-short} and enabled the TrustyAI service for the data science project where the models are deployed.

.Procedure
. Pass a custom Unitxt Card in JSON format:
+
[source]
----
apiVersion: trustyai.opendatahub.io/v1alpha1
kind: LMEvalJob
metadata:
  name: evaljob-sample
spec:
  model: hf
  modelArgs:
  - name: pretrained
    value: google/flan-t5-base
  taskList:
    taskRecipes:
    - template: "templates.classification.multi_class.relation.default"
      card:
        custom: |
          {
            "__type__": "task_card",
            "loader": {
              "__type__": "load_hf",
              "path": "glue",
              "name": "wnli"
            },
            "preprocess_steps": [
              {
                "__type__": "split_random_mix",
                "mix": {
                  "train": "train[95%]",
                  "validation": "train[5%]",
                  "test": "validation"
                }
              },
              {
                "__type__": "rename",
                "field": "sentence1",
                "to_field": "text_a"
              },
              {
                "__type__": "rename",
                "field": "sentence2",
                "to_field": "text_b"
              },
              {
                "__type__": "map_instance_values",
                "mappers": {
                  "label": {
                    "0": "entailment",
                    "1": "not entailment"
                  }
                }
              },
              {
                "__type__": "set",
                "fields": {
                  "classes": [
                    "entailment",
                    "not entailment"
                  ]
                }
              },
              {
                "__type__": "set",
                "fields": {
                  "type_of_relation": "entailment"
                }
              },
              {
                "__type__": "set",
                "fields": {
                  "text_a_type": "premise"
                }
              },
              {
                "__type__": "set",
                "fields": {
                  "text_b_type": "hypothesis"
                }
              }
            ],
            "task": "tasks.classification.multi_class.relation",
            "templates": "templates.classification.multi_class.relation.all"
          }
  logSamples: true
----

. Inside the custom card specify the Hugging Face dataset loader:
+
[source]
----

"loader": {
              "__type__": "load_hf",
              "path": "glue",
              "name": "wnli"
            },

----

. (Optional) You can use other Unitxt loaders (found on the Unitxt website) that contain the `volumes` and `volumeMounts` parameters to mount the dataset from persistent volumes. For example, if you use the `LoadCSV` Unitxt command, mount the files to the container and make the dataset accessible for the evaluation process.

== Using PVCs as storage

To use a PVC as storage for the `LMEvalJob` results, you can use either managed PVCS or existing PVCs. Managed PVCs are managed by the TrustyAI operator. Existing PVCs are created by the end-user before the `LMEvalJob` is created.

[NOTE]
--
If both managed and existing PVCs are referenced in outputs, the TrustyAI operator defaults to the managed PVC.
--

.Prerequisites
* You have logged in to {productname-long}.

* Your cluster administrator has installed {productname-short} and enabled the TrustyAI service for the data science project where the models are deployed.

=== Managed PVCs

To create a managed PVC, specify its size. The managed PVC is named `<job-name>-pvc` and is available after the job finishes. When the `LMEvalJob` is deleted, the managed PVC is also deleted.

.Procedure
* Enter the following code:
+
[source]
----
apiVersion: trustyai.opendatahub.io/v1alpha1
kind: LMEvalJob
metadata:
  name: evaljob-sample
spec:
  # other fields omitted ...
  outputs: 
    pvcManaged: 
      size: 5Gi 
----

.Notes on the code
* `outputs` is the section for specifying custom storage locations
* `pvcManaged` will create an operator-managed PVC
* `size` (compatible with standard PVC syntax) is the only supported value

=== Existing PVCs

To use an existing PVC, pass its name as a reference. The PVC must exist when you create the `LMEvalJob`. 
The PVC is not managed by the TrustyAI operator, so it is available after deleting the `LMEvalJob`.

.Procedure
. Create a PVC. An example is the following:
+
[source]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: "my-pvc"
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
----

. Reference the new PVC from the `LMEvalJob`.
+
[source]
----
apiVersion: trustyai.opendatahub.io/v1alpha1
kind: LMEvalJob
metadata:
  name: evaljob-sample
spec:
  # other fields omitted ...
  outputs:
    pvcName: "my-pvc" 
----

== Using an InferenceService

To run an evaluation job on an `InferenceService` which is already deployed and running in your namespace, define your `LMEvalJob` CR, then apply this CR into the same namespace as your model.

.Prerequisites
* You have logged in to {productname-long}.

* Your cluster administrator has installed {productname-short} and enabled the TrustyAI service for the data science project where the models are deployed.

* You have a namespace that contains an InferenceService with a vLLM model. This example assumes that the vLLM model is already deployed in your cluster.

.Procedure

. Define your `LMEvalJob` CR:
+
[source]
----
  apiVersion: trustyai.opendatahub.io/v1alpha1
kind: LMEvalJob
metadata:
  name: evaljob
spec:
  model: local-completions
  taskList:
    taskNames:
      - mmlu
  logSamples: true
  batchSize: 1
  modelArgs:
    - name: model
      value: granite
    - name: base_url
      value: $ROUTE_TO_MODEL/v1/completions 
    - name: num_concurrent
      value:  "1"
    - name: max_retries
      value:  "3"
    - name: tokenized_requests
      value: "False"
    - name: tokenizer
      value: huggingfacespace/model
 env:
   - name: OPENAI_TOKEN
     valueFrom:
          secretKeyRef: 
            name: <secret-name> 
            key: token 
----

. Apply this CR into the same namespace as your model. 

.Verification

A pod spins up in your model namespace called `evaljob`. In the pod terminal, you can see the output via `tail -f output/stderr.log`.

.Notes on the code
* `base_url` should be set to the route/service URL of your model. Make sure to include the `/v1/completions` endpoint in the URL.
* `env.valueFrom.secretKeyRef.name` should point to a secret that contains a token that can authenticate to your model. `secretRef.name` should be the secret's name in the namespace, while `secretRef.key` should point at the token's key within the secret.
* `secretKeyRef.name` can equal the output of:
+
[source]
----
oc get secrets -o custom-columns=SECRET:.metadata.name --no-headers | grep user-one-token
----
* `secretKeyRef.key` is set to `token`


== Setting up LMEval S3 Support

Learn how to set up S3 support for your LM-Eval service. 

.Prerequisites
* You have logged in to {productname-long}.

* Your cluster administrator has installed {productname-short} and enabled the TrustyAI service for the data science project where the models are deployed.

* You have a namespace that contains an S3-compatible storage service and bucket.

* You have created an `LMEvalJob` that references the S3 bucket containing your model and dataset. 

* An S3 bucket that contains the model files and the dataset(s) to be evaluated.


.Procedure

. Create a Kubernetes Secret containing your S3 connection details:
+
[source]
----
apiVersion: v1
kind: Secret
metadata:
    name: "s3-secret"
    namespace: test
    labels:
        opendatahub.io/dashboard: "true"
        opendatahub.io/managed: "true"
    annotations:
        opendatahub.io/connection-type: s3
        openshift.io/display-name: "S3 Data Connection - LMEval"
data:
    AWS_ACCESS_KEY_ID: BASE64_ENCODED_ACCESS_KEY  # Replace with your key
    AWS_SECRET_ACCESS_KEY: BASE64_ENCODED_SECRET_KEY  # Replace with your key
    AWS_S3_BUCKET: BASE64_ENCODED_BUCKET_NAME  # Replace with your bucket name
    AWS_S3_ENDPOINT: BASE64_ENCODED_ENDPOINT  # Replace with your endpoint URL (for example,  https://s3.amazonaws.com)
    AWS_DEFAULT_REGION: BASE64_ENCODED_REGION  # Replace with your region
type: Opaque
----
+
[NOTE]
--
All values must be `base64` encoded. For example: `echo -n "my-bucket" | base64`
--
+
. Deploy the `LMEvalJob` CR that references the S3 bucket containing your model and dataset:
+
[source]
----
apiVersion: trustyai.opendatahub.io/v1alpha1
kind: LMEvalJob
metadata:
    name: evaljob-sample
spec:
    allowOnline: false
    model: hf  # Model type (HuggingFace in this example)
    modelArgs:
        - name: pretrained
          value: /opt/app-root/src/hf_home/flan  # Path where model is mounted in container
    taskList:
        taskNames:
            - arc_easy  # The evaluation task to run
    logSamples: true
    offline:
        storage:
            s3:
                accessKeyId:
                    name: s3-secret
                    key: AWS_ACCESS_KEY_ID
                secretAccessKey:
                    name: s3-secret
                    key: AWS_SECRET_ACCESS_KEY
                bucket:
                    name: s3-secret
                    key: AWS_S3_BUCKET
                endpoint:
                    name: s3-secret
                    key: AWS_S3_ENDPOINT
                region:
                    name: s3-secret
                    key: AWS_DEFAULT_REGION
                path: ""  # Optional subfolder within bucket
                verifySSL: false
----
+
[IMPORTANT]
--
 The `LMEvalJob` will copy all the files from the specified bucket/path. If your bucket contains many files and you only want to use a subset, set the `path` field to the specific sub-folder containing the files the you require. For example use `path: "my-models/"`.
--
+
. Set up a secure connection using SSL.
.. Create a ConfigMap object with your CA certificate:
+
[source]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: s3-ca-cert
  namespace: test
  annotations:
    service.beta.openshift.io/inject-cabundle: "true"  # For injection
data: {}  # OpenShift will inject the service CA bundle
# Or add your custom CA:
# data:
#   ca.crt: |-
#     -----BEGIN CERTIFICATE-----
#     ...your CA certificate content...
#     -----END CERTIFICATE-----
----
+
.. Update the `LMEvalJob` to use SSL verification:
+
[source]
----
apiVersion: trustyai.opendatahub.io/v1alpha1
kind: LMEvalJob
metadata:
    name: evaljob-sample
spec:
    # ... same as above ...
    offline:
        storage:
            s3:
                # ... same as above ...
                verifySSL: true  # Enable SSL verification
                caBundle:
                    name: s3-ca-cert  # ConfigMap name containing your CA
                    key: service-ca.crt  # Key in ConfigMap containing the certificate
----

.Verification
. After deploying the `LMEvalJob`, open the `kubectl` command-line and enter this command to check its status: `kubectl logs -n test job/evaljob-sample -n test`
. View the logs with the `kubectl` command `kubectl logs -n test job/<job-name>` to make sure it has functioned correctly. 
. The results are displayed in the logs after the evaluation is completed.








 

