:_module-type: CONCEPT

[id='overview-of-distributed-workloads_{context}']
= Overview of distributed workloads

[role='_abstract']
You can use the distributed workloads feature to queue, scale, and manage the resources required to run data science workloads across multiple nodes in an OpenShift cluster simultaneously.
Typically, data science workloads include several types of artificial intelligence (AI) workloads, including machine learning (ML) and Python workloads.

Distributed workloads provide the following benefits:

* You can iterate faster and experiment more frequently because of the reduced processing time.
* You can use larger datasets, which can lead to more accurate models.
* You can use complex models that could not be trained on a single node.
* You can submit distributed workloads at any time, and the system then schedules the distributed workload when the required resources are available.


== Distributed workloads infrastructure

The distributed workloads infrastructure includes the following components:

CodeFlare Operator::
Secures deployed Ray clusters and grants access to their URLs

CodeFlare SDK::
Defines and controls the remote distributed compute jobs and infrastructure for any Python-based environment 
+
[NOTE]
====
The CodeFlare SDK is not installed as part of {productname-short}, but it is included in some of the workbench images provided by {productname-short}.
====

Kubeflow Training Operator::
Provides fine-tuning and scalable distributed training of ML models created with different ML frameworks such as PyTorch

Kubeflow Training Operator Python Software Development Kit (Training Operator SDK)::
Simplifies the creation of distributed training and fine-tuning jobs
[NOTE]
====
The Training Operator SDK is not installed as part of {productname-short}, but it is included in some of the workbench images provided by {productname-short}.
====

KubeRay::
Manages remote Ray clusters on OpenShift for running distributed compute workloads

Kueue::
Manages quotas and how distributed workloads consume them, and manages the queueing of distributed workloads with respect to quotas

ifdef::upstream[]
For information about installing these components, see link:{odhdocshome}/installing-open-data-hub/#installing-the-distributed-workloads-components_install[Installing the distributed workloads components].
endif::[]

ifdef::self-managed[]
For information about installing these components, see link:{rhoaidocshome}{default-format-url}/installing_and_uninstalling_{url-productname-short}/installing-the-distributed-workloads-components_install[Installing the distributed workloads components] (for disconnected environments, see link:{rhoaidocshome}{default-format-url}/installing_and_uninstalling_{url-productname-short}_in_a_disconnected_environment/installing-the-distributed-workloads-components_install[Installing the distributed workloads components]).
endif::[]

ifdef::cloud-service[]
For information about installing these components, see link:{rhoaidocshome}{default-format-url}/installing_and_uninstalling_{url-productname-short}/installing-the-distributed-workloads-components_install[Installing the distributed workloads components].
endif::[]



== Types of distributed workloads

Depending on which type of distributed workloads you want to run, you must enable different {productname-short} components:

* Ray-based distributed workloads: Enable the `codeflare`, `kueue`, and `ray` components.
* Training Operator-based distributed workloads: Enable the `trainingoperator` and `kueue` components.


For both Ray-based and Training Operator-based distributed workloads, you can use Kueue and supported accelerators:

* Use Kueue to manage the resources for the distributed workload.
* Use CUDA training images for NVIDIA GPUs, and ROCm-based training images for AMD GPUs.

ifndef::upstream[]
For more information about supported accelerators, see the link:https://access.redhat.com/articles/rhoai-supported-configs[{productname-long}: Supported Configurations] Knowledgebase article
endif::[]


You can run distributed workloads from data science pipelines, from Jupyter notebooks, or from Microsoft Visual Studio Code files.

[NOTE]
====
Data science pipelines workloads are not managed by the distributed workloads feature, and are not included in the distributed workloads metrics.
====




////
[role="_additional-resources"]
.Additional resources
* link:https://url/[link text]
////
