:_module-type: PROCEDURE
[id="querying-ingested-content-in-a-llama-model_{context}"]
= Querying ingested content in a Llama model

[role="_abstract"]
You can use the Llama Stack SDK in your Jupyter notebook to query ingested content by running retrieval-augmented generation (RAG) queries on content stored in your vector store. You can perform one-off lookups without setting up a separate retrieval service.

.Prerequisites
* You have installed {openshift-platform} {ocp-minimum-version} or newer. 
ifndef::upstream[]
* You have enabled GPU support in {productname-short}. This includes installing the Node Feature Discovery operator and NVIDIA GPU Operators. For more information, see link:https://docs.redhat.com/en/documentation/openshift_container_platform/{ocp-latest-version}/html/specialized_hardware_and_driver_enablement/psap-node-feature-discovery-operator#installing-the-node-feature-discovery-operator_psap-node-feature-discovery-operator[Installing the Node Feature Discovery operator^] and link:{rhoaidocshome}{default-format-url}/managing_openshift_ai/enabling-accelerators#enabling-nvidia-gpus_managing-rhoai[Enabling NVIDIA GPUs^].
endif::[]
ifdef::upstream[]
* You have enabled GPU support. This includes installing the Node Feature Discovery and NVIDIA GPU Operators. For more information, see link:https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/index.html[NVIDIA GPU Operator on {org-name} OpenShift Container Platform^] in the NVIDIA documentation. 
endif::[]
* If you are using GPU acceleration, you have at least one NVIDIA GPU available.
* You have activated the Llama Stack Operator in {productname-short}.
* You have deployed an inference model, for example, the *llama-3.2-3b-instruct* model. 
* You have created a `LlamaStackDistribution` instance to enable RAG functionality.
* You have created a workbench within a project and opened a running Jupyter notebook.
* You have installed `llama_stack_client` version 0.3.1 or later in your workbench environment. 
* You have already ingested content into a vector store.

[NOTE]
====
This procedure requires that you have already ingested some text, HTML, or document data into a vector store, and that this content is available for retrieval. If no content is ingested, queries return empty results.
====

.Procedure
. In a new notebook cell, install the client:
+
[source,python]
----
%pip install -q llama_stack_client
----

. In a new notebook cell, import `LlamaStackClient`:
+
[source,python]
----
from llama_stack_client import LlamaStackClient
----

. Create a client instance by setting your deployment endpoint:
+
[source,python]
----
client = LlamaStackClient(base_url="http://<llama-stack-service>:8321/v1")
----
+
[NOTE]
====
For Llama Stack 0.3.5 and later, the base URL must include the `/v1` path suffix. For example, use `http://llama-stack-service:8321/v1`, not the service endpoint alone.
====

. List available models:
+
[source,python]
----
models = client.models.list()
----

. Select an LLM. If you plan to register a new vector store later in this procedure, also capture an embedding model:
+
[source,python]
----
model_id = next(m.identifier for m in models if m.model_type == "llm")

embedding = next((m for m in models if m.model_type == "embedding"), None)
if embedding:
    embedding_model_id = embedding.identifier
    embedding_dimension = int(embedding.metadata.get("embedding_dimension", 768))
----

. If you do not already have a vector store ID, register a vector store (choose one):
+
.Option 1: Inline Milvus Lite (embedded)
====
[source,python]
----
vector_store_name = "my_inline_db"
vector_store = client.vector_stores.create(
    name=vector_store_name,
    extra_body={
        "embedding_model": embedding_model_id,
        "embedding_dimension": embedding_dimension,
        "provider_id": "milvus",   # inline Milvus Lite
    },
)
vector_store_id = vector_store.id
print(f"Registered inline Milvus Lite DB: {vector_store_id}")
----
[NOTE]
Use inline Milvus Lite for development and small datasets. Persistence and scale are limited compared to remote Milvus.
====

.Option 2: Remote Milvus (recommended for production)
====
[source,python]
----
vector_store_name = "my_remote_db"
vector_store = client.vector_stores.create(
    name=vector_store_name,
    extra_body={
        "embedding_model": embedding_model_id,
        "embedding_dimension": embedding_dimension,
        "provider_id": "milvus-remote",  # remote Milvus provider
    },
)
vector_store_id = vector_store.id
print(f"Registered remote Milvus DB: {vector_store_id}")
----
[NOTE]
Ensure your `LlamaStackDistribution` sets `MILVUS_ENDPOINT` (gRPC `:19530`) and `MILVUS_TOKEN`.
====

.Option 3: Inline FAISS 
====
[source,python]
----
vector_store_name = "my_faiss_db"
vector_store = client.vector_stores.create(
    name=vector_store_name,
    extra_body={
        "embedding_model": embedding_model_id,
        "embedding_dimension": embedding_dimension,
        "provider_id": "faiss",   # inline FAISS provider
    },
)
vector_store_id = vector_store.id
print(f"Registered inline FAISS DB: {vector_store_id}")
----
[NOTE]
Inline FAISS (available in {productname-short} 3.0 and later) is a lightweight, in-process vector store. It is best for local experimentation, disconnected environments, or single-node RAG deployments.
====

. If you already have a vector store, set its identifier:
+
[source,python]
----
# For an existing store:
# vector_store_id = "<your existing vector store ID>"
----

. Query without using a vector store:
+
[source,python]
----
system_instructions = """You are a precise and reliable AI assistant.
Use retrieved context when it is available.
If nothing relevant is found in the available files, say so clearly."""

query = "How do you do great work?"

response = client.responses.create(
    model=model_id,
    input=query,
    instructions=system_instructions,
)

print("Answer (without vector stores):")
print(response.output_text)
----

. Query by using the Responses API with file search:
+
[source,python]
----
response = client.responses.create(
    model=model_id,
    input=query,
    instructions=system_instructions,
    tools=[
        {
            "type": "file_search",
            "vector_store_ids": [vector_store_id],
        }
    ],
)

print("Answer (with vector stores):")
print(response.output_text)
----

[NOTE]
====
When you include the `file_search` tool with `vector_store_ids`, Llama Stack retrieves relevant chunks from the specified vector store and provides them to the model as context for the response.
====

.Verification
* The notebook prints an answer for the query without vector stores and an answer for the same query with vector stores enabled.
* No errors appear in the output, confirming the model can retrieve and respond to ingested content from your vector store.