:_module-type: PROCEDURE

[id="querying-ingested-content-in-a-llama-model_{context}"]
= Querying ingested content in a Llama model

[role="_abstract"]
You can use the Llama Stack SDK in your Jupyter notebook to query ingested content by running retrieval-augmented generation (RAG) queries on content stored in your vector store. You can perform one-off lookups without setting up a separate retrieval service.

.Prerequisites
* You have installed {openshift-platform} {ocp-minimum-version} or newer.
ifndef::upstream[]
* You have enabled GPU support in {productname-short}. This includes installing the Node Feature Discovery Operator and NVIDIA GPU Operator. For more information, see link:https://docs.redhat.com/en/documentation/openshift_container_platform/{ocp-latest-version}/html/specialized_hardware_and_driver_enablement/psap-node-feature-discovery-operator#installing-the-node-feature-discovery-operator_psap-node-feature-discovery-operator[Installing the Node Feature Discovery Operator^] and link:{rhoaidocshome}{default-format-url}/managing_openshift_ai/enabling-accelerators#enabling-nvidia-gpus_managing-rhoai[Enabling NVIDIA GPUs^].
endif::[]
ifdef::upstream[]
* You have enabled GPU support. This includes installing the Node Feature Discovery and NVIDIA GPU Operators. For more information, see link:https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/index.html[NVIDIA GPU Operator on {org-name} OpenShift Container Platform^] in the NVIDIA documentation.
endif::[]
* If you are using GPU acceleration, you have at least one NVIDIA GPU available.
* You have activated the Llama Stack Operator in {productname-short}.
* You have deployed an inference model, for example, the *llama-3.2-3b-instruct* model.
* You have created a `LlamaStackDistribution` instance with:
** PostgreSQL configured as the metadata store.
** An embedding model configured, preferably as a remote embedding provider.
* You have created a workbench within a project and opened a running Jupyter notebook.
* You have installed `llama_stack_client` version 0.3.1 or later in your workbench environment.
* You have already ingested content into a vector store.

[NOTE]
====
This procedure requires that content has already been ingested into a vector store. If no content is available, RAG queries return empty or non-contextual responses.
====

.Procedure
. In a new notebook cell, install the client:
+
[source,python]
----
%pip install -q llama_stack_client
----

. Import `LlamaStackClient`:
+
[source,python]
----
from llama_stack_client import LlamaStackClient
----

. Create a client instance:
+
[source,python]
----
# Use the Llama Stack service or route URL that is reachable from the workbench.
# Do not append /v1 when using llama_stack_client.
client = LlamaStackClient(base_url="<llama-stack-base-url>")
----

. List available models:
+
[source,python]
----
models = client.models.list()
----

. Select an LLM. If you plan to register a new vector store, also capture an embedding model:
+
[source,python]
----
model_id = next(m.identifier for m in models if m.model_type == "llm")

embedding = next((m for m in models if m.model_type == "embedding"), None)
if embedding:
    embedding_model_id = embedding.identifier
    embedding_dimension = int(embedding.metadata.get("embedding_dimension", 768))
----

. If you do not already have a vector store ID, register a vector store (choose one):
+
.Option 1: Inline Milvus (embedded)
====
[source,python]
----
vector_store = client.vector_stores.create(
    name="my_inline_milvus",
    extra_body={
        "embedding_model": embedding_model_id,
        "embedding_dimension": embedding_dimension,
        "provider_id": "milvus",
    },
)
vector_store_id = vector_store.id
----
[NOTE]
Inline Milvus is suitable for development and small datasets. In {productname-short} 3.2 and later, metadata persistence uses PostgreSQL by default.
====

.Option 2: Remote Milvus (recommended for production)
====
[source,python]
----
vector_store = client.vector_stores.create(
    name="my_remote_milvus",
    extra_body={
        "embedding_model": embedding_model_id,
        "embedding_dimension": embedding_dimension,
        "provider_id": "milvus-remote",
    },
)
vector_store_id = vector_store.id
----
[NOTE]
Ensure your `LlamaStackDistribution` sets `MILVUS_ENDPOINT` (gRPC port 19530) and `MILVUS_TOKEN`.
====

.Option 3: Inline FAISS
====
[source,python]
----
vector_store = client.vector_stores.create(
    name="my_inline_faiss",
    extra_body={
        "embedding_model": embedding_model_id,
        "embedding_dimension": embedding_dimension,
        "provider_id": "faiss",
    },
)
vector_store_id = vector_store.id
----
[NOTE]
Inline FAISS is an in-process vector store intended for development and testing. In {productname-short} 3.2 and later, FAISS uses PostgreSQL as the default metadata store.
====

.Option 4: Remote PostgreSQL with pgvector
====
[source,python]
----
vector_store = client.vector_stores.create(
    name="my_pgvector_store",
    extra_body={
        "embedding_model": embedding_model_id,
        "embedding_dimension": embedding_dimension,
        "provider_id": "pgvector",
    },
)
vector_store_id = vector_store.id
----
[NOTE]
Ensure the pgvector provider is enabled in your `LlamaStackDistribution` and that the PostgreSQL instance has the pgvector extension installed. This option is suitable for production-grade RAG workloads that require durability and concurrency.
====

. If you already have a vector store, set its identifier:
+
[source,python]
----
# vector_store_id = "<existing-vector-store-id>"
----

. Query without using a vector store:
+
[source,python]
----
system_instructions = """You are a precise and reliable AI assistant.
Use retrieved context when it is available.
If nothing relevant is found, say so clearly."""

query = "How do you do great work?"

response = client.responses.create(
    model=model_id,
    input=query,
    instructions=system_instructions,
)

print(response.output_text)
----

. Query by using the Responses API with file search:
+
[source,python]
----
response = client.responses.create(
    model=model_id,
    input=query,
    instructions=system_instructions,
    tools=[
        {
            "type": "file_search",
            "vector_store_ids": [vector_store_id],
        }
    ],
)

print(response.output_text)
----

[NOTE]
====
When you include the `file_search` tool with `vector_store_ids`, Llama Stack retrieves relevant chunks from the specified vector store and provides them to the model as context for the response.
====

.Verification
* The notebook returns a response without vector stores and a context-aware response when vector stores are enabled.
* No errors appear, confirming successful retrieval and model execution.
