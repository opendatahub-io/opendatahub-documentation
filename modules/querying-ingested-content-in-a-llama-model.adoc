:_module-type: PROCEDURE
[id="querying-ingested-content-in-a-llama-model_{context}"]
= Querying ingested content in a Llama model

[role="_abstract"]
You can use the Llama Stack SDK in your Jupyter notebook to query ingested content by running retrieval-augmented generation (RAG) queries on text or HTML stored in your vector store. You can perform one-off lookups or start multi-turn conversational flows without setting up a separate retrieval service.

.Prerequisites
* You have installed {openshift-platform} {ocp-minimum-version} or newer. 
ifndef::upstream[]
* You have enabled GPU support in {productname-short}. This includes installing the Node Feature Discovery operator and NVIDIA GPU Operators. For more information, see link:https://docs.redhat.com/en/documentation/openshift_container_platform/{ocp-latest-version}/html/specialized_hardware_and_driver_enablement/psap-node-feature-discovery-operator#installing-the-node-feature-discovery-operator_psap-node-feature-discovery-operator[Installing the Node Feature Discovery operator^] and link:{rhoaidocshome}{default-format-url}/managing_openshift_ai/enabling-accelerators#enabling-nvidia-gpus_managing-rhoai[Enabling NVIDIA GPUs^].
endif::[]
ifdef::upstream[]
* You have enabled GPU support. This includes installing the Node Feature Discovery and NVIDIA GPU Operators. For more information, see link:https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/index.html[NVIDIA GPU Operator on {org-name} OpenShift Container Platform^] in the NVIDIA documentation. 
endif::[]
* If you are using GPU acceleration, you have at least one NVIDIA GPU available.
* You have activated the Llama Stack Operator in {productname-short}.
* You have deployed an inference model, for example, the *llama-3.2-3b-instruct* model. 
* You have created a `LlamaStackDistribution` instance to enable RAG functionality.
* You have created a workbench within a project and opened a running Jupyter notebook.
* You have installed `llama_stack_client` version 0.3.1 or later in your workbench environment. 
* You have already ingested content into a vector store.

[NOTE]
====
This procedure requires that you have already ingested some text, HTML, or document data into a vector store, and that this content is available for retrieval. If no content is ingested, queries return empty results.
====

.Procedure

. In a new notebook cell, install the client:
+
[source,python]
----
%pip install -q llama_stack_client
----

. In a new notebook cell, import `Agent`, `AgentEventLogger`, and `LlamaStackClient`:
+
[source,python]
----
from llama_stack_client import Agent, AgentEventLogger, LlamaStackClient
----

. Create a client instance by setting your deployment endpoint:
+
[source,python]
----
client = LlamaStackClient(base_url="<your deployment endpoint>")
----

. List available models:
+
[source,python]
----
models = client.models.list()
----

. Select an LLM (and, if needed below, capture an embedding model for store registration):
+
[source,python]
----
model_id = next(m.identifier for m in models if m.model_type == "llm")

embedding = next((m for m in models if m.model_type == "embedding"), None)
if embedding:
    embedding_model_id = embedding.identifier
    embedding_dimension = int(embedding.metadata.get("embedding_dimension", 768))
----
  
. If you do not already have a vector store ID, register a vector store (choose one):
+
.Option 1: Inline Milvus Lite (embedded)
====
[source,python]
----
vector_store_name = "my_inline_db"
vector_store = client.vector_stores.create(
    name=vector_store_name,
    extra_body={
        "embedding_model": embedding_model_id,
        "embedding_dimension": embedding_dimension,
        "provider_id": "milvus",   # inline Milvus Lite
    },
)
vector_store_id = vector_store.id
print(f"Registered inline Milvus Lite DB: {vector_store_id}")
----
[NOTE]
Use inline Milvus Lite for development and small datasets. Persistence and scale are limited compared to remote Milvus.
====

.Option 2: Remote Milvus (recommended for production)
====
[source,python]
----
vector_store_name = "my_remote_db"
vector_store = client.vector_stores.create(
    name=vector_store_name,
    extra_body={
        "embedding_model": embedding_model_id,
        "embedding_dimension": embedding_dimension,
        "provider_id": "milvus-remote",  # remote Milvus provider
    },
)
vector_store_id = vector_store.id
print(f"Registered remote Milvus DB: {vector_store_id}")
----
[NOTE]
Ensure your `LlamaStackDistribution` sets `MILVUS_ENDPOINT` (gRPC `:19530`) and `MILVUS_TOKEN`.
====

.Option 3: Inline FAISS (SQLite backend)
====
[source,python]
----
vector_store_name = "my_faiss_db"
vector_store = client.vector_stores.create(
    name=vector_store_name,
    extra_body={
        "embedding_model": embedding_model_id,
        "embedding_dimension": embedding_dimension,
        "provider_id": "faiss",   # inline FAISS provider
    },
)
vector_store_id = vector_store.id
print(f"Registered inline FAISS DB: {vector_store_id}")
----
[NOTE]
Inline FAISS (available in {productname-short} 3.0 and later) is a lightweight, in-process vector store with SQLite-based persistence. It is best for local experimentation, disconnected environments, or single-node RAG deployments.
====

. If you already have a vector store, set its identifier:
+
[source,python]
----
# For an existing store:
# vector_store_id = "<your existing vector store ID>"
----

. Query the ingested content by using the OpenAI-compatible Responses API with file search:
+
[source,python]
----
query = "What benefits do the ingested passages provide for retrieval?"

response = client.responses.create(
    model=model_id,
    input=query,
    tools=[
        {
            "type": "file_search",
            "vector_store_ids": [vector_store_id],
        }
    ],
)
print("Responses API result:", getattr(response, "output_text", response))
----

. Query the ingested content by using the high-level Agent API:
+
[source,python]
----
agent = Agent(
    client,
    model=model_id,
    instructions="You are a helpful assistant.",
    tools=[
        {
            "name": "builtin::rag/knowledge_search",
            "args": {"vector_store_ids": [vector_store_id]},
        }
    ],
)

prompt = "How do you do great work?"
print("Prompt>", prompt)

session_id = agent.create_session("rag_session")
stream = agent.create_turn(
    messages=[{"role": "user", "content": prompt}],
    session_id=session_id,
    stream=True,
)

for log in AgentEventLogger().log(stream):
    log.print()
----

.Verification
* The notebook prints query results for both the Responses API and the Agent API.
* No errors appear in the output, confirming the model can retrieve and respond to ingested content from your vector store.
