:_module-type: PROCEDURE

[id="ingesting-content-into-a-llama-model_{context}"]
= Ingesting content into a Llama model

[role="_abstract"]
You can quickly customize and prototype your retrievable content by ingesting raw text into your model from inside a Jupyter notebook. This approach avoids requiring a separate ingestion pipeline. By using the LlamaStack SDK, you can embed and store text in your vector store in real time, enabling immediate RAG workflows.

.Prerequisites
* You have installed {openshift-platform} {ocp-minimum-version} or newer. 
* You have deployed a Llama 3.2 model with a vLLM model server and you have integrated LlamaStack.
* You have created a workbench within a project.
* You have opened a Jupyter notebook and it is running in your workbench environment.
* You have installed the `llama_stack_client` version 0.2.22 or later in your workbench environment. 
* You have a vector database identifier, or you plan to create or register one in this procedure.
ifdef::self-managed[]
* Your environment has network access to the vector database service through {openshift-platform}.
endif::[]

.Procedure
. In a new notebook cell, install the `llama_stack_client` package and its dependencies:
+
[source,python]
----
%pip install llama_stack_client fire
----

. In a new notebook cell, import `RAGDocument` and `LlamaStackClient`:
+
[source,python]
----
from llama_stack_client import RAGDocument, LlamaStackClient
----

. In a new notebook cell, assign your deployment endpoint to the `base_url` parameter to create a `LlamaStackClient` instance:
+
[source,python]
----
client = LlamaStackClient(base_url="<your deployment endpoint>")
----

. List the available models:
+
[source,python]
----
# Fetch all registered models
models = client.models.list()
----

. Verify that the list of registered models includes your Llama model and an embedding model. Here is an example of a list of registered models:
+
[source,python]
----
[Model(identifier='llama-32-3b-instruct', metadata={}, api_model_type='llm', provider_id='vllm-inference', provider_resource_id='llama-32-3b-instruct', type='model', model_type='llm'),
 Model(identifier='ibm-granite/granite-embedding-125m-english', metadata={'embedding_dimension': 768.0}, api_model_type='embedding', provider_id='sentence-transformers', provider_resource_id='ibm-granite/granite-embedding-125m-english', type='model', model_type='embedding')]
----

. Select the first LLM and the first embedding model:
+
[source,python]
----
model_id = next(m.identifier for m in models if m.model_type == "llm")

embedding_model = next(m for m in models if m.model_type == "embedding")
embedding_model_id = embedding_model.identifier
embedding_dimension = int(embedding_model.metadata["embedding_dimension"])
----

. (Optional) Register a vector database (choose one). Skip if you already have a vector DB ID.
+
.Option 1: Inline Milvus Lite (embedded)
====
[source,python]
----
vector_db_id = "my_inline_db"

vector_store = client.vector_stores.create(
    name=vector_db_id,
    embedding_model=embedding_model_id,
    embedding_dimension=embedding_dimension,
    provider_id="milvus",   # inline Milvus Lite
)
print(f"Registered inline Milvus Lite DB: {vector_db_id}")
----
====
[NOTE]
Use inline Milvus Lite for development and small datasets. Persistence and scale are limited compared to remote Milvus.
====
====

.Option 2: Remote Milvus (recommended for production)
====
[source,python]
----
vector_db_id = "my_remote_db"

vector_store = client.vector_stores.create(
    name=vector_db_id,
    embedding_model=embedding_model_id,
    embedding_dimension=embedding_dimension,
    provider_id="milvus-remote",  # remote Milvus provider (v2.25+)
)
print(f"Registered remote Milvus DB: {vector_db_id}")
----
[NOTE]
====
* Ensure your `LlamaStackDistribution` sets `MILVUS_ENDPOINT` (gRPC `:19530`) and `MILVUS_TOKEN`.
* Aside from the `provider_id`, ingestion and query APIs are identical for inline and remote Milvus.
====
====

.Option 3: Inline FAISS (SQLite backend)
====
[source,python]
----
vector_db_id = "my_faiss_db"

vector_store = client.vector_stores.create(
    name=vector_db_id,
    embedding_model=embedding_model_id,
    embedding_dimension=embedding_dimension,
    provider_id="faiss",   # inline FAISS provider
)
print(f"Registered inline FAISS DB: {vector_db_id}")
----
====

. If you already have a vector database, set its identifier:
+
[source,python]
----
# If a DB already exists, set it here instead of registering above
# Example:
# vector_db_id = "<your existing vector database ID>"
----

. In a new notebook cell, define the raw text that you want to ingest into the vector store: 
+ 
[source,python]
----
# Example raw text passage
raw_text = """
LlamaStack can embed raw text into a vector store for retrieval.
This example ingests a small passage for demonstration.
"""
----

. In a new notebook cell, create a `RAGDocument` object to contain the raw text:
+
[source,python]
----
document = RAGDocument(
    document_id="raw_text_001",
    content=raw_text,
    mime_type="text/plain",
    metadata={"source": "example_passage"},
)
----

. In a new notebook cell, ingest the raw text into the vector store by using the Vector Store Files API:
+
[source,python]
----
client.vector_stores.files.create(
    vector_store_id=vector_db_id,
    documents=[document],
    chunk_size_in_tokens=100,
)
print("Raw text ingested successfully")
----

. In a new notebook cell, create a `RAGDocument` from an HTML source and ingest it into the vector store:
+
[source,python]
----
source = "https://www.paulgraham.com/greatwork.html"
print("Ingesting document:", source)

document = RAGDocument(
    document_id="document_1",
    content=source,
    mime_type="text/html",
    metadata={},
)
----

. In a new notebook cell, ingest the content into the vector store:
+
[source,python]
----
client.vector_stores.files.create(
    vector_store_id=vector_db_id,
    documents=[document],
    chunk_size_in_tokens=50,
)
print("HTML document ingested successfully")
----

.Verification

* Review the output to confirm successful ingestion. A typical response after ingestion includes the number of text chunks inserted and any warnings or errors.
* The model list returned by `client.models.list()` includes your Llama 3.2 model and an embedding model.
