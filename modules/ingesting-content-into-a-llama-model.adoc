:_module-type: PROCEDURE
[id="ingesting-content-into-a-llama-model_{context}"]
= Ingesting content into a Llama model

[role="_abstract"]
You can quickly customize and prototype your retrievable content by ingesting raw text into your model from inside a Jupyter notebook. This approach avoids building a separate ingestion pipeline. By using the Llama Stack SDK, you can embed and store text in your vector store in real time, enabling immediate RAG workflows.

.Prerequisites
* You have installed {openshift-platform} {ocp-minimum-version} or newer. 
* You have deployed a Llama 3.2 model with a vLLM model server.
* You have created a `LlamaStackDistribution` instance (Llama Stack).
* You have created a workbench within a project.
* You have opened a Jupyter notebook and it is running in your workbench environment.
* You have installed the `llama_stack_client` version 0.3.1 or later in your workbench environment. 
ifdef::self-managed[]
* If you use a remote vector store, your environment has network access to that service through {openshift-platform}.
endif::[]

.Procedure
. In a new notebook cell, install the client:
+
[source,python]
----
%pip install llama_stack_client
----

. Import `LlamaStackClient` and create a client instance:
+
[source,python]
----
from llama_stack_client import LlamaStackClient
client = LlamaStackClient(base_url="<your deployment endpoint>")
----

. List the available models:
+
[source,python]
----
# Fetch all registered models
models = client.models.list()
----

. Verify that the list includes your Llama model and an embedding model. For example:
+
[source,python]
----
[Model(identifier='llama-32-3b-instruct', metadata={}, api_model_type='llm', provider_id='vllm-inference', provider_resource_id='llama-32-3b-instruct', type='model', model_type='llm'),
 Model(identifier='ibm-granite/granite-embedding-125m-english', metadata={'embedding_dimension': 768.0}, api_model_type='embedding', provider_id='sentence-transformers', provider_resource_id='ibm-granite/granite-embedding-125m-english', type='model', model_type='embedding')]
----

. Select one LLM and one embedding model:
+
[source,python]
----
model_id = next(m.identifier for m in models if m.model_type == "llm")
embedding_model = next(m for m in models if m.model_type == "embedding")
embedding_model_id = embedding_model.identifier
embedding_dimension = int(embedding_model.metadata["embedding_dimension"])
----

. (Optional) Create a vector store (choose one). Skip this step if you already have one.
+
.Option 1: Inline Milvus Lite (embedded)
====
[source,python]
----
vector_store_name = "my_inline_db"
vector_store = client.vector_stores.create(
    name=vector_store_name,
    extra_body={
        "embedding_model": embedding_model_id,
        "embedding_dimension": embedding_dimension,
        "provider_id": "milvus",   # inline Milvus Lite
    },
)
vector_store_id = vector_store.id
print(f"Registered inline Milvus Lite DB: {vector_store_id}")
----
[NOTE]
Use inline Milvus Lite for development and small datasets. Persistence and scale are limited compared to remote Milvus.
====

.Option 2: Remote Milvus (recommended for production)
====
[source,python]
----
vector_store_name = "my_remote_db"
vector_store = client.vector_stores.create(
    name=vector_store_name,
    extra_body={
        "embedding_model": embedding_model_id,
        "embedding_dimension": embedding_dimension,
        "provider_id": "milvus-remote",  # remote Milvus provider
    },
)
vector_store_id = vector_store.id
print(f"Registered remote Milvus DB: {vector_store_id}")
----
[NOTE]
Ensure your `LlamaStackDistribution` sets `MILVUS_ENDPOINT` (gRPC `:19530`) and `MILVUS_TOKEN`.
====

.Option 3: Inline FAISS (SQLite backend)
====
[source,python]
----
vector_store_name = "my_faiss_db"
vector_store = client.vector_stores.create(
    name=vector_store_name,
    extra_body={
        "embedding_model": embedding_model_id,
        "embedding_dimension": embedding_dimension,
        "provider_id": "faiss",   # inline FAISS provider
    },
)
vector_store_id = vector_store.id
print(f"Registered inline FAISS DB: {vector_store_id}")
----
[NOTE]
Inline FAISS (available in {productname-short} 3.0 and later) is a lightweight, in-process vector store with SQLite-based persistence. It is best for local experimentation, disconnected environments, or single-node RAG deployments.
====

. If you already have a vector store, set its identifier:
+
[source,python]
----
# For an existing vector store:
# vector_store_id = "<your existing vector store ID>"
----

. Define raw text to ingest:
+
[source,python]
----
raw_text = """Llama Stack can embed raw text into a vector store for retrieval.
This example ingests a small passage for demonstration."""
----

. Ingest raw text by using the Vector Store Files API:
+
[source,python]
----
items = [
    {
        "id": "raw_text_001",
        "text": raw_text,
        "mime_type": "text/plain",
        "metadata": {"source": "example_passage"},
    }
]
result = client.vector_stores.files.create(
    vector_store_id=vector_store_id,
    items=items,
    chunk_size_in_tokens=100,
)
print("Text ingestion result:", result)
----

. Ingest an HTML source:
+
[source,python]
----
html_item = [
    {
        "id": "doc_html_001",
        "text": "https://www.paulgraham.com/greatwork.html",
        "mime_type": "text/html",
        "metadata": {"note": "Example URL"},
    }
]
result = client.vector_stores.files.create(
    vector_store_id=vector_store_id,
    items=html_item,
    chunk_size_in_tokens=50,
)
print("HTML ingestion result:", result)
----

.Verification
* Review the output to confirm successful ingestion. A typical response includes file or chunk counts and any warnings or errors.
* The model list returned by `client.models.list()` includes your Llama 3.2 model and an embedding model.
