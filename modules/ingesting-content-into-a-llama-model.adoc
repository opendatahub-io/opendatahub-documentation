:_module-type: PROCEDURE
[id="ingesting-content-into-a-llama-model_{context}"]
= Ingesting content into a Llama model

[role="_abstract"]
You can quickly customize and prototype retrievable content by uploading a document and adding it to a vector store from inside a Jupyter notebook. This approach avoids building a separate ingestion pipeline. By using the Llama Stack SDK, you can ingest documents into a vector store and enable retrieval-augmented generation (RAG) workflows.

.Prerequisites
* You have installed {openshift-platform} {ocp-minimum-version} or newer. 
* You have deployed a Llama 3.2 model with a vLLM model server.
* You have created a `LlamaStackDistribution` instance (Llama Stack).
* You have created a workbench within a project.
* You have opened a Jupyter notebook and it is running in your workbench environment.
* You have installed the `llama_stack_client` version 0.3.1 or later in your workbench environment. 
ifdef::self-managed[]
* If you use a remote vector store, your environment has network access to that service through {openshift-platform}.
endif::[]

.Procedure
. In a new notebook cell, install the client:
+
[source,python]
----
%pip install llama_stack_client
----

. Import `LlamaStackClient` and create a client instance:
+
[source,python]
----
from llama_stack_client import LlamaStackClient
client = LlamaStackClient(base_url="http://<llama-stack-service>:8321/v1")
----
+
[NOTE]
====
For Llama Stack 0.3.5 and later, the base URL must include the `/v1` path suffix. For example, use `http://llama-stack-service:8321/v1`, not the service endpoint alone.
====

. List the available models:
+
[source,python]
----
models = client.models.list()
----

. Verify that the list includes your Llama model and an embedding model. For example:
+
[source,python]
----
[Model(identifier='llama-32-3b-instruct', metadata={}, api_model_type='llm', provider_id='vllm-inference', provider_resource_id='llama-32-3b-instruct', type='model', model_type='llm'),
 Model(identifier='ibm-granite/granite-embedding-125m-english', metadata={'embedding_dimension': 768.0}, api_model_type='embedding', provider_id='sentence-transformers', provider_resource_id='ibm-granite/granite-embedding-125m-english', type='model', model_type='embedding')]
----

. Select one LLM and one embedding model:
+
[source,python]
----
model_id = next(m.identifier for m in models if m.model_type == "llm")
embedding_model = next(m for m in models if m.model_type == "embedding")
embedding_model_id = embedding_model.identifier
embedding_dimension = int(embedding_model.metadata["embedding_dimension"])
----

. (Optional) Create a vector store (choose one). Skip this step if you already have one.
+
.Option 1: Inline Milvus Lite (embedded)
====
[source,python]
----
vector_store_name = "my_inline_db"
vector_store = client.vector_stores.create(
    name=vector_store_name,
    extra_body={
        "embedding_model": embedding_model_id,
        "embedding_dimension": embedding_dimension,
        "provider_id": "milvus",   # inline Milvus Lite
    },
)
vector_store_id = vector_store.id
print(f"Registered inline Milvus Lite DB: {vector_store_id}")
----
[NOTE]
Use inline Milvus Lite for development and small datasets. Persistence and scale are limited compared to remote Milvus.
====
+
.Option 2: Remote Milvus (recommended for production)
====
[source,python]
----
vector_store_name = "my_remote_db"
vector_store = client.vector_stores.create(
    name=vector_store_name,
    extra_body={
        "embedding_model": embedding_model_id,
        "embedding_dimension": embedding_dimension,
        "provider_id": "milvus-remote",  # remote Milvus provider
    },
)
vector_store_id = vector_store.id
print(f"Registered remote Milvus DB: {vector_store_id}")
----
[NOTE]
Ensure your `LlamaStackDistribution` sets `MILVUS_ENDPOINT` (gRPC `:19530`) and `MILVUS_TOKEN`.
====
+
.Option 3: Inline FAISS
====
[source,python]
----
vector_store_name = "my_faiss_db"
vector_store = client.vector_stores.create(
    name=vector_store_name,
    extra_body={
        "embedding_model": embedding_model_id,
        "embedding_dimension": embedding_dimension,
        "provider_id": "faiss",   # inline FAISS provider
    },
)
vector_store_id = vector_store.id
print(f"Registered inline FAISS DB: {vector_store_id}")
----
[NOTE]
Inline FAISS (available in {productname-short} 3.0 and later) is a lightweight, in-process vector store with SQLite-based persistence. It is best for local experimentation, disconnected environments, or single-node RAG deployments.
====

. If you already have a vector store, set its identifier:
+
[source,python]
----
# For an existing vector store:
# vector_store_id = "<your existing vector store ID>"
----

. Download a PDF, upload it to Llama Stack, and add it to your vector store:
+
[source,python]
----
import requests

# Download a PDF from a URL
pdf_url = "https://www.federalreserve.gov/aboutthefed/files/quarterly-report-20250822.pdf"
filename = "quarterly-report-20250822.pdf"

response = requests.get(pdf_url)
response.raise_for_status()

# Save it locally
with open(filename, "wb") as f:
    f.write(response.content)

print(f"Downloaded and saved: {filename}")

# Upload the PDF to Llama Stack with a specified filename
with open(filename, "rb") as f:
    file_info = client.files.create(
        file=(filename, f),
        purpose="assistants",
    )

print("File uploaded successfully:")
print(file_info)

# Add the uploaded file to the vector store
# If you do not set a chunking_strategy, default values are applied.
vector_store_file = client.vector_stores.files.create(
    vector_store_id=vector_store_id,
    file_id=file_info.id,
    chunking_strategy={
        "type": "static",  # required
        "static": {
            "max_chunk_size_tokens": 800,  # default ~800
            "chunk_overlap_tokens": 400,   # default ~400
        },
    },
)

print("File added to vector store:")
print(vector_store_file)
----

.Verification
* The output from `client.vector_stores.files.create()` confirms that the file was added to the vector store.
* The model list returned by `client.models.list()` includes your Llama 3.2 model and an embedding model.
