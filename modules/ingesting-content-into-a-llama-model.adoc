:_module-type: PROCEDURE

[id="ingesting-content-into-a-llama-model_{context}"]
= Ingesting content into a Llama model

[role="_abstract"]
You can quickly customize and prototype retrievable content by uploading a document and adding it to a vector store from inside a Jupyter notebook. This approach avoids building a separate ingestion pipeline. By using the Llama Stack SDK, you can ingest documents into a vector store and enable retrieval-augmented generation (RAG) workflows.

.Prerequisites
* You have installed {openshift-platform} {ocp-minimum-version} or newer.
* You have deployed a Llama 3.2 model with a vLLM model server.
* You have created a `LlamaStackDistribution` instance.
* You have configured a PostgreSQL database for Llama Stack metadata storage.
* You have configured an embedding model:
** *Recommended:* You have configured a remote embedding model by using environment variables in the `LlamaStackDistribution`.
** *Optional:* You have enabled inline embeddings with the sentence-transformers library for development or testing.
* You have created a workbench within a project.
* You have opened a Jupyter notebook and it is running in your workbench environment.
* You have installed `llama_stack_client` version 0.3.1 or later in your workbench environment.
ifdef::self-managed[]
* If you use a remote vector store or remote embedding model, your environment has network access to those services through {openshift-platform}.
endif::[]

.Procedure
. In a new notebook cell, install the client:
+
[source,python]
----
%pip install llama_stack_client
----

. Import `LlamaStackClient` and create a client instance:
+
[source,python]
----
from llama_stack_client import LlamaStackClient

# Use the Llama Stack service or route URL that is reachable from the workbench.
# Do not append /v1 when using llama_stack_client.
client = LlamaStackClient(base_url="<llama-stack-base-url>")
----

. List the available models:
+
[source,python]
----
models = client.models.list()
----

. Verify that the list includes:
* At least one LLM model.
* At least one embedding model (remote or inline).
+
[source,python]
----
[Model(identifier='llama-32-3b-instruct', model_type='llm', provider_id='vllm-inference'),
 Model(identifier='nomic-embed-text-v1-5', model_type='embedding', metadata={'embedding_dimension': 768})]
----

. Select one LLM and one embedding model:
+
[source,python]
----
model_id = next(m.identifier for m in models if m.model_type == "llm")

embedding_model = next(m for m in models if m.model_type == "embedding")
embedding_model_id = embedding_model.identifier
embedding_dimension = int(embedding_model.metadata["embedding_dimension"])
----

. (Optional) Create a vector store. Skip this step if you already have one.
+
.Option 1: Inline Milvus (embedded)
====
[source,python]
----
vector_store = client.vector_stores.create(
    name="my_inline_milvus",
    extra_body={
        "embedding_model": embedding_model_id,
        "embedding_dimension": embedding_dimension,
        "provider_id": "milvus",
    },
)
vector_store_id = vector_store.id
----
[NOTE]
Inline Milvus is suitable for development and small datasets. In {productname-short} 3.2 and later, metadata persistence uses PostgreSQL by default.
====

.Option 2: Remote Milvus (recommended for production)
====
[source,python]
----
vector_store = client.vector_stores.create(
    name="my_remote_milvus",
    extra_body={
        "embedding_model": embedding_model_id,
        "embedding_dimension": embedding_dimension,
        "provider_id": "milvus-remote",
    },
)
vector_store_id = vector_store.id
----
[NOTE]
Ensure your `LlamaStackDistribution` is configured with `MILVUS_ENDPOINT` and `MILVUS_TOKEN`.
====

.Option 3: Inline FAISS
====
[source,python]
----
vector_store = client.vector_stores.create(
    name="my_inline_faiss",
    extra_body={
        "embedding_model": embedding_model_id,
        "embedding_dimension": embedding_dimension,
        "provider_id": "faiss",
    },
)
vector_store_id = vector_store.id
----
[NOTE]
Inline FAISS is an in-process vector store intended for development and testing. In {productname-short} 3.2 and later, FAISS uses PostgreSQL as the default metadata store.
====

.Option 4: Remote PostgreSQL with pgvector
====
[source,python]
----
vector_store = client.vector_stores.create(
    name="my_pgvector_store",
    extra_body={
        "embedding_model": embedding_model_id,
        "embedding_dimension": embedding_dimension,
        "provider_id": "pgvector",
    },
)
vector_store_id = vector_store.id
----
[NOTE]
Ensure that the pgvector provider is enabled in your `LlamaStackDistribution` and that the PostgreSQL instance has the pgvector extension installed.
====

. If you already have a vector store, set its identifier:
+
[source,python]
----
# vector_store_id = "<existing-vector-store-id>"
----

. Download a PDF, upload it to Llama Stack, and add it to your vector store:
+
[source,python]
----
import requests

pdf_url = "https://www.federalreserve.gov/aboutthefed/files/quarterly-report-20250822.pdf"
filename = "quarterly-report-20250822.pdf"

response = requests.get(pdf_url)
response.raise_for_status()

with open(filename, "wb") as f:
    f.write(response.content)

with open(filename, "rb") as f:
    file_info = client.files.create(
        file=(filename, f),
        purpose="assistants",
    )

vector_store_file = client.vector_stores.files.create(
    vector_store_id=vector_store_id,
    file_id=file_info.id,
    chunking_strategy={
        "type": "static",
        "static": {
            "max_chunk_size_tokens": 800,
            "chunk_overlap_tokens": 400,
        },
    },
)

print(vector_store_file)
----

.Verification
* The call to `client.vector_stores.files.create()` succeeds and returns metadata for the ingested file.
* The vector store contains indexed chunks associated with the uploaded document.
* Subsequent RAG queries can retrieve content from the vector store.
