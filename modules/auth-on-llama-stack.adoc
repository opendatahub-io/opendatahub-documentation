:_module-type: PROCEDURE

[id="auth-on-llama-stack_{context}"]
= Configuring Llama Stack with OAuth Authentication

You can configure Llama Stack to enable Role-Based Access Control (RBAC) for model access using OAuth authentication on {productname-short}. The following example shows how to configure Llama Stack so that a vLLM model can be accessed by all authenticated users, while an OpenAI model is restricted to specific users. This example uses Keycloak to issue and validate tokens.

Before you begin, you must already have Keycloak set up with the following parameters:

* Realm: `llamastack-demo`
* Client: `llamastack` with direct access grants enabled
* Role: `inference_max` grants access to restricted models and a protocol mapper that adds realm roles to the access token under the claim name `llamastack_roles`
* Two test users:
** `user1` as a basic user with no assigned roles
** `user2` as an advanced user assigned the `inference_max` role
* The client secret generated by Keycloak must be saved as you will need it for token requests.

This document assumes the Keycloak server is available at `https://my-keycloak-server.com`

[IMPORTANT]
====
When accessing Llama Stack APIs, the required base URL depends on the client you are using.

* **OpenAI-compatible clients or raw HTTP requests**  
  You must include the `/v1` path suffix in the base URL.  
  For example: `http://llama-stack-service:8321/v1`

* **`LlamaStackClient` SDK**  
  Do not include the `/v1` path suffix.  
  For example: `http://llama-stack-service:8321`

Using an incorrect base URL results in request failures.
====

.Prerequisites

* You have installed {openshift-platform} 4.19 or newer.
* You have logged in to {productname-long}.
* You have cluster administrator privileges for your OpenShift cluster.
* You have installed the {openshift-cli} as described in the appropriate documentation for your cluster:
ifdef::upstream,self-managed[]
** link:https://docs.redhat.com/en/documentation/openshift_container_platform/{ocp-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli[Installing the OpenShift CLI^] for OpenShift Container Platform
** link:https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws/{rosa-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli[Installing the OpenShift CLI^] for {rosa-productname}
endif::[]
ifdef::cloud-service[]
** link:https://docs.redhat.com/en/documentation/openshift_dedicated/{osd-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli[Installing the OpenShift CLI^] for OpenShift Dedicated
** link:https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws_classic_architecture/{rosa-classic-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli[Installing the OpenShift CLI^] for {rosa-classic-productname}
endif::[]

.Procedure

. To configure Llama Stack to use Role-Based Access Control (RBAC) to access models, you need to view and verify the OAuth provider token structure.

.. Generate a Keycloak test token to view the structure with the following command:
+
[source,terminal]
----
$ curl -d client_id=llamastack -d client_secret=YOUR_CLIENT_SECRET -d username=user1 -d password=user-password -d grant_type=password https://my-keycloak-server.com/realms/llamastack-demo/protocol/openid-connect/token | jq -r .access_token > test.token
----

.. View the token claims by running the following command:
+
[source,terminal]
----
$ cat test.token | cut -d . -f 2 | base64 -d 2>/dev/null | jq .
----
+
.Example token structure from Keycloak
[source,terminal]
----
$ {
  "iss": "http://my-keycloak-server.com/realms/llamastack-demo",
  "aud": "account",
  "sub": "761cdc99-80e5-4506-9b9e-26a67a8566f7",
  "preferred_username": "user1",
  "llamastack_roles": [
    "inference_max"
  ],
}
----

. Create a `run.yaml` file that defines the necessary configurations for OAuth.

.. Define a configuration with two inference providers and OAuth authentication with the following `run.yaml` example:
+
[source,yaml]
----
version: 2
image_name: rh
apis:
  - inference
  - agents
  - safety
  - telemetry
  - tool_runtime
  - vector_io
providers:
  inference:
    - provider_id: vllm-inference
      provider_type: remote::vllm
      config:
        url: ${env.VLLM_URL:=http://localhost:8000/v1}
        max_tokens: ${env.VLLM_MAX_TOKENS:=4096}
        api_token: ${env.VLLM_API_TOKEN:=fake}
        tls_verify: ${env.VLLM_TLS_VERIFY:=true}
    - provider_id: openai
      provider_type: remote::openai
      config:
        api_key: ${env.OPENAI_API_KEY:=}
        base_url: ${env.OPENAI_BASE_URL:=https://api.openai.com/v1}  # OpenAI-compatible API requires /v1
  telemetry:
  - provider_id: meta-reference
    provider_type: inline::meta-reference
    config:
      service_name: "${env.OTEL_SERVICE_NAME:=}"
      sinks: ${env.TELEMETRY_SINKS:=console}
      sqlite_db_path: /opt/app-root/src/.llama/distributions/rh/trace_store.db
      otel_exporter_otlp_endpoint: ${env.OTEL_EXPORTER_OTLP_ENDPOINT:=}
  agents:
  - provider_id: meta-reference
    provider_type: inline::meta-reference
    config:
      persistence_store:
        type: sqlite
        namespace: null
        db_path: /opt/app-root/src/.llama/distributions/rh/agents_store.db
      responses_store:
        type: sqlite
        db_path: /opt/app-root/src/.llama/distributions/rh/responses_store.db
models:
  - model_id: llama-3-2-3b
    provider_id: vllm-inference
    model_type: llm
    metadata: {}

  - model_id: gpt-4o-mini
    provider_id: openai
    model_type: llm
    metadata: {}

server:
  port: 8321
  auth:
    provider_config:
      type: "oauth2_token"
      jwks:
        uri: "https://my-keycloak-server.com/realms/llamastack-demo/protocol/openid-connect/certs" <1>
        key_recheck_period: 3600
      issuer: "https://my-keycloak-server.com/realms/llamastack-demo" <1>
      audience: "account"
      verify_tls: true
      claims_mapping:
        llamastack_roles: "roles" <2>
    access_policy:
      - permit: <3>
          actions: [read]
          resource: model::vllm-inference/llama-3-2-3b
        description: Allow all authenticated users to access Llama 3.2 model
      - permit: <4>
          actions: [read]
          resource: model::openai/gpt-4o-mini
        when: user with inference_max in roles
        description: Allow only users with inference_max role to access OpenAI models
----
+
<1> Specify your Keycloak host and Realm in the URL.  
<2> Maps the `llamastack_roles` path from the token to the `roles` field.  
<3> Policy 1: Allow all authenticated users to access vLLM models.  
<4> Policy 2: Restrict OpenAI models to users with the `inference_max` role.

. Create a ConfigMap that uses the `run.yaml` configuration by running the following command:
+
[source,terminal]
----
$ oc create configmap llamastack-custom-config --from-file=run.yaml=run.yaml -n redhat-ods-operator
----

. Create a `llamastack-distribution.yaml` file with the following parameters:
+
[source,yaml]
----
apiVersion: llamastack.io/v1alpha1
kind: LlamaStackDistribution
metadata:
  name: llamastack-distribution
  namespace: redhat-ods-operator
spec:
  replicas: 1
  server:
    distribution:
      name: rh-dev
    containerSpec:
      name: llama-stack
      port: 8321
      env:
        # vLLM Provider Configuration
        - name: VLLM_URL
          value: "https://your-vllm-service:8000/v1"
        - name: VLLM_API_TOKEN
          value: "your-vllm-token"
        - name: VLLM_TLS_VERIFY
          value: "false"
        # OpenAI Provider Configuration
        - name: OPENAI_API_KEY
          value: "your-openai-api-key"
        - name: OPENAI_BASE_URL
          value: "https://api.openai.com/v1"
    userConfig:
      configMapName: llamastack-custom-config
      configMapNamespace: redhat-ods-operator
----

. To apply the distribution, run the following command:
+
[source,terminal]
----
$ oc apply -f llamastack-distribution.yaml
----

. Wait for the distribution to be ready by running the following command:
+
[source,terminal]
----
oc wait --for=jsonpath='{.status.phase}'=Ready llamastackdistribution/llamastack-distribution -n redhat-ods-operator --timeout=300s
----

. Generate the OAuth tokens for each user account to authenticate API requests.

.. To request a basic access token, and to add the token to a `user1.token` file, run the following command:
+
[source,terminal]
----
$ curl -d client_id=llamastack \
  -d client_secret=YOUR_CLIENT_SECRET \
  -d username=user1 \
  -d password=user1-password \
  -d grant_type=password \
  https://my-keycloak-server.com/realms/llamastack-demo/protocol/openid-connect/token \
  | jq -r .access_token > user1.token
----

.. To request full access token and add it to a `user2.token` file, run the following command:
+
[source,terminal]
----
$ curl -d client_id=llamastack \
  -d client_secret=YOUR_CLIENT_SECRET \
  -d username=user2 \
  -d password=user2-password \
  -d grant_type=password \
  https://my-keycloak-server.com/realms/llamastack-demo/protocol/openid-connect/token \
  | jq -r .access_token > user2.token
----

.. Verify the credentials by running the following command:
+
[source,terminal]
----
$ cat user2.token | cut -d . -f 2 | base64 -d 2>/dev/null | jq .
----
