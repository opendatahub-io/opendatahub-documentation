:_module-type: PROCEDURE

[id="auth-on-llama-stack_{context}"]
= Configuring Llama Stack with OAuth Authentication
[role="_abstract"]
You can configure Llama Stack to enable Role-Based Access Control (RBAC) for model access using OAuth authentication on {productname-short}. The following example shows how to configure Llama Stack so that a vLLM model can be accessed by all authenticated users, while an OpenAI model is restricted to specific users. This example uses Keycloak to issue and validate tokens.



This document assumes the Keycloak server is available at `https://my-keycloak-server.com`

[IMPORTANT]
====
When accessing Llama Stack APIs, the required base URL depends on the client you are using.

* **OpenAI-compatible clients or raw HTTP requests**  
  You must include the `/v1` path suffix in the base URL.  
  For example: `http://llama-stack-service:8321/v1`

* **`LlamaStackClient` SDK**  
  Do not include the `/v1` path suffix.  
  For example: `http://llama-stack-service:8321`

Using an incorrect base URL results in request failures.
====

.Prerequisites

* You have installed {openshift-platform} 4.19 or newer.
* You have logged in to {productname-long}.
* You have cluster administrator privileges for your OpenShift cluster.
* You have a Keycloak instance configured with the following settings:
** Realm: `llamastack-demo`
** Client: `llamastack` with direct access grants enabled
** Role: `inference_max`
** A protocol mapper that adds realm roles to the access token under the `llamastack_roles` claim
** Two test users:
*** `user1` with no assigned roles
*** `user2` assigned the `inference_max` role
* You have saved the Keycloak client secret for token requests.
* Your Keycloak server is reachable at `https://my-keycloak-server.com`.
* You have installed the {openshift-cli} as described in the appropriate documentation for your cluster:
ifdef::upstream,self-managed[]
** link:https://docs.redhat.com/en/documentation/openshift_container_platform/{ocp-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli[Installing the OpenShift CLI^] for OpenShift Container Platform
** link:https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws/{rosa-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli[Installing the OpenShift CLI^] for {rosa-productname}
endif::[]
ifdef::cloud-service[]
** link:https://docs.redhat.com/en/documentation/openshift_dedicated/{osd-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli[Installing the OpenShift CLI^] for OpenShift Dedicated
** link:https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws_classic_architecture/{rosa-classic-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli[Installing the OpenShift CLI^] for {rosa-classic-productname}
endif::[]

.Procedure

. To configure Llama Stack to use Role-Based Access Control (RBAC) to access models, you need to view and verify the OAuth provider token structure.

.. Generate a Keycloak test token to view the structure with the following command:
+
[source,terminal]
----
$ curl -d client_id=llamastack -d client_secret=YOUR_CLIENT_SECRET -d username=user1 -d password=user-password -d grant_type=password https://my-keycloak-server.com/realms/llamastack-demo/protocol/openid-connect/token | jq -r .access_token > test.token
----

.. View the token claims by running the following command:
+
[source,terminal]
----
$ cat test.token | cut -d . -f 2 | base64 -d 2>/dev/null | jq .
----
+
.Example token structure from Keycloak
[source,terminal]
----
$ {
  "iss": "http://my-keycloak-server.com/realms/llamastack-demo",
  "aud": "account",
  "sub": "761cdc99-80e5-4506-9b9e-26a67a8566f7",
  "preferred_username": "user1",
  "llamastack_roles": [
    "inference_max"
  ],
}
----

. Update your existing `run.yaml` file by adding the OAuth parameter. 

.Example OAuth parameters in the `run.yaml` file
+
[source,yaml]
----
server:
  port: 8321
  auth:
    provider_config:
      type: "oauth2_token"
      jwks:
        uri: "https://my-keycloak-server.com/realms/llamastack-demo/protocol/openid-connect/certs" <1>
        key_recheck_period: 3600
      issuer: "https://my-keycloak-server.com/realms/llamastack-demo" <1>
      audience: "account"
      verify_tls: true
      claims_mapping:
        llamastack_roles: "roles" <2>
    access_policy:
      - permit: <3>
          actions: [read]
          resource: model::vllm-inference/llama-3-2-3b
        description: Allow all authenticated users to access Llama 3.2 model
      - permit: <4>
          actions: [read]
          resource: model::openai/gpt-4o-mini
        when: user with inference_max in roles
        description: Allow only users with inference_max role to access OpenAI models
----
+
<1> Specify your Keycloak host and Realm in the URL.  
<2> Maps the `llamastack_roles` path from the token to the `roles` field.  
<3> Policy 1: Allow all authenticated users to access vLLM models.  
<4> Policy 2: Restrict OpenAI models to users with the `inference_max` role.

. Create a ConfigMap that uses the updated `run.yaml` configuration by running the following command:
+
[source,terminal]
----
$ oc create configmap llamastack-custom-config --from-file=run.yaml=run.yaml -n redhat-ods-operator
----

. Create a `llamastack-distribution.yaml` file with the following parameters:
+
[source,yaml]
----
apiVersion: llamastack.io/v1alpha1
kind: LlamaStackDistribution
metadata:
  name: llamastack-distribution
  namespace: redhat-ods-operator
spec:
  replicas: 1
  server:
    distribution:
      name: rh-dev
    containerSpec:
      name: llama-stack
      port: 8321
      env:
        # vLLM Provider Configuration
        - name: VLLM_URL
          value: "https://your-vllm-service:8000/v1"
        - name: VLLM_API_TOKEN
          value: "your-vllm-token"
        - name: VLLM_TLS_VERIFY
          value: "false"
        # OpenAI Provider Configuration
        - name: OPENAI_API_KEY
          value: "your-openai-api-key"
        - name: OPENAI_BASE_URL
          value: "https://api.openai.com/v1"
    userConfig:
      configMapName: llamastack-custom-config
      configMapNamespace: redhat-ods-operator
----

. To apply the distribution, run the following command:
+
[source,terminal]
----
$ oc apply -f llamastack-distribution.yaml
----

. Wait for the distribution to be ready by running the following command:
+
[source,terminal]
----
oc wait --for=jsonpath='{.status.phase}'=Ready llamastackdistribution/llamastack-distribution -n redhat-ods-operator --timeout=300s
----

. Generate the OAuth tokens for each user account to authenticate API requests.

.. To request a basic access token, and to add the token to a `user1.token` file, run the following command:
+
[source,terminal]
----
$ curl -d client_id=llamastack \
  -d client_secret=YOUR_CLIENT_SECRET \
  -d username=user1 \
  -d password=user1-password \
  -d grant_type=password \
  https://my-keycloak-server.com/realms/llamastack-demo/protocol/openid-connect/token \
  | jq -r .access_token > user1.token
----

.. To request full access token and add it to a `user2.token` file, run the following command:
+
[source,terminal]
----
$ curl -d client_id=llamastack \
  -d client_secret=YOUR_CLIENT_SECRET \
  -d username=user2 \
  -d password=user2-password \
  -d grant_type=password \
  https://my-keycloak-server.com/realms/llamastack-demo/protocol/openid-connect/token \
  | jq -r .access_token > user2.token
----

.. Verify the credentials by running the following command:
+
[source,terminal]
----
$ cat user2.token | cut -d . -f 2 | base64 -d 2>/dev/null | jq .
----

.Verification
. Set the Llama Stack base URL:
+
[source,terminal]
----
export LLAMASTACK_URL="http://<llama-stack-host>:8321"
----

. Verify basic access for `user1` (no privileged roles).
+
Load the token:
+
[source,terminal]
----
USER1_TOKEN=$(cat user1.token)
----
+
Confirm that `user1` can access the vLLM-served model:
+
[source,terminal]
----
curl -s -o /dev/null -w "%{http_code}\n" \
  -X POST "${LLAMASTACK_URL}/v1/openai/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer ${USER1_TOKEN}" \
  -d '{"model":"vllm-inference/llama-3-2-3b","messages":[{"role":"user","content":"Hello!"}],"max_tokens":50}'
----
+
Expected result: HTTP `200`.
+
Confirm that `user1` is denied access to the restricted OpenAI model:
+
[source,terminal]
----
curl -s -o /dev/null -w "%{http_code}\n" \
  -X POST "${LLAMASTACK_URL}/v1/openai/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer ${USER1_TOKEN}" \
  -d '{"model":"openai/gpt-4o-mini","messages":[{"role":"user","content":"Hello!"}],"max_tokens":50}'
----
+
Expected result: HTTP `403` (forbidden).

. Verify privileged access for `user2` (assigned the `inference_max` role).
+
Load the token:
+
[source,terminal]
----
USER2_TOKEN=$(cat user2.token)
----
+
Confirm that `user2` can access both models:
+
[source,terminal]
----
curl -s -o /dev/null -w "%{http_code}\n" \
  -X POST "${LLAMASTACK_URL}/v1/openai/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer ${USER2_TOKEN}" \
  -d '{"model":"vllm-inference/llama-3-2-3b","messages":[{"role":"user","content":"Hello!"}],"max_tokens":50}'
----
+
[source,terminal]
----
curl -s -o /dev/null -w "%{http_code}\n" \
  -X POST "${LLAMASTACK_URL}/v1/openai/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer ${USER2_TOKEN}" \
  -d '{"model":"openai/gpt-4o-mini","messages":[{"role":"user","content":"Hello!"}],"max_tokens":50}'
----
+
Expected result: HTTP `200` for both requests.

. Verify that requests without a Bearer token are denied.
+
[source,terminal]
----
curl -s -o /dev/null -w "%{http_code}\n" \
  -X POST "${LLAMASTACK_URL}/v1/openai/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{"model":"vllm-inference/llama-3-2-3b","messages":[{"role":"user","content":"Hello!"}],"max_tokens":50}'
----
+
Expected result: HTTP `401` (unauthorized).

