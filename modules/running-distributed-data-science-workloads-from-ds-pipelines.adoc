:_module-type: PROCEDURE

[id="running-distributed-data-science-workloads-from-ds-pipelines_{context}"]
= Running distributed data science workloads from data science pipelines

[role='_abstract']
To run a distributed data science workload from a data science pipeline, you must first update the pipeline to include a link to your Ray cluster image.

.Prerequisites
ifdef::upstream,self-managed[]
* You have logged in to {openshift-platform} with the `cluster-admin` role.
endif::[]
ifdef::cloud-service[]
* You have logged in to OpenShift with the `cluster-admin` role.
endif::[]

ifndef::upstream[]
* You have access to a data science cluster that is configured to run distributed workloads as described in link:{rhoaidocshome}{default-format-url}/working_with_distributed_workloads/configuring-distributed-workloads_distributed-workloads[Configuring distributed workloads].
endif::[]
ifdef::upstream[]
* You have access to a data science cluster that is configured to run distributed workloads as described in link:{odhdocshome}/working-with-distributed-workloads/#configuring-distributed-workloads_distributed-workloads[Configuring distributed workloads].
endif::[]

ifndef::upstream[]
* You have created the required Kueue resources as described in link:{rhoaidocshome}{default-format-url}/working_with_distributed_workloads/configuring-distributed-workloads_distributed-workloads#configuring-quota-management-for-distributed-workloads_distributed-workloads[Configuring quota management for distributed workloads].
endif::[]
ifdef::upstream[]
* You have created the required Kueue resources as described in link:{odhdocshome}/working-with-distributed-workloads/#configuring-quota-management-for-distributed-workloads_distributed-workloads[Configuring quota management for distributed workloads].
endif::[]

ifndef::upstream[]
* Optional: You have defined a _default_ local queue for the Ray cluster by creating a `LocalQueue` resource and adding the following annotation to the configuration details for that `LocalQueue` resource, as described in link:{rhoaidocshome}{default-format-url}/working_with_distributed_workloads/configuring-distributed-workloads_distributed-workloads#configuring-quota-management-for-distributed-workloads_distributed-workloads[Configuring quota management for distributed workloads]:
+
[source,bash]
----
"kueue.x-k8s.io/default-queue": "true"
----
+
[NOTE]
====
If you do not create a default local queue, you must specify a local queue in each pipeline.
====
endif::[]
ifdef::upstream[]
* Optional: You have defined a _default_ local queue for the Ray cluster by creating a `LocalQueue` resource and adding the following annotation to the configuration details for that `LocalQueue` resource, as described in link:{odhdocshome}/working-with-distributed-workloads/#configuring-quota-management-for-distributed-workloads_distributed-workloads[Configuring quota management for distributed workloads]:
+
[source,bash]
----
"kueue.x-k8s.io/default-queue": "true"
----
+
[NOTE]
====
If you do not create a default local queue, you must specify a local queue in each pipeline.
====
endif::[]

ifndef::upstream[]
* You have created a data science project that contains a workbench, and the workbench is running a default notebook image that contains the CodeFlare SDK, for example,the *Standard Data Science* notebook. For information about how to create a project, see link:{rhoaidocshome}/working_on_data_science_projects/working-on-data-science-projects_nb-server#creating-a-data-science-project_nb-server[Creating a data science project].
For a complete list of the default notebook images and their preinstalled packages, see the table in link:{rhoaidocshome}/working_on_data_science_projects/creating-and-importing-notebooks_notebooks#notebook-images-for-data-scientists_notebooks[Notebook images for data scientists].
endif::[]
ifdef::upstream[]
* You have created a data science project that contains a workbench, and the workbench is running a default notebook image that contains the CodeFlare SDK, for example, the *Standard Data Science* notebook. For information about how to create a project, see link:{odhdocshome}/working-on-data-science-projects/#_using_data_science_projects[Creating a data science project].
For a complete list of the default notebook images and their preinstalled packages, see the table in link:{odhdocshome}/working-on-data-science-projects/#_using_data_science_projects[Notebook images for data scientists].
endif::[]

* You have access to S3-compatible object storage.
* You have logged in to {productname-long}.

.Procedure
ifndef::upstream[]
. Create a data connection to connect the object storage to your data science project, as described in link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects/working-on-data-science-projects_nb-server#adding-a-data-connection-to-your-data-science-project_nb-server[Adding a data connection to your data science project].
endif::[]
ifdef::upstream[]
. Create a data connection to connect the object storage to your data science project, as described in link:{odhdocshome}/working-on-data-science-projects/#adding-a-data-connection-to-your-data-science-project_nb-server[Adding a data connection to your data science project].
endif::[]

ifndef::upstream[]
. Configure a pipeline server to use the data connection, as described in link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects/working-with-data-science-pipelines_ds-pipelines#configuring-a-pipeline-server_ds-pipelines[Configuring a pipeline server].
endif::[]
ifdef::upstream[]
. Configure a pipeline server to use the data connection, as described in link:{odhdocshome}/working-on-data-science-projects/#configuring-a-pipeline-server_ds-pipelines[Configuring a pipeline server].
endif::[]

. Create the data science pipeline as follows:
.. Install the `kfp` Python package, which is required for all pipelines:
+
[source,bash]
----
$ pip install kfp
----
.. Install any other dependencies that are required for your pipeline.
.. Build your data science pipeline in Python code.
+
For example, create a file named `compile_example.py` with the following content:
+
[source,Python]
----
from kfp import dsl


@dsl.component(
    base_image="registry.redhat.io/ubi8/python-39:latest",
    packages_to_install=['codeflare-sdk']
)


def ray_fn(openshift_server: str, openshift_token: str):
   import ray <1>
   from codeflare_sdk import Cluster, ClusterConfiguration, TokenAuthentication, generate_cert


   auth = TokenAuthentication( <2>
       token=openshift_token, server=openshift_server
   )
   auth_return = auth.login()


   cluster = Cluster( <3>
       ClusterConfiguration(
           name="raytest",
           num_workers=1,
           head_cpus="500m",
           min_memory=1,
           max_memory=1,
           num_gpus=0,
           image="quay.io/project-codeflare/ray:latest-py39-cu118", <4>
           local_queue="local_queue_name", <5>
       )
   )


   print(cluster.status())
   cluster.up() <6>
   cluster.wait_ready() <7>
   print(cluster.status())
   print(cluster.details())


   ray_dashboard_uri = cluster.cluster_dashboard_uri()
   ray_cluster_uri = cluster.cluster_uri()
   print(ray_dashboard_uri, ray_cluster_uri)

   # Enable Ray client to connect to secure Ray cluster that has mTLS enabled
   generate_cert.generate_tls_cert(cluster.config.name, cluster.config.namespace) <8>
   generate_cert.export_env(cluster.config.name, cluster.config.namespace)


   ray.init(address=ray_cluster_uri)
   print("Ray cluster is up and running: ", ray.is_initialized())


   @ray.remote
   def train_fn(): <9>
       # complex training function
       return 100


   result = ray.get(train_fn.remote())
   assert 100 == result
   ray.shutdown()
   cluster.down() <10>
   auth.logout()
   return result


@dsl.pipeline( <11>
   name="Ray Simple Example",
   description="Ray Simple Example",
)


def ray_integration(openshift_server: str, openshift_token: str): <12>
   ray_fn(openshift_server=openshift_server, openshift_token=openshift_token)


if __name__ == '__main__':
    from kfp.compiler import Compiler
    Compiler().compile(ray_integration, 'compiled-example.yaml')

----
<1> Imports from the CodeFlare SDK the packages that define the cluster functions
<2> Authenticates with the cluster by using values that you specify when creating the pipeline run;
if your cluster uses self-signed certificates, include `ca-cert-path=<path>` in the `TokenAuthentication` parameter list, where `<path>` is the path to the cluster-wide Certificate Authority (CA) bundle that contains the self-signed certificates
// Commenting out second part of callout 2 until RHOAIENG-880 is fixed
//; you can omit this section if the Ray cluster is configured to use the same namespace as the data science project
<3> Specifies the Ray cluster configuration: replace these example values with the values for your Ray cluster
<4> Specifies the location of the Ray cluster image: if using a disconnected environment, replace the default value with the location for your environment
<5> Specifies the local queue to which the Ray cluster will be submitted: you can omit this line if you configured a default local queue
<6> Creates a Ray cluster using the specified image and configuration
<7> Waits for the Ray cluster to be ready before proceeding
<8> Enables the Ray client to connect to a secure Ray cluster that has mutual Transport Layer Security (mTLS) enabled; mTLS is enabled by default in the CodeFlare component in {productname-short}
<9> Replace the example details in this section with the details for your workload
<10> Removes the Ray cluster when your workload is finished
<11> Replace the example name and description with the values for your workload
<12> Compiles the Python code and saves the output in a YAML file

.. Compile the Python file (in this example, the `compile_example.py` file):
+
[source,bash]
----
$ python compile_example.py
----
This command creates a YAML file (in this example, `compiled-example.yaml`), which you can import in the next step.

ifndef::upstream[]
. Import your data science pipeline, as described in link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects/working-with-data-science-pipelines_ds-pipelines#importing-a-data-science-pipeline_ds-pipelines[Importing a data science pipeline].
endif::[]
ifdef::upstream[]
. Import your data science pipeline, as described in link:{odhdocshome}/working-on-data-science-projects/#importing-a-data-science-pipeline_ds-pipelines[Importing a data science pipeline].
endif::[]

ifndef::upstream[]
. Schedule the pipeline run, as described in link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects/working-with-data-science-pipelines_ds-pipelines#scheduling-a-pipeline-run_ds-pipelines[Scheduling a pipeline run].
endif::[]
ifdef::upstream[]
. Schedule the pipeline run, as described in link:{odhdocshome}/working-on-data-science-projects/#scheduling-a-pipeline-run_ds-pipelines[Scheduling a pipeline run].
endif::[]

ifndef::upstream[]
. When the pipeline run is complete, confirm that it is included in the list of triggered pipeline runs, as described in link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects/working-with-data-science-pipelines_ds-pipelines#viewing-the-details-of-a-pipeline-run_ds-pipelines[Viewing the details of a pipeline run].
endif::[]
ifdef::upstream[]
. When the pipeline run is complete, confirm that it is included in the list of triggered pipeline runs, as described in link:{odhdocshome}/working-on-data-science-projects/#viewing-the-details-of-a-pipeline-run_ds-pipelines[Viewing the details of a pipeline run].
endif::[]


.Verification
The YAML file is created and the pipeline run completes without errors.

ifndef::upstream[]
You can view the run details, as described in link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects/working-with-data-science-pipelines_ds-pipelines#viewing-the-details-of-a-pipeline-run_ds-pipelines[Viewing the details of a pipeline run].
endif::[]
ifdef::upstream[]
You can view the run details, as described in link:{odhdocshome}/working-on-data-science-projects/#viewing-the-details-of-a-pipeline-run_ds-pipelines[Viewing the details of a pipeline run].
endif::[]

[role='_additional-resources']
.Additional resources
ifndef::upstream[]
* link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects/working-with-data-science-pipelines_ds-pipelines[Working with data science pipelines]
endif::[]
ifdef::upstream[]
* link:{odhdocshome}/working-on-data-science-projects/#working-with-data-science-pipelines_ds-pipelines[Working with data science pipelines]
endif::[]

* link:https://docs.ray.io/en/latest/cluster/getting-started.html[Ray Clusters documentation]
