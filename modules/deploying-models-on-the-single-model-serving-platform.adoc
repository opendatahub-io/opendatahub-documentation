:_module-type: PROCEDURE

[id="deploying-models-on-the-single-model-serving-platform_{context}"]
= Deploying models on the single-model serving platform

[role='_abstract']
When you have enabled the single-model serving platform, you can enable a pre-installed or custom model-serving runtime and start to deploy models on the platform.

ifdef::upstream[]
NOTE: link:https://github.com/IBM/text-generation-inference[Text Generation Inference Server (TGIS)^] is based on an early fork of link:https://github.com/huggingface/text-generation-inference[Hugging Face TGI^]. Red Hat will continue to develop the standalone TGIS runtime to support TGI models. If a model does not work in the current version of {productname-short}, support might be added in a future version. In the meantime, you can also add your own, custom runtime to support a TGI model. For more information, see link:{odhdocshome}/serving-models/#adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models[Adding a custom model-serving runtime for the single-model serving platform].
endif::[]

ifndef::upstream[]
NOTE: link:https://github.com/IBM/text-generation-inference[Text Generation Inference Server (TGIS)^] is based on an early fork of link:https://github.com/huggingface/text-generation-inference[Hugging Face TGI^]. Red Hat will continue to develop the standalone TGIS runtime to support TGI models. If a model does not work in the current version of {productname-short}, support might be added in a future version. In the meantime, you can also add your own, custom runtime to support a TGI model. For more information, see link:{rhoaidocshome}{default-format-url}/serving_models/serving-large-models_serving-large-models#adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_serving-large-models[Adding a custom model-serving runtime for the single-model serving platform].
endif::[]

.Prerequisites
* You have logged in to {productname-long}.
ifndef::upstream[]
* If you are using {productname-short} groups, you are part of the user group or admin group (for example, {oai-user-group} or {oai-admin-group}) in OpenShift.
endif::[]
ifdef::upstream[]
* If you are using {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group} or {odh-admin-group}) in OpenShift.
endif::[]
* You have installed KServe.
* You have enabled the single-model serving platform.
* You have created a data science project.
* You have access to S3-compatible object storage.
* For the model that you want to deploy, you know the associated folder path in your S3-compatible object storage bucket.
* To use the Caikit-TGIS runtime, you have converted your model to Caikit format. For an example, see link:https://github.com/opendatahub-io/caikit-tgis-serving/blob/main/demo/kserve/built-tip.md#bootstrap-process[Converting Hugging Face Hub models to Caikit format^] in the link:https://github.com/opendatahub-io/caikit-tgis-serving/tree/main[caikit-tgis-serving^] repository.
ifndef::upstream[]
* If you want to use graphics processing units (GPUs) with your model server, you have enabled GPU support in {productname-short}. See link:{rhoaidocshome}{default-format-url}/managing_openshift_ai/enabling_accelerators#enabling-nvidia-gpus_managing-rhoai[Enabling NVIDIA GPUs^].
* To use the vLLM runtime, you have enabled GPU support in {productname-short} and have installed and configured the Node Feature Discovery operator on your cluster. For more information, see link:https://docs.openshift.com/container-platform/{ocp-latest-version}/hardware_enablement/psap-node-feature-discovery-operator.html#installing-the-node-feature-discovery-operator_node-feature-discovery-operator[Installing the Node Feature Discovery operator] and link:{rhoaidocshome}{default-format-url}/managing_openshift_ai/enabling_accelerators#enabling-nvidia-gpus_managing-rhoai[Enabling NVIDIA GPUs^]
endif::[]
ifdef::upstream[]
* To use the vLLM runtime or use graphics processing units (GPUs) with your model server, you have enabled GPU support. This includes installing the Node Feature Discovery and NVIDIA GPU Operators. For more information, see https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/index.html[NVIDIA GPU Operator on {org-name} OpenShift Container Platform^] in the NVIDIA documentation.
endif::[]

ifdef::self-managed[]
[NOTE]
====
In {productname-short} {vernum}, {org-name} supports only NVIDIA GPU accelerators for model serving.
====
endif::[]
ifdef::cloud-service[]
[NOTE]
====
In {productname-short}, {org-name} supports only NVIDIA GPU accelerators for model serving.
====
endif::[]

*  To deploy {rhelai-productname-short} models:
** You have enabled the vLLM runtime.
** You have downloaded the model from the {org-name} container registry and uploaded it to S3-compatible object storage.

.Procedure
. In the left menu, click *Data Science Projects*.
+
The *Data Science Projects* page opens.
. Click the name of the project that you want to deploy a model in.
+
A project details page opens.
. Click the *Models* tab.
. Perform one of the following actions:
+
--
* If you see a *​​Single-model serving platform* tile, click *Deploy model* on the tile.
* If you do not see any tiles, click the *Deploy model* button.
--
+
The *Deploy model* dialog opens.

. In the *Model name* field, enter a unique name for the model that you are deploying.
. In the *Serving runtime* field, select an enabled runtime.
. From the *Model framework* list, select a value.
. In the *Number of model replicas to deploy* field, specify a value.
. From the *Model server size* list, select a value.
. Optional: In the *Model route* section, select the *Make deployed models available through an external route* checkbox to make your deployed models available to external clients.
. To require token authorization for inference requests to the deployed model, perform the following actions:
.. Select *Require token authorization*.
.. In the *Service account name* field, enter the service account name that the token will be generated for.
. To specify the location of your model, perform one of the following sets of actions:
+
--
* *To use an existing data connection*
.. Select *Existing data connection*.
.. From the *Name* list, select a data connection that you previously defined.
.. In the *Path* field, enter the path that contains the model in your specified data source.
ifdef::self-managed,cloud-service[]
+
IMPORTANT: The OpenVINO Model Server runtime has specific requirements for how you specify the model path. For more information, see known issue link:{rhoaidocshome}html-single/release_notes/index#known-issues_RHOAIENG-3025_relnotes[RHOAIENG-3025] in the {productname-short} release notes.
endif::[]

* *To use a new data connection*
.. To define a new data connection that your model can access, select *New data connection*.
.. In the *Name* field, enter a unique name for the data connection.
.. In the *Access key* field, enter the access key ID for your S3-compatible object storage provider.
.. In the *Secret key* field, enter the secret access key for the S3-compatible object storage account that you specified.
.. In the *Endpoint* field, enter the endpoint of your S3-compatible object storage bucket.
.. In the *Region* field, enter the default region of your S3-compatible object storage account.
.. In the *Bucket* field, enter the name of your S3-compatible object storage bucket.
.. In the *Path* field, enter the folder path in your S3-compatible object storage that contains your data file.
ifdef::self-managed,cloud-service[]
+
IMPORTANT: The OpenVINO Model Server runtime has specific requirements for how you specify the model path. For more information, see known issue link:{rhoaidocshome}html-single/release_notes/index#known-issues_RHOAIENG-3025_relnotes[RHOAIENG-3025] in the {productname-short} release notes.
endif::[]
--
. Click *Deploy*.

.Verification
* Confirm that the deployed model is shown on the *Models* tab for the project, and on the *Model Serving* page of the dashboard with a checkmark in the *Status* column.

// [role="_additional-resources"]
// .Additional resources
