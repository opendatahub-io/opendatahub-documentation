:_module-type: PROCEDURE

[id="deploying-models-on-the-single-model-serving-platform_{context}"]
= Deploying models on the single model serving platform

[role='_abstract']
When you have enabled the single model serving platform, you can enable the standalone TGIS or composite Caikit-TGIS runtimes and start to deploy models on the platform.

ifdef::upstream[]
NOTE: link:https://github.com/IBM/text-generation-inference[Text Generation Inference Server (TGIS)^] is based on an early fork of link:https://github.com/huggingface/text-generation-inference[Hugging Face TGI^]. Red Hat will continue to develop the standalone TGIS runtime to support TGI models. If a model does not work in the current version of {productname-short}, support might be added in a future version. In the meantime, you can also add your own, custom runtime to support a TGI model. For more information, see link:{odhdocshome}/serving-models/#adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_serving-large-language-models[Adding a custom model-serving runtime for the single model serving platform].
endif::[]

ifndef::upstream[]
NOTE: link:https://github.com/IBM/text-generation-inference[Text Generation Inference Server (TGIS)^] is based on an early fork of link:https://github.com/huggingface/text-generation-inference[Hugging Face TGI^]. Red Hat will continue to develop the standalone TGIS runtime to support TGI models. If a model does not work in the current version of {productname-short}, support might be added in a future version. In the meantime, you can also add your own, custom runtime to support a TGI model. For more information, see link:{rhoaidocshome}{default-format-url}/serving_models/serving-large-language-models_serving-large-language-models#adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_serving-large-language-models[Adding a custom model-serving runtime for the single model serving platform].
endif::[]

.Prerequisites
* You have logged in to {productname-long}.
ifndef::upstream[]
* If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {oai-user-group} or {oai-admin-group}) in OpenShift.
endif::[]
ifdef::upstream[]
* If you are using specialized {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group} or {odh-admin-group}) in OpenShift.
endif::[]
* You have installed KServe.
* You have enabled the single model serving platform.
* You have created a data science project.
* To use the Caikit-TGIS runtime, you have converted your model to Caikit format. For an example, see link:https://github.com/opendatahub-io/caikit-tgis-serving/blob/main/demo/kserve/built-tip.md#bootstrap-process[Converting Hugging Face Hub models to Caikit format^] in the link:https://github.com/opendatahub-io/caikit-tgis-serving/tree/main[caikit-tgis-serving^] repository.
* You know the folder path for the data connection that you want the model to access.
ifndef::upstream[]
* If you want to use graphics processing units (GPUs) with your model server, you have enabled GPU support in {productname-short}. See link:{rhoaidocshome}{default-format-url}/managing_resources/managing-cluster-resources_cluster-mgmt#enabling-gpu-support_cluster-mgmt[Enabling GPU support in {productname-short}^].
endif::[]
ifdef::upstream[]
* If you want to use graphics processing units (GPUs) with your model server, you have enabled GPU support. This includes installing the Node Feature Discovery and GPU Operators. For more information, see https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/index.html[NVIDIA GPU Operator on {org-name} OpenShift Container Platform^] in the NVIDIA documentation.
endif::[]

.Procedure
. In the left menu, click *Data Science Projects*.
. Click the name of the project that you want to deploy a model in.
. In the *Models and model servers* section, perform one of the following actions:
+
--
* If you see a *​​Single model serving platform* tile, click *Deploy model* on the tile. 
* If you do not see any tiles, click the *Deploy model* button.
--
+
The *Deploy model* dialog opens.
. Configure properties for deploying your model as follows:
.. In the *Model name* field, enter a unique name for the model that you are deploying.
.. In the *Serving runtime* field, select an enabled runtime.
.. From the *Model framework* list, select a value.
.. In the *Number of model replicas to deploy* field, specify a value.
.. From the *Model server size* list, select a value.
.. To specify the location of your model, perform one of the following sets of actions:
+
--
* *To use an existing data connection*
... Select *Existing data connection*.
... From the *Name* list, select a data connection that you previously defined.
... In the *Path* field, enter the folder path that contains the model in your specified data source.

* *To use a new data connection*
... To define a new data connection that your model can access, select *New data connection*.
... In the *Name* field, enter a unique name for the data connection.
... In the *Access key* field, enter the access key ID for your S3-compatible object storage provider.
... In the *Secret key* field, enter the secret access key for the S3-compatible object storage account that you specified.
... In the *Endpoint* field, enter the endpoint of your S3-compatible object storage bucket.
... In the *Region* field, enter the default region of your S3-compatible object storage account.
... In the *Bucket* field, enter the name of your S3-compatible object storage bucket.
... In the *Path* field, enter the folder path in your S3-compatible object storage that contains your data file.
--
.. Click *Deploy*.

.Verification
* Confirm that the deployed model is shown in the *Models and model servers* section of your project, and on the *Model Serving* page of the dashboard with a check mark in the *Status* column.

// [role="_additional-resources"]
// .Additional resources
