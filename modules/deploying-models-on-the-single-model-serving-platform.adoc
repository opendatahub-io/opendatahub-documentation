:_module-type: PROCEDURE

[id="deploying-models-on-the-single-model-serving-platform_{context}"]
= Deploying models on the single-model serving platform

[role='_abstract']

When you have enabled the single-model serving platform, you can enable a preinstalled or custom model-serving runtime and deploy models on the platform.

ifdef::upstream[]
You can use preinstalled model-serving runtimes to start serving models without modifying or defining the runtime yourself. For help adding a custom runtime, see link:{odhdocshome}/configuring-your-model-serving-platform/#adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_odh-admin[Adding a custom model-serving runtime for the single-model serving platform].
endif::[]

ifndef::upstream[]
You can use preinstalled model-serving runtimes to start serving models without modifying or defining the runtime yourself. For help adding a custom runtime, see link:{rhoaidocshome}{default-format-url}/configuring_your_model-serving_platform/configuring_model_servers_on_the_single_model_serving_platform#adding-a-custom-model-serving-runtime-for-the-single-model-serving-platform_rhoai-admin[Adding a custom model-serving runtime for the single-model serving platform].
endif::[]

To successfully deploy a model, you must meet the following prerequisites.

.General prerequisites
* You have logged in to {productname-long}.
* You have installed KServe and enabled the single-model serving platform.
ifdef::self-managed[]
ifndef::disconnected[]
* (Knative Serverless deployments only) To enable token authentication and external model routes for deployed models, you have added Authorino as an authorization provider. For more information, see link:{rhoaidocshome}{default-format-url}/installing_and_uninstalling_{url-productname-short}/installing-the-single-model-serving-platform_component-install#adding-an-authorization-provider_component-install[Adding an authorization provider for the single-model serving platform].
endif::[]
ifdef::disconnected[]
* (Knative Serverless deployments only) To enable token authentication and external model routes for deployed models, you have added Authorino as an authorization provider. For more information, see link:{rhoaidocshome}{default-format-url}/installing_and_uninstalling_{url-productname-short}_in_a_disconnected_environment/installing-the-single-model-serving-platform_component-install#adding-an-authorization-provider_component-install[Adding an authorization provider for the single-model serving platform].
endif::[]
endif::[]
ifdef::cloud-service[]
* (Knative Serverless deployments only) To enable token authentication and external model routes for deployed models, you have added Authorino as an authorization provider. For more information, see link:{rhoaidocshome}{default-format-url}/installing_and_uninstalling_{url-productname-short}/installing-the-single-model-serving-platform_component-install#adding-an-authorization-provider_component-install[Adding an authorization provider for the single-model serving platform].
endif::[]
ifndef::upstream[]
* (Knative Serverless deployments only) To enable token authentication and external model routes for deployed models, you have added Authorino as an authorization provider.
endif::[]
* You have created a data science project.
ifndef::upstream[]
* You have access to S3-compatible object storage, a URI-based repository, an OCI-compliant registry or a persistent volume claim (PVC) and have added a connection to your data science project. For more information about adding a connection, see link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects/using-connections_projects#adding-a-connection-to-your-data-science-project_projects[Adding a connection to your data science project].
* If you want to use graphics processing units (GPUs) with your model server, you have enabled GPU support in {productname-short}. If you use NVIDIA GPUs, see link:{rhoaidocshome}{default-format-url}/managing_openshift_ai/enabling-accelerators#enabling-nvidia-gpus_managing-rhoai[Enabling NVIDIA GPUs^]. If you use AMD GPUs, see link:{rhoaidocshome}{default-format-url}/managing_openshift_ai/enabling-accelerators#amd-gpu-integration_managing-rhoai[AMD GPU integration^].
endif::[]
ifdef::upstream[]
* You have access to S3-compatible object storage, a URI-based repository, an OCI-compliant registry or a persistent volume claim (PVC) and have added a connection to your data science project. For more information about adding a connection, see link:{odhdocshome}/working-on-data-science-projects/#adding-a-connection-to-your-data-science-project_projects[Adding a connection to your data science project].
* If you want to use graphics processing units (GPUs) with your model server, you have enabled GPU support in {productname-short}. If you use NVIDIA GPUs, see link:{odhdocshome}/managing-odh/#enabling-nvidia-gpus_managing-odh[Enabling NVIDIA GPUs^]. If you use AMD GPUs, see link:{odhdocshome}/managing-odh/#amd-gpu-integration_managing-odh[AMD GPU integration^].
endif::[]

.Runtime-specific prerequisites

Meet the requirements for the specific runtime you intend to use.

* **Caikit-TGIS runtime**
** To use the Caikit-TGIS runtime, you have converted your model to Caikit format. For an example, see link:https://github.com/opendatahub-io/caikit-tgis-serving/blob/main/demo/kserve/built-tip.md#bootstrap-process[Converting Hugging Face Hub models to Caikit format^] in the link:https://github.com/opendatahub-io/caikit-tgis-serving/tree/main[caikit-tgis-serving^] repository.

* **vLLM NVIDIA GPU ServingRuntime for KServe**
ifndef::upstream[]
** To use the *vLLM NVIDIA GPU ServingRuntime for KServe* runtime, you have enabled GPU support in {productname-short} and have installed and configured the Node Feature Discovery Operator on your cluster. For more information, see link:https://docs.redhat.com/en/documentation/openshift_container_platform/{ocp-latest-version}/html/specialized_hardware_and_driver_enablement/psap-node-feature-discovery-operator#installing-the-node-feature-discovery-operator_psap-node-feature-discovery-operator[Installing the Node Feature Discovery Operator] and link:{rhoaidocshome}{default-format-url}/managing_openshift_ai/enabling-accelerators#enabling-nvidia-gpus_managing-rhoai[Enabling NVIDIA GPUs^].
endif::[]
ifdef::upstream[]
** To use the *vLLM NVIDIA GPU ServingRuntime for KServe* runtime or use graphics processing units (GPUs) with your model server, you have enabled GPU support. This includes installing the Node Feature Discovery and NVIDIA GPU Operators. For more information, see link:https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/index.html[NVIDIA GPU Operator on {org-name} OpenShift Container Platform^] in the NVIDIA documentation.
** To deploy {rhelai-productname-short} models, use the *vLLM NVIDIA GPU ServingRuntime for KServe* runtime and ensure you have downloaded the model from the {org-name} container registry and uploaded it to S3-compatible object storage.
endif::[]

ifdef::self-managed[]
* **vLLM CPU ServingRuntime for KServe**
** To use the VLLM runtime on IBM Z and IBM Power, use the *vLLM CPU ServingRuntime for KServe*. You cannot use GPU accelerators with IBM Z and IBM Power architectures.
For more information, see link:https://access.redhat.com/solutions/7109527[Red{nbsp}Hat {openshift-platform} Multi Architecture Component Availability Matrix].
endif::[]
ifdef::upstream[]
* **vLLM CPU ServingRuntime for KServe**
** To use the VLLM runtime on IBM Z and IBM Power, use the *vLLM CPU ServingRuntime for KServe*. For IBM Z and IBM Power, vLLM runtime is supported only on CPU.
endif::[]

* **vLLM Intel Gaudi Accelerator ServingRuntime for KServe**
ifndef::upstream[]
** To use the *vLLM Intel Gaudi Accelerator ServingRuntime for KServe* runtime, you have enabled support for hybrid processing units (HPUs) in {productname-short}. This includes installing the Intel Gaudi Base Operator and configuring a hardware profile. For more information, see link:https://docs.habana.ai/en/latest/Installation_Guide/Additional_Installation/OpenShift_Installation/index.html#openshift-installation[Intel Gaudi Base Operator OpenShift installation^] in the AMD documentation and link:{rhoaidocshome}{default-format-url}/working_with_accelerators/working-with-hardware-profiles_accelerators[Working with hardware profiles^].
endif::[]
ifdef::upstream[]
** To use the *vLLM Intel Gaudi Accelerator ServingRuntime for KServe* runtime, you have enabled support for hybrid processing units (HPUs) in {productname-short}. This includes installing the Intel Gaudi Base Operator and configuring a hardware profile. For more information, see link:https://docs.habana.ai/en/latest/Installation_Guide/Additional_Installation/OpenShift_Installation/index.html#openshift-installation[Intel Gaudi Base Operator OpenShift installation^] and link:{odhdocshome}/working-with-accelerators/#working-with-hardware-profiles_accelerators[Working with hardware profiles^].
endif::[]

* **vLLM AMD GPU ServingRuntime for KServe**
ifndef::upstream[]
** To use the *vLLM AMD GPU ServingRuntime for KServe* runtime, you have enabled support for AMD graphic processing units (GPUs) in {productname-short}. This includes installing the AMD GPU operator and configuring a hardware profile. For more information, see link:https://instinct.docs.amd.com/projects/gpu-operator/en/latest/installation/openshift-olm.html[Deploying the AMD GPU operator on OpenShift^] and link:{rhoaidocshome}{default-format-url}/working_with_accelerators/working-with-hardware-profiles_accelerators[Working with hardware profiles^].
endif::[]
ifdef::upstream[]
** To use the *vLLM AMD GPU ServingRuntime for KServe* runtime, you have enabled support for AMD graphic processing units (GPUs) in {productname-short}. This includes installing the AMD GPU Operator and configuring a hardware profile. For more information, see link:https://instinct.docs.amd.com/projects/gpu-operator/en/latest/installation/openshift-olm.html[Deploying the AMD GPU operator on OpenShift^] and link:{odhdocshome}/working-with-accelerators/#working-with-hardware-profiles_accelerators[Working with hardware profiles^].
endif::[]

* **vLLM Spyre AI Accelerator ServingRuntime for KServe**
+
ifndef::upstream[]
[IMPORTANT]
====
ifdef::self-managed[]
Support for IBM Spyre AI Accelerators on x86 is currently available in {productname-long} {vernum} as a Technology Preview feature.
endif::[]
ifdef::cloud-service[]
Support for IBM Spyre AI Accelerators on x86 is currently available in {productname-long} as a Technology Preview feature.
endif::[]
Technology Preview features are not supported with {org-name} production service level agreements (SLAs) and might not be functionally complete.
{org-name} does not recommend using them in production.
These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.

For more information about the support scope of {org-name} Technology Preview features, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].
====
endif::[]
ifndef::upstream[]
** To use the *vLLM Spyre AI Accelerator ServingRuntime for KServe* runtime on x86, you have installed the Spyre Operator and configured a hardware profile. For more information, see link:https://catalog.redhat.com/en/software/containers/ibm-aiu/spyre-operator/688a1121575e62c686a471d4[Spyre operator image] and link:{rhoaidocshome}{default-format-url}/working_with_accelerators/working-with-hardware-profiles_accelerators[Working with hardware profiles^].
endif::[]
ifdef::upstream[]
** To use the *vLLM Spyre AI Accelerator ServingRuntime for KServe* runtime on x86, you have installed the Spyre Operator and configured a hardware profile. For more information, see link:https://catalog.redhat.com/en/software/containers/ibm-aiu/spyre-operator/688a1121575e62c686a471d4[Spyre operator image] and link:{odhdocshome}/working-with-accelerators/#working-with-hardware-profiles_accelerators[Working with hardware profiles^].
endif::[]

.Procedure
. In the left menu, click *Data science projects*.
. Click the name of the project that you want to deploy a model in.
+
A project details page opens.
. Click the *Deployments* tab.
. Click *Select single-model* to deploy your model using single-model serving.
. Click the *Deploy model* button.
+
The *Deploy model* dialog opens.
. In the *Model deployment name* field, enter a unique name for the model that you are deploying.
. In the *Serving runtime* field, select an enabled runtime.
If project-scoped runtimes exist, the *Serving runtime* list includes subheadings to distinguish between global runtimes and project-scoped runtimes.
. From the *Model framework (name - version)* list, select a value if applicable.
ifndef::upstream[]
. From the **Deployment mode** list, select *KServe RawDeployment* or *Knative Serverless*. For more information about deployment modes, see link:{rhoaidocshome}{default-format-url}/deploying_models/deploying_models_on_the_single_model_serving_platform#about-kserve-deployment-modes_rhoai-user[About KServe deployment modes].
endif::[]
ifdef::upstream[]
. From the **Deployment mode** list, select *KServe RawDeployment* or *Knative Serverless*. For more information about deployment modes, see link:{odhdocshome}/deploying-models/#about-kserve-deployment-modes_odh-user[About KServe deployment modes].
endif::[]
. In the *Number of model server replicas to deploy* field, specify a value.
. The following options are only available if you have created a hardware profile:
.. From the *Hardware profile* list, select a hardware profile.
If project-scoped hardware profiles exist, the *Hardware profile* list includes subheadings to distinguish between global hardware profiles and project-scoped hardware profiles.
+
[IMPORTANT]
====
By default, hardware profiles are hidden in the dashboard navigation menu and user interface, while accelerator profiles remain visible. In addition, user interface components associated with the deprecated accelerator profiles functionality are still displayed. If you enable hardware profiles, the *Hardware profiles* list is displayed instead of the *Accelerator profiles* list. To show the *Settings -> Hardware profiles* option in the dashboard navigation menu, and the user interface components associated with hardware profiles, set the `disableHardwareProfiles` value to `false` in the `OdhDashboardConfig` custom resource (CR) in {openshift-platform}. 
ifdef::upstream[]
For more information about setting dashboard configuration options, see link:{odhdocshome}/managing-resources/#customizing-the-dashboard[Customizing the dashboard].
endif::[]
ifndef::upstream[]
For more information about setting dashboard configuration options, see link:{rhoaidocshome}{default-format-url}/managing_resources/customizing-the-dashboard[Customizing the dashboard].
endif::[] 
====
.. Optional: To change these default values, click *Customize resource requests and limit* and enter new minimum (request) and maximum (limit) values. The hardware profile specifies the number of CPUs and the amount of memory allocated to the container, setting the guaranteed minimum (request) and maximum (limit) for both. 
. Optional: In the *Model route* section, select the *Make deployed models available through an external route* checkbox to make your deployed models available to external clients.
. To require token authentication for inference requests to the deployed model, perform the following actions:
.. Select *Require token authentication*.
.. In the *Service account name* field, enter the service account name that the token will be generated for.
.. To add an additional service account, click *Add a service account* and enter another service account name.
. To specify the location of your model, select a *Connection type* that you have added. The *OCI-compliant registry*, *S3 compatible object storage*, and *URI* options are pre-installed connection types. Additional options might be available if your {productname-short} administrator added them.
.. *For S3-compatible object storage*: In the *Path* field, enter the folder path that contains the model in your specified data source.
ifdef::self-managed,cloud-service[]
+
IMPORTANT: The OpenVINO Model Server runtime has specific requirements for how you specify the model path. For more information, see known issue link:{rhoaidocshome}{default-format-url}/release_notes/known-issues_relnotes#known-issues_RHOAIENG-3025_relnotes[RHOAIENG-3025] in the {productname-short} release notes.
endif::[]
.. *For Open Container Image connections*: In the *OCI storage location* field, enter the model URI where the model is located.
+
[NOTE]
====
If you are deploying a registered model version with an existing S3, URI, or OCI data connection, some of your connection details might be autofilled. This depends on the type of data connection and the number of matching connections available in your data science project. For example, if only one matching connection exists, fields like the path, URI, endpoint, model URI, bucket, and region might populate automatically. Matching connections will be labeled as **Recommended**.
====
.. Complete the connection detail fields.
.. Optional: If you have uploaded model files to a persistent volume claim (PVC) and the PVC is attached to your workbench, use the *Existing cluster storage* option to select the PVC and specify the path to the model file.
ifdef::self-managed,cloud-service[]
+
IMPORTANT: If your connection type is an S3-compatible object storage, you must provide the folder path that contains your data file. The OpenVINO Model Server runtime has specific requirements for how you specify the model path. For more information, see known issue link:{rhoaidocshome}{default-format-url}/release_notes/known-issues_relnotes#known-issues_RHOAIENG-3025_relnotes[RHOAIENG-3025] in the {productname-short} release notes.
endif::[]
. (Optional) Customize the runtime parameters in the *Configuration parameters* section:
.. Modify the values in *Additional serving runtime arguments* to define how the deployed model behaves.
.. Modify the values in *Additional environment variables* to define variables in the model's environment.
+
The *Configuration parameters* section shows predefined serving runtime parameters, if any are available.
+
NOTE: Do not modify the port or model serving runtime arguments, because they require specific values to be set. Overwriting these parameters can cause the deployment to fail.
. Click *Deploy*.

.Verification
* Confirm that the deployed model is shown on the *Deployments* tab for the project, and on the *Deployments* page of the dashboard with a checkmark in the *Status* column.

[role="_additional-resources"]
.Additional resources

* link:{rhoaidocshome}{default-format-url}/configuring_your_model-serving_platform/configuring-your-model-serving-platform_rhoai-admin#model-serving-runtimes-for-accelerators_rhoai-admin[Model-serving runtimes for accelerators]
