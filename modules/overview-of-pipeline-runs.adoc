:_module-type: CONCEPT

[id='overview-of-pipeline-runs_{context}']
= Overview of pipeline runs

[role='_abstract']
A pipeline run is a single execution of a data science pipeline. As data scientist, you can use {productname-short} to define, manage, and track executions of a data science pipeline. You can view a record of your data science project's previously executed and scheduled runs from the *Runs* page in the {productname-short} user interface.

Runs are intended for portability. Therefore, you can clone your pipeline runs to reproduce and scale them accordingly, or delete them when you longer require them. You can configure a run to execute only once immediately after creation or on a recurring basis. Recurring runs consist of a copy of a pipeline with all of its parameter values and a run trigger. A run trigger indicates when a recurring run executes. You can define the following run triggers:

* Periodic: used for scheduling runs to execute in intervals.
* Cron: used for scheduling runs as a cron job.

When executed, you can track the run's progress from the run's *Details* page on the {productname-short} user interface. From here, you can view the run's graph, and output artifacts.

A pipeline run can be classified as the following: 

* Scheduled run: A pipeline run scheduled to execute at least once
* Triggered run: A previously executed pipeline run.

Each triggered pipeline run generates logs pertaining to each pipeline step. To help you troubleshoot and audit your pipelines, you can review and analyze these logs by using the log viewer. From here, you can search for specific log messages, stop or start the log for pipeline runs that are in progress, and download the step logs to your local machine.

//[role="_additional-resources"]
//.Additional resources
//*
