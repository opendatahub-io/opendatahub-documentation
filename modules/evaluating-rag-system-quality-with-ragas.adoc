:_module-type: PROCEDURE

[id="evaluating-rag-system-quality-with-ragas_{context}"]
= Evaluating RAG system quality with RAGAS metrics

[role='_abstract']
You can use RAGAS (Retrieval-Augmented Generation Assessment) to measure the quality and reliability of your RAG system by evaluating retrieval quality, answer relevance, and factual consistency. RAGAS provides objective metrics that help you identify areas for improvement in your RAG pipeline.

.Prerequisites
* You have deployed a Llama Stack distribution with the RAGAS evaluation provider enabled, either Inline or Remote provider.
ifdef::upstream[]
For more information, see link:{odhdocshome}/evaluating-rag-systems/configuring-ragas-remote-provider-for-production[Configuring the RAGAS remote provider for production].
endif::[]
ifndef::upstream[]
For more information, see link:{rhoaidocshome}evaluating-rag-systems/setting-up-ragas-inline-provider[Setting up the RAGAS inline provider for development].
endif::[]

* You have a deployed RAG model that you want to evaluate.
ifdef::upstream[]
For more information, see link:{odhdocshome}/working-with-rag/#deploying-a-llama-model-with-kserve_rag[Deploying a Llama model with KServe].
endif::[]
ifndef::upstream[]
For more information, see link:{rhoaidocshome}{default-format-url}/working_with_rag/deploying-a-rag-stack-in-a-data-science-project_rag#Deploying-a-llama-model-with-kserve_rag[Deploying a Llama model with KServe].
endif::[]

* You have access to a workbench or notebook environment where you can run Python code.

.Procedure
. Initialize the Llama Stack client:
+
[source,python]
----
from llama_stack_client import LlamaStackClient

client = LlamaStackClient(base_url="<your_llama_stack_url>")
----

. Create an evaluation dataset in the required format:
+
[source,python]
----
evaluation_data = [
    {
        "user_input": "What is the capital of France?",
        "response": "The capital of France is Paris, a major European city.",
        "retrieved_contexts": [
            "Paris is the capital and most populous city of France. It is located in northern France."
        ],
        "reference": "Paris"
    },
    {
        "user_input": "Who wrote Romeo and Juliet?",
        "response": "William Shakespeare wrote Romeo and Juliet in the 1590s.",
        "retrieved_contexts": [
            "William Shakespeare was an English playwright and poet. Romeo and Juliet is one of his most famous tragedies."
          ],
        "reference": "William Shakespeare"
    },
    {
        "user_input": "What is the boiling point of water?",
        "response": "Water boils at 100 degrees Celsius at sea level.",
        "retrieved_contexts": [
            "The boiling point of water is 100°C (212°F) at standard atmospheric pressure."
        ],
        "reference": "100 degrees Celsius"
    }
]
----
+
[NOTE]
====
The `reference` field is optional but recommended for metrics like `Answer Correctness` and `Answer Similarity`. If you do not have reference answers, you can omit this field and still use metrics like `Faithfulness`, `Answer Relevancy`, `Context Precision`, and `Context Recall`.
====

. Register your evaluation dataset with Llama Stack:
+
[NOTE]
====
You must de-register a dataset if one already exists by running the following code:
`client.datasets.unregister("my-rag-evaluation")
    print(f"Dataset 'my-rag-evaluation' unregistered successfully")`
====
+
[source,python]
----
client.datasets.register(
    dataset_id="my-rag-evaluation",
    dataset_def=dataset,
    provider_id="<your_configured_provider>",
)
----

. Register a benchmark that specifies which RAGAS metrics to evaluate:
+
[source,python]
----
client.benchmarks.register(
    task_id="rag-quality-benchmark",
    dataset_id="my-rag-evaluation",
    scoring_functions=[
        "faithfulness",          # Factual consistency with context
        "answer_relevancy",      # Relevance to the question
        "context_precision",     # Precision of retrieval
        "context_recall",        # Recall of retrieval
        "answer_correctness",    # Accuracy vs ground truth
        "answer_similarity"      # Semantic similarity to ground truth
    ]
)
----
+
[NOTE]
====
You can select a subset of metrics based on your evaluation needs. For example, if you are primarily concerned about hallucination prevention, focus on `faithfulness`. If you want to optimize retrieval, use `context_precision` and `context_recall`.
====

. Run the evaluation job:
+
[source,python]
----
job_response = client.eval.run_eval(
    benchmark_id="rag-quality-benchmark",
    benchmark_config={
        "eval_candidate": {
            "type": "model",
            "model": "ollama/granite3.3:2b",
            "sampling_params": {
                "temperature": 0.1,
                "max_tokens": 100
            }
        },
        "scoring_params": {}
    }
)

print(f"Evaluation job started: {job_response.job_id}")
----

. Check the evaluation status:
+
[source,python]
----
  job_status = client.eval.jobs.status(
      benchmark_id="rag-quality-benchmark",
      job_id=job_response.job_id
  )

  print(f"Job status: {job_status.status}")
----

. Retrieve and analyze the results:
+
[source,python]
----
results = client.eval.jobs.retrieve(
      benchmark_id="rag-quality-benchmark",
      job_id=job_response.job_id
  )

  print(results)
----
+
.Example output
[source,terminal]
----
faithfulness: 0.892
answer_relevancy: 0.845
context_precision: 0.778
context_recall: 0.823
answer_correctness: 0.867
answer_similarity: 0.901
----

.Verification
. Verify that the evaluation completed successfully:
+
[source,python]
----
# Check that all expected metrics are present
expected_metrics = [
    "faithfulness",
    "answer_relevancy",
    "context_precision",
    "context_recall",
    "answer_correctness",
    "answer_similarity"
]

for metric in expected_metrics:
    if metric in results.aggregate_scores:
        print(f"✓ {metric}: {results.aggregate_scores[metric]:.3f}")
    else:
        print(f"✗ {metric}: Not available")
----

. Verify that scores are within expected ranges:
+
All RAGAS metrics return scores between 0.0 and 1.0, where higher scores indicate better performance. Use the following guidelines to interpret your results:
+
** *Faithfulness > 0.8*: High factual consistency, low risk of hallucination
** *Answer Relevancy > 0.7*: Answers are relevant to questions
** *Context Precision > 0.6*: Retrieval is returning relevant documents
** *Context Recall > 0.6*: Retrieval is finding necessary information