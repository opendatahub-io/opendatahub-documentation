:_module-type: PROCEDURE

[id="evaluating-rag-system-quality-with-Ragas_{context}"]
= Evaluating RAG system quality with Ragas metrics

[role='_abstract']
You can use Ragas (Retrieval-Augmented Generation Assessment) to measure the quality and reliability of your RAG system by evaluating retrieval quality, answer relevance, and factual consistency. Ragas provides objective metrics that help you identify areas for improvement in your RAG pipeline. Run the following Ragas evaluation demo in a Jupyter notebook.

.Prerequisites
* You have logged in to {productname-long}.
* You have created a data science project.
* You have created a data science pipeline.
* You have created a secret for your AWS credentials in your project namespace.
* You have deployed a Llama Stack distribution with the Ragas evaluation provider enabled, either Inline or Remote provider.
ifdef::upstream[]
For more information, see link:{odhdocshome}/evaluating-rag-systems/configuring-Ragas-remote-provider-for-production[Configuring the Ragas remote provider for production].
endif::[]
ifndef::upstream[]
For more information, see link:{rhoaidocshome}evaluating-rag-systems/setting-up-Ragas-inline-provider[Setting up the Ragas inline provider for development].
endif::[]
* You have access to a workbench or notebook environment where you can run Python code.

.Procedure
. From the {productname-short} dashboard, click *Data science projects*.
. Click the name of the project that contains the workbench.
. Click the *Workbenches* tab.
. If the status of the workbench is *Running*, skip to the next step. 
+
If the status of the workbench is *Stopped*, in the *Status* column for the workbench, click *Start*. 
+
The *Status* column changes from *Stopped* to *Starting* when the workbench server is starting, and then to *Running* when the workbench has successfully started.
.  Click the open icon (image:images/open.png[The open icon]) next to the workbench.
+ 
Your Jupyter environment window opens.
. On the toolbar, click the *Git Clone* icon and then select *Clone a Repository*.
. In the *Clone a repo* dialog, enter the following URL `https://github.com/trustyai-explainability/llama-stack-provider-ragas.git`                                                         
. In the file browser, select the newly-created `/llama-stack-provider-ragas/demos` folder.
+ 
You see a Jupyter notebook named `basic_demo.ipynb`.
. Double-click the `basic_demo.ipynb` file to launch the Jupyter notebook.
+
The Jupyter notebook opens. You see code examples for the following tasks: 
+
** Run your Llama Stack distribution
** Setup and Imports 
** Llama Stack Client Setup
** Dataset Preparation
** Dataset Registration
** Benchmark Registration
** Evaluation Execution
** Inline vs Remote Side-by-side

. In the Jupyter notebook, run the code cells sequentially through the *Evaluation Execution*.
. Return to the {productname-short} dashboard.
. Click *Data science pipelines*->*Runs*. You might need to refresh the page to see the new evaluation job running.
. Wait for the job to show *Successful*. 
. Return to the workbench and run the *Results Display* cell.
. Inspect the results displayed.