:_module-type: PROCEDURE

[id='requesting-a-lime-explanation-using-CLI_{context}']
= Requesting a LIME explanation by using the CLI

[role='_abstract']
You can use the {openshift-cli} to request a LIME explanation.

.Prerequisites

* Your OpenShift cluster administrator added you as a user to the {openshift-platform} cluster and has installed the TrustyAI service for the project that contains the deployed models.

* You authenticated the TrustyAI service, as described in link:{odhdocshome}/monitoring-data-science-models/#authenticating-trustyai-service_monitor[Authenticating the TrustyAI service].

* You have real-world data from the deployed models.
* You have installed the {openshift-cli} as described in the appropriate documentation for your cluster:
ifdef::upstream,self-managed[]
** link:https://docs.redhat.com/en/documentation/openshift_container_platform/{ocp-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli[Installing the OpenShift CLI^] for OpenShift Container Platform  
** link:https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws/{rosa-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli[Installing the OpenShift CLI^] for {rosa-productname}
endif::[]
ifdef::cloud-service[]
** link:https://docs.redhat.com/en/documentation/openshift_dedicated/{osd-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli[Installing the OpenShift CLI^] for OpenShift Dedicated  
** link:https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws_classic_architecture/{rosa-classic-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli[Installing the OpenShift CLI^] for {rosa-classic-productname}
endif::[]

.Procedure

. Open a new terminal window.
. Follow these steps to log in to your {openshift-platform} cluster:
.. In the upper-right corner of the OpenShift web console, click your user name and select *Copy login command*. 
.. After you have logged in, click *Display token*.
.. Copy the *Log in with this token* command and paste it in the {openshift-cli}.
+
[source,subs="+quotes"]
----
$ oc login --token=__<token>__ --server=__<openshift_cluster_url>__
----

. Set an environment variable to define the external route for the TrustyAI service pod.
+
----
export TRUSTY_ROUTE=$(oc get route trustyai-service -n $NAMESPACE -o jsonpath='{.spec.host}')
----

. Set an environment variable to define the name of your model.
+
----
export MODEL="model-name"
----

. Use `GET /info/inference/ids/${MODEL}` to get a list of all inference IDs within your model inference data set.
+
[source]
----
curl -skv -H "Authorization: Bearer ${TOKEN}" \
   https://${TRUSTY_ROUTE}/info/inference/ids/${MODEL}?type=organic
----
+
You see output similar to the following:
+
[source]
----
[
  {
    "id":"a3d3d4a2-93f6-4a23-aedb-051416ecf84f",
    "timestamp":"2024-06-25T09:06:28.75701201"
  }
]
----

. Set environment variables to define the two latest inference IDs (highest and lowest predictions).
+
[source]
----
export ID_LOWEST=$(curl -s ${TRUSTY_ROUTE}/info/inference/ids/${MODEL}?type=organic | jq -r '.[-1].id')

export ID_HIGHEST=$(curl -s ${TRUSTY_ROUTE}/info/inference/ids/${MODEL}?type=organic | jq -r '.[-2].id')
----

. Use `POST /explainers/local/lime` to request the LIME explanation with the following syntax and payload structure:
+
*Syntax*:
+
----
curl -sk -H "Authorization: Bearer ${TOKEN}" -X POST \
 -H "Content-Type: application/json" \
 -d <payload>
----
+
*Payload structure*:

`PredictionId`:: The inference ID.
`config`:: The configuration for the LIME explanation, including `model` and `explainer` parameters. For more information, see link:https://trustyai.org/docs/main/trustyai-service-api-reference.html#ModelConfig[Model configuration parameters] and link:https://trustyai.org/docs/main/trustyai-service-api-reference.html#LimeExplainerConfig[LIME explainer configuration parameters].

For example:

[source]
----
echo "Requesting LIME for lowest"
curl -s -H "Authorization: Bearer ${TOKEN}" -X POST \
    -H "Content-Type: application/json" \
    -d "{
        \"predictionId\": \"$ID_LOWEST\",
        \"config\": {
            \"model\": { <1>
                \"target\": \"modelmesh-serving:8033\", <2>
                \"name\": \"${MODEL}\",
                \"version\": \"v1\"
            },
            \"explainer\": { <3>
              \"n_samples\": 50,
              \"normalize_weights\": \"false\",
              \"feature_selection\": \"false\"
            }
        }
    }" \
    ${TRUSTYAI_ROUTE}/explainers/local/lime
----

[source]
----
echo "Requesting LIME for highest"
curl -sk -H "Authorization: Bearer ${TOKEN}" -X POST \
    -H "Content-Type: application/json" \
    -d "{
        \"predictionId\": \"$ID_HIGHEST\",
        \"config\": {
            \"model\": { <1>
                \"target\": \"modelmesh-serving:8033\", <2>
                \"name\": \"${MODEL}\",
                \"version\": \"v1\"
            },
            \"explainer\": { <3>
              \"n_samples\": 50,
              \"normalize_weights\": \"false\",
              \"feature_selection\": \"false\"
            }
        }
    }" \
    ${TRUSTYAI_ROUTE}/explainers/local/lime
----
<1> Specifies configuration for the model. For more information about the model configuration options, see link:https://trustyai.org/docs/main/trustyai-service-api-reference.html#ModelConfig[Model configuration parameters].
<2> Specifies the model server service URL. This field only accepts model servers in the same namespace as the TrustyAI service, with or without protocol or port number.
+
* `http[s]://service[:port]`
* `service[:port]`
<3> Specifies the configuration for the explainer. For more information about the explainer configuration parameters, see link:https://trustyai.org/docs/main/trustyai-service-api-reference.html#LimeExplainerConfig[LIME explainer configuration parameters].

//.Verification