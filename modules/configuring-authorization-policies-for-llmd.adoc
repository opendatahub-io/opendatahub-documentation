:_module-type: PROCEDURE

[id="configuring-authorization-policies-for-llmd_{context}"]
= Configuring authorization policies for {llmd}

[role='_abstract']
After enabling authentication for {llmd}, you can configure fine-grained authorization policies to control which ServiceAccounts can access specific `LLMInferenceService` resources. This enables you to implement role-based access control for your inference endpoints.

.Prerequisites

* You have configured {org-name} Connectivity Link for {llmd} as described in _Configuring authentication for {llmd} using {org-name} Connectivity Link_.
* You have enabled authentication for your `LLMInferenceService` as described in _Enabling authentication and authorization for an LLM inference service_.
* You have access to the OpenShift CLI (`oc`).
* You have cluster administrator or equivalent permissions to create authorization policies.

.Procedure

. Create an `AuthPolicy` resource that targets your `LLMInferenceService`:
+
[source,yaml]
----
apiVersion: kuadrant.io/v1beta2
kind: AuthPolicy
metadata:
  name: llm-inference-authz-policy
  namespace: <namespace>
spec:
  targetRef:
    group: serving.kserve.io
    kind: LLMInferenceService
    name: <llm-inference-service-name>
  rules:
    authentication:
      "k8s-service-account-token":
        apiKey:
          selector:
            matchLabels:
              authorino.kuadrant.io/managed-by: authorino
        credentials:
          authorizationHeader:
            prefix: Bearer
    authorization:
      "resource-owners":
        kubernetesSubjectAccessReview:
          user:
            selector: auth.identity.sub
          resourceAttributes:
            namespace:
              value: <namespace>
            group:
              value: serving.kserve.io
            resource:
              value: llminferenceservices
            name:
              value: <llm-inference-service-name>
            verb:
              value: get
----
+
where:

`<namespace>`:: Specifies the namespace where the `LLMInferenceService` is deployed.
`<llm-inference-service-name>`:: Specifies the name of your `LLMInferenceService` resource.
`auth.identity.sub`:: Specifies the selector that validates the ServiceAccount has permission to get the specific `LLMInferenceService` resource.

. Apply the authorization policy:
+
[source,bash]
----
oc apply -f <authz-policy-file>.yaml
----

. Optional: Create a more restrictive policy that allows access only to specific ServiceAccounts:
+
[source,yaml]
----
apiVersion: kuadrant.io/v1beta2
kind: AuthPolicy
metadata:
  name: llm-inference-restricted-policy
  namespace: <namespace>
spec:
  targetRef:
    group: serving.kserve.io
    kind: LLMInferenceService
    name: <llm-inference-service-name>
  rules:
    authentication:
      "k8s-service-account-token":
        apiKey:
          selector:
            matchLabels:
              authorino.kuadrant.io/managed-by: authorino
        credentials:
          authorizationHeader:
            prefix: Bearer
    authorization:
      "allowed-service-accounts":
        patternMatching:
          patterns:
          - selector: auth.identity.sub
            operator: matches
            value: ^system:serviceaccount:<namespace>:(llm-user-1|llm-user-2)$
----
+
where:

`<namespace>`:: Specifies the namespace where the ServiceAccounts are located.
`<llm-inference-service-name>`:: Specifies the name of your `LLMInferenceService` resource.
`llm-user-1|llm-user-2`:: Specifies the allowed ServiceAccount names in a regular expression pattern. Replace with the names of the ServiceAccounts that should have access.

.Verification

* Verify that authorized ServiceAccounts can access the inference service:
+
[source,bash]
----
TOKEN=$(oc create token <authorized-serviceaccount> -n <namespace>)
curl -v https://<inference-endpoint-url>/v1/models \
  -H "Authorization: Bearer ${TOKEN}"
----
+
You should receive a successful response.

* Verify that unauthorized ServiceAccounts are denied access:
+
[source,bash]
----
TOKEN=$(oc create token <unauthorized-serviceaccount> -n <namespace>)
curl -v https://<inference-endpoint-url>/v1/models \
  -H "Authorization: Bearer ${TOKEN}"
----
+
You should receive a `403 Forbidden` response.

[role='_additional-resources']
.Additional resources

* For more information about Connectivity Link authorization policies, see link:https://docs.redhat.com/en/documentation/red_hat_connectivity_link/1.2/html/configuring_and_deploying_gateway_policies_with_connectivity_link/app-developer-workflow_rhcl[Override your Gateway policies for auth and rate limiting^].
* For information about Kubernetes SubjectAccessReview, see link:https://kubernetes.io/docs/reference/access-authn-authz/authorization/[Authorization in Kubernetes^].
