:_module-type: CONCEPT

[id='intel-gaudi-ai-accelerator-integration_{context}']
= Intel Gaudi AI Accelerator integration  

[role='_abstract']
To accelerate your high-performance deep learning models, you can integrate Intel Gaudi AI accelerators into {productname-short}. This integration enables your data scientists to use Gaudi libraries and software associated with Intel Gaudi AI accelerators through custom-configured workbench instances.  

Intel Gaudi AI accelerators offer optimized performance for deep learning workloads, with the latest Gaudi 3 devices providing significant improvements in training speed and energy efficiency. These accelerators are suitable for enterprises running machine learning and AI applications on {productname-short}.  

Before you can enable Intel Gaudi AI accelerators in {productname-short}, you must complete the following steps:

. Install the latest version of the Intel Gaudi AI Accelerator Operator from OperatorHub.  
. Create and configure a custom workbench image for Intel Gaudi AI accelerators. A prebuilt workbench image for Gaudi accelerators is not included in {productname-short}.  
. Manually define and configure an accelerator profile for each Intel Gaudi AI device in your environment.  
+
[IMPORTANT]
====
By default, hardware profiles are hidden from appearing in the dashboard navigation menu and user interface, while accelerator profiles remain visible. In addition, user interface components associated with the deprecated accelerator profiles functionality are still displayed. If you have enabled hardware profiles, create a hardware profile instead of creating an accelerator profile. 
ifdef::upstream[]
link:{odhdocshome}/working-with-accelerators/#working-with-hardware-profiles_accelerators[Working with hardware profiles].
endif::[]
ifndef::upstream[]
link:{rhoaidocshome}{default-format-url}/working_with_accelerators/working-with-hardware-profiles_accelerators[Working with hardware profiles].
endif::[] 
====

{org-name} supports Intel Gaudi devices up to Intel Gaudi 3. The Intel Gaudi 3 accelerators, in particular, offer the following benefits:

* Improved training throughput: Reduce the time required to train large models by using advanced tensor processing cores and increased memory bandwidth.  
* Energy efficiency: Lower power consumption while maintaining high performance, reducing operational costs for large-scale deployments.  
* Scalable architecture: Scale across multiple nodes for distributed training configurations.  

Your OpenShift platform must support EC2 DL1 instances to use Intel Gaudi AI accelerators in an Amazon EC2 DL1 instance. You can use Intel Gaudi AI accelerators in workbench instances or model serving after you enable the accelerators, create a custom workbench image, and configure the accelerator profile.

To identify the Intel Gaudi AI accelerators present in your deployment, use the `lspci` utility. For more information, see link:https://linux.die.net/man/8/lspci[lspci(8) - Linux man page].  

[IMPORTANT]
====
The presence of Intel Gaudi AI accelerators in your deployment, as indicated by the `lspci` utility, does not guarantee that the devices are ready to use. You must ensure that all installation and configuration steps are completed successfully.
====  

[role="_additional-resources"]
.Additional resources  
* link:https://linux.die.net/man/8/lspci[lspci(8) - Linux man page]  
* link:https://aws.amazon.com/ec2/instance-types/dl1/[Amazon EC2 DL1 Instances]  
* link:https://docs.habana.ai/en/latest/Installation_Guide/Additional_Installation/OpenShift_Installation/index.html[Intel Gaudi AI Operator OpenShift installation]
* link:https://access.redhat.com/solutions/4870701[What version of the Kubernetes API is included with each OpenShift 4.x release?]  
