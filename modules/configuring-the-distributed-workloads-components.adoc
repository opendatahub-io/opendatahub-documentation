:_module-type: PROCEDURE

[id="configuring-the-distributed-workloads-components_{context}"]
= Configuring the distributed workloads components

[role='_abstract']
To configure the distributed workloads feature for your data scientists to use in {productname-short}, you must enable several components.

.Prerequisites
ifdef::upstream,self-managed[]
* You have logged in to {openshift-platform} with the `cluster-admin` role.
endif::[]
ifdef::cloud-service[]
* You have logged in to OpenShift with the `cluster-admin` role.
endif::[]

* You have access to the data science cluster.
* You have installed {productname-long}.

ifdef::cloud-service[]
* You have sufficient resources. In addition to the minimum {productname-short} resources described in link:{rhoaidocshome}{default-format-url}/installing_and_uninstalling_{url-productname-short}/installing-and-deploying-openshift-ai_install[Installing and deploying {productname-short}], you need 1.6 vCPU and 2 GiB memory to deploy the distributed workloads infrastructure.
endif::[]
ifdef::self-managed[]
* You have sufficient resources. In addition to the minimum {productname-short} resources described in link:{rhoaidocshome}{default-format-url}/installing_and_uninstalling_{url-productname-short}/installing-and-deploying-openshift-ai_install[Installing and deploying {productname-short}] (for disconnected environments, see link:{rhoaidocshome}{default-format-url}/installing_and_uninstalling_{url-productname-short}_in_a_disconnected_environment/deploying-openshift-ai-in-a-disconnected-environment_install[Deploying {productname-short} in a disconnected environment]), you need 1.6 vCPU and 2 GiB memory to deploy the distributed workloads infrastructure.
endif::[]
ifdef::upstream[]
* You have sufficient resources. In addition to the minimum {productname-short} resources described in link:{odhdocshome}/installing-open-data-hub/#installing-the-odh-operator-v2_installv2[Installing the {productname-short} Operator version 2], you need 1.6 vCPU and 2 GiB memory to deploy the distributed workloads infrastructure.
endif::[]

ifndef::upstream[]
* You have created a data science project that contains a workbench, and the workbench is running a default notebook image that contains the CodeFlare SDK, for example,the *Standard Data Science* notebook. For information about how to create a project, see link:{rhoaidocshome}/working_on_data_science_projects/working-on-data-science-projects_nb-server#creating-a-data-science-project_nb-server[Creating a data science project].
For a complete list of the default notebook images and their preinstalled packages, see the table in link:{rhoaidocshome}/working_on_data_science_projects/creating-and-importing-notebooks_notebooks#notebook-images-for-data-scientists_notebooks[Notebook images for data scientists].
endif::[]
ifdef::upstream[]
* You have created a data science project that contains a workbench, and the workbench is running a default notebook image that contains the CodeFlare SDK, for example, the *Standard Data Science* notebook. For information about how to create a project, see link:{odhdocshome}/working-on-data-science-projects/#_using_data_science_projects[Creating a data science project].
For a complete list of the default notebook images and their preinstalled packages, see the table in link:{odhdocshome}/working-on-data-science-projects/#_using_data_science_projects[Notebook images for data scientists].
endif::[]

* You have access to a Ray cluster image. For information about how to create a Ray cluster, see the link:https://docs.ray.io/en/latest/cluster/getting-started.html[Ray Clusters documentation].
+
[NOTE]
====
Mutual Transport Layer Security (mTLS) is enabled by default in the CodeFlare component in {productname-short}.
In the current {productname-short} version, `submissionMode=K8sJobMode` is not supported in the Ray job specification, so the KubeRay Operator cannot create a submitter Kubernetes Job to submit the Ray job.
Instead, users must configure the Ray job specification to set `submissionMode=HTTPMode` only, so that the KubeRay Operator sends a request to the RayCluster to create a Ray job.
====
* You have access to the data sets and models that the distributed workload uses.
* You have access to the Python dependencies for the distributed workload.

ifndef::upstream[]
* You have removed any previously installed instances of the CodeFlare Operator, as described in the Knowledgebase solution link:https://access.redhat.com/solutions/7043796[How to migrate from a separately installed CodeFlare Operator in your data science cluster].
endif::[]
ifdef::upstream[]
* You have removed any previously installed instances of the CodeFlare Operator.
endif::[]

ifndef::upstream[]
* If you want to use graphics processing units (GPUs), you have enabled GPU support in {productname-short}.
See link:{rhoaidocshome}{default-format-url}/managing_resources/managing-cluster-resources_cluster-mgmt#enabling-gpu-support_cluster-mgmt[Enabling GPU support in {productname-short}].
endif::[]
ifdef::upstream[]
* If you want to use graphics processing units (GPUs), you have enabled GPU support.
This process includes installing the Node Feature Discovery Operator and the NVIDIA GPU Operator.
For more information, see https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/index.html[NVIDIA GPU Operator on {org-name} OpenShift Container Platform^] in the NVIDIA documentation.
endif::[]

ifdef::cloud-service[]
* If you want to use self-signed certificates, you have added them to a central Certificate Authority (CA) bundle as described in link:{rhoaidocshome}{default-format-url}/installing_and_uninstalling_{url-productname-short}/working-with-certificates_certs[Working with certificates].
No additional configuration is necessary to use those certificates with distributed workloads.
The centrally configured self-signed certificates are automatically available in the workload pods at the following mount points:
** Cluster-wide CA bundle:
+
[source,bash]
----
/etc/pki/tls/certs/odh-trusted-ca-bundle.crt
/etc/ssl/certs/odh-trusted-ca-bundle.crt
----
** Custom CA bundle:
+
[source,bash]
----
/etc/pki/tls/certs/odh-ca-bundle.crt
/etc/ssl/certs/odh-ca-bundle.crt
----
endif::[]
ifdef::self-managed[]
* If you want to use self-signed certificates, you have added them to a central Certificate Authority (CA) bundle as described in link:{rhoaidocshome}{default-format-url}/installing_and_uninstalling_{url-productname-short}/working-with-certificates_certs[Working with certificates] (for disconnected environments, see link:{rhoaidocshome}{default-format-url}/installing_and_uninstalling_{url-productname-short}_in_a_disconnected_environment/working-with-certificates_certs[Working with certificates]).
No additional configuration is necessary to use those certificates with distributed workloads.
The centrally configured self-signed certificates are automatically available in the workload pods at the following mount points:
** Cluster-wide CA bundle:
+
[source,bash]
----
/etc/pki/tls/certs/odh-trusted-ca-bundle.crt
/etc/ssl/certs/odh-trusted-ca-bundle.crt
----
** Custom CA bundle:
+
[source,bash]
----
/etc/pki/tls/certs/odh-ca-bundle.crt
/etc/ssl/certs/odh-ca-bundle.crt
----
endif::[]
ifdef::upstream[]
* If you want to use self-signed certificates, you have added them to a central Certificate Authority (CA) bundle as described in link:{odhdocshome}/installing-open-data-hub/#understanding-certificates_certs[Understanding certificates in {productname-short}].
No additional configuration is necessary to use those certificates with distributed workloads.
The centrally configured self-signed certificates are automatically available in the workload pods at the following mount points:
** Cluster-wide CA bundle:
+
[source,bash]
----
/etc/pki/tls/certs/odh-trusted-ca-bundle.crt
/etc/ssl/certs/odh-trusted-ca-bundle.crt
----
** Custom CA bundle:
+
[source,bash]
----
/etc/pki/tls/certs/odh-ca-bundle.crt
/etc/ssl/certs/odh-ca-bundle.crt
----
endif::[]

.Procedure
ifdef::upstream,self-managed[]
. In the {openshift-platform} console, click *Operators* -> *Installed Operators*.
endif::[]
ifdef::cloud-service[]
. In the OpenShift console, click *Operators* -> *Installed Operators*.
endif::[]

ifdef::self-managed,cloud-service[]
. Search for the *Red Hat OpenShift AI* Operator, and then click the Operator name to open the Operator details page.
endif::[]
ifdef::upstream[]
. Search for the *Open Data Hub Operator*, and then click the Operator name to open the Operator details page.
endif::[]

. Click the *Data Science Cluster* tab.
. Click the default instance name (for example, *default-dsc*) to open the instance details page.
. Click the *YAML* tab to show the instance specifications.
. Enable the required distributed workloads components.
In the `spec:components` section, set the `managementState` field correctly for the required components.
The list of required components depends on whether the distributed workload is run from a pipeline or notebook or both, as shown in the following table.
+
.Components required for distributed workloads
[cols="34,20,20,26"]
|===
|Component | Pipelines only | Notebooks only | Pipelines and notebooks

|`codeflare`
|`Managed`
|`Managed`
|`Managed`

|`dashboard`
|`Managed`
|`Managed`
|`Managed`

|`datasciencepipelines`
|`Managed`
|`Removed`
|`Managed`

|`kueue`
|`Managed`
|`Managed`
|`Managed`

|`ray`
|`Managed`
|`Managed`
|`Managed`

|`workbenches`
|`Removed`
|`Managed`
|`Managed`
|===

. Click *Save*.
After a short time, the components with a `Managed` state are ready.


.Verification
Check the status of the *codeflare-operator-manager*, *kuberay-operator*, and *kueue-controller-manager* pods, as follows:

ifdef::cloud-service[]
. In the OpenShift console, from the *Project* list, select *redhat-ods-applications*.
endif::[]
ifdef::self-managed[]
. In the {openshift-platform} console, from the *Project* list, select *redhat-ods-applications*.
endif::[]
ifdef::upstream[]
. In the {openshift-platform} console, from the *Project* list, select *odh*.
endif::[]

. Click *Workloads* -> *Deployments*.
. Search for the *codeflare-operator-manager*, *kuberay-operator*, and *kueue-controller-manager* deployments.
In each case, check the status as follows:
.. Click the deployment name to open the deployment details page.
.. Click the *Pods* tab.
.. Check the pod status.
+
When the status of the *codeflare-operator-manager-_<pod-id>_*, *kuberay-operator-_<pod-id>_*, and *kueue-controller-manager-_<pod-id>_* pods is *Running*, the pods are ready to use.
.. To see more information about each pod, click the pod name to open the pod details page, and then click the *Logs* tab.
