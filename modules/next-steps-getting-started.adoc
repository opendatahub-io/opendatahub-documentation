:_module-type: CONCEPT

[id="next-steps_{context}"]
= Next steps

[role="_abstract"]
The following product documentation provides more information on how to develop, test, and deploy data science solutions with {productname-short}.

ifndef::upstream[]
Try the end-to-end tutorial::
link:{rhoaidocshome}{default-format-url}/openshift_ai_tutorial_-_fraud_detection_example[{productname-short} tutorial - Fraud detection example]
+
Step-by-step guidance to complete the following tasks with an example fraud detection model:	

* Explore a pre-trained fraud detection model by using a Jupyter notebook. 		
* Deploy the model by using {productname-short} model serving. 				
* Refine and train the model by using automated pipelines.
endif::[]

Develop and train a model in your workbench IDE::
ifndef::upstream[]
link:{rhoaidocshome}{default-format-url}/working_in_your_data_science_ide/[Working in your data science IDE]
endif::[]
ifdef::upstream[]
link:{odhdocshome}/working-in-your-data-science-ide/[Working in your data science IDE]
endif::[]
+
Learn how to access your workbench IDE (JupyterLab, code-server, or RStudio Server).
+
For the JupyterLab IDE, learn about the following tasks:

* Creating and importing Jupyter notebooks
* Using Git to collaborate on Jupyter notebooks
* Viewing and installing Python packages 
* Troubleshooting common problems

Automate your ML workflow with pipelines::
ifndef::upstream[]
link:{rhoaidocshome}{default-format-url}/working_with_data_science_pipelines/[Working with data science pipelines]
endif::[]
ifdef::upstream[]
link:{odhdocshome}/working-with-data-science-pipelines/[Working with data science pipelines]
endif::[]
+
Enhance your data science projects on {productname-short} by building portable machine learning (ML) workflows with data science pipelines, by using Docker containers. Use pipelines for continuous retraining and updating of a model based on newly received data. 

Deploy and test a model::
ifndef::upstream[]
link:{rhoaidocshome}{default-format-url}/deploying_models/[Deploying models]
endif::[]
ifdef::upstream[]
link:{odhdocshome}/deploying-models/[Deploying models]
endif::[]
+
Deploy your ML models on your OpenShift cluster to test and then integrate them into intelligent applications. When you deploy a model, it is available as a service that you can access by using API calls. You can return predictions based on data inputs that you provide through API calls. 

Monitor and manage models::
ifndef::upstream[]
link:{rhoaidocshome}{default-format-url}/deploying_models/[Deploying models]
endif::[]
ifdef::upstream[]
link:{odhdocshome}/deploying-models/[Deploying models]
endif::[]
+
The {productname-long} service includes model deployment options for hosting the model on Red Hat OpenShift Dedicated or Red Hat Openshift Service on AWS for integration into an external application.

Add accelerators to optimize performance::
ifndef::upstream[]
link:{rhoaidocshome}{default-format-url}/working_with_accelerators/[Working with accelerators]
endif::[]
ifdef::upstream[]
link:{odhdocshome}/working-with-accelerators/[Working with accelerators]
endif::[]
+
If you work with large data sets, you can use accelerators, such as NVIDIA GPUs, AMD GPUs, and Intel Gaudi AI accelerators, to optimize the performance of your data science models in {productname-short}. With accelerators, you can scale your work, reduce latency, and increase productivity. 

Implement distributed workloads for higher performance::
ifndef::upstream[]
link:{rhoaidocshome}{default-format-url}/working_with_distributed_workloads/[Working with distributed workloads]
endif::[]
ifdef::upstream[]
link:{odhdocshome}/working-with-distributed-workloads/[Working with distributed workloads]
endif::[]
+
Implement distributed workloads to use multiple cluster nodes in parallel for faster, more efficient data processing and model training. 			

Explore extensions::
ifndef::upstream[]
link:{rhoaidocshome}{default-format-url}/working_with_connected_applications/[Working with connected applications]
endif::[]
ifdef::upstream[]
link:{odhdocshome}/working-with-connected-applications/[Working with connected applications]
endif::[]
+
Extend your core {productname-short} solution with integrated third-party applications. Several leading AI/ML software technology partners, including Starburst, Intel AI Tools, and IBM are also available through https://catalog.redhat.com/en/partners[Red Hat partners] and https://www.ibm.com/partnerplus/directory/companies[IBM Partner Plus Directory].


== Additional resources

ifndef::upstream[]
In addition to product documentation, Red Hat provides a rich set of learning resources for {productname-short} and supported applications. 
endif::[]

On the *Resources* page of the {productname-short} dashboard, you can use the category links to filter the resources for various stages of your data science workflow. For example, click the *Model serving* category to display resources that describe various methods of deploying models. Click *All items* to show the resources for all categories. 

For the selected category, you can apply additional options to filter the available resources. For example, you can filter by type, such as how-to articles, quick starts, or tutorials; these resources provide the answers to common questions. 

ifndef::upstream[]
For information about {productname-long} support requirements and limitations, see link:https://access.redhat.com/articles/rhoai-supported-configs[{productname-long}: Supported Configurations].
endif::[]