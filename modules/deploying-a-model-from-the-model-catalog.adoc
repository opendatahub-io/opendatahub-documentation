:_module-type: PROCEDURE

[id='deploying-a-model-from-the-model-catalog_{context}']
= Deploying a model from the model catalog

[role='_abstract']
You can deploy models directly from the model catalog. 

[NOTE]
====
{productname-short} model serving deployments use the global cluster pull secret to pull models in ModelCar format from the catalog. 

ifdef::upstream,self-managed[]
For more information about using pull secrets in {openshift-platform}, see link:https://docs.redhat.com/en/documentation/openshift_container_platform/{ocp-latest-version}/html/images/managing-images#images-update-global-pull-secret_using-image-pull-secrets[Updating the global cluster pull secret] in the {openshift-platform} documentation.
endif::[]
====

.Prerequisites
ifdef::upstream[]
* You have completed the prerequisites in link:{odhdocshome}/deploying-models/#deploying-models-on-the-model-serving-platform_odh-user[Deploying models].
endif::[]
ifndef::upstream[]
* You have completed the prerequisites in link:{rhoaidocshome}{default-format-url}/deploying_models/deploying_models#deploying-models-on-the-model-serving-platform_rhoai-user[Deploying models].
endif::[]
ifdef::upstream[]
* The model registry component is enabled in your {productname-short} deployment. For more information, see link:{odhdocshome}/working-with-model-registries/#enabling-the-model-registry-component_model-registry[Enabling the model registry component].
endif::[]
ifdef::self-managed[]
* The model registry component is enabled in your {productname-short} deployment. For more information, see link:{rhoaidocshome}{default-format-url}/enabling_the_model_registry_component[Enabling the model registry component].
endif::[]


.Procedure
. From the {productname-short} dashboard, click *AI hub -> Catalog*.
. The *Catalog* page provides a high-level view of available models, including the model category, name, description, and labels such as task, license, and provider.
. You can use the search bar to search by model name, description, or provider.
. You can use the filter menu to search and select filters by task, provider, or license. 
. Click the name of a model to view the model details page.
. Click *Deploy model* to display the *Deploy a model* wizard.
+
[NOTE] 
====
For models in the catalog, the *Model details* section displays read-only information about the model such as the model location URI and the model type of generative AI.
====
. In the *Model deployment* section, configure the deployment as follows:
.. From the *Project* list, select the project in which to deploy your model.
.. In the *Model deployment name* field, enter a unique name for your model deployment. This field is autofilled with the model name by default. 
+
This is the name of the inference service created when the model is deployed.
.. Optional: Click *Edit resource name*, and enter a specific resource name in the *Resource name* field. By default, the resource name matches the name of the model deployment.
+
[IMPORTANT]
====
Resource names are what your resources are labeled as in OpenShift. Your resource name cannot exceed 253 characters, must consist of lowercase alphanumeric characters or '-', and must start and end with an alphanumeric character. Resource names are not editable after creation.

The resource name must not match the name of any other model deployment resource in your {openshift-platform} cluster.
====
.. In the *Description* field, enter a description of your deployment.
.. From the *Hardware profile* list, select a hardware profile. Models provided in the catalog use the `default-profile`.
.. Optional: To modify the default resource allocation, click *Customize resource requests and limits* and enter new values for the CPU and memory requests and limits. 
.. In the *Serving runtime* field, select an enabled runtime.
+
[NOTE]
====
If project-scoped runtimes exist, the *Serving runtime* list includes subheadings to distinguish between global runtimes and project-scoped runtimes.
====
// Not applicable yet. Support for predictive AI models might be added to catalog in a future release.
//.. Optional: For predictive AI models only, you can select a framework from the *Model framework (name - version)* list. This field is not displayed for generative AI models.  
.. In the *Number of model server replicas to deploy* field, specify a value.
.. Click *Next*.
. In the *Advanced settings* section, configure the following options:
** Select the *Add as AI asset endpoint* checkbox if you want to add your gen AI model endpoint to the *Gen AI studio -> AI asset endpoints* page.  
*** In the *Use case* field, enter the types of tasks that your model performs, such as chat, multimodal, or natural language processing.
+
[NOTE]
====
You must add your model as an AI asset endpoint to test your model on the *Gen AI studio -> playground* page.
====
** To require token authentication for inference requests to the deployed model, select *Require token authentication*.
** In the *Service account name* field, enter the service account name that the token will be generated for.
*** To add an additional service account, click *Add a service account* and enter another service account name.
** In the *Configuration parameters* section:
*** Select *Add custom runtime arguments*, and then enter arguments in the text field.
*** Select *Add custom runtime environment variables*, and then click *Add variable* to enter custom variables in the text field.
. In the *Review* section, review the settings that you have selected before deploying the model. 
. Click *Deploy model*.

.Verification
* The model deployment is displayed in the following places in the dashboard: 
** The *AI hub* -> *Deployments* page.
** The *Latest deployments* section of the model details page.
** The *Deployments* tab for the model version.

[role="_additional-resources"]
.Additional resources
ifdef::upstream[]
* link:{odhdocshome}/deploying-models/#deploying-models-on-the-model-serving-platform_odh-user[Deploying models on the model serving platform].
endif::[]
ifndef::upstream[]
* link:{rhoaidocshome}{default-format-url}/deploying_models/deploying_models#deploying-models-on-the-model-serving-platform_rhoai-user[Deploying models on the model serving platform].
endif::[]
