:_module-type: ASSEMBLY

ifdef::context[:parent-context: {context}]

:context: distributed-llm-roce

[id="configuring-roce-networking-for-distributed-llm-deployments_{context}"]
= Configuring RoCE networking for distributed LLM deployments

High-performance distributed large language model (LLM) deployments require low-latency, high-bandwidth GPU-to-GPU communication across pods. RDMA over Converged Ethernet (RoCE) with GPUDirect RDMA (GDR) provides this capability by enabling direct memory access between GPUs without CPU involvement.

RoCE (RDMA over Converged Ethernet) is a network protocol that enables RDMA communication over Ethernet networks. When combined with NVIDIA GPUDirect RDMA, it provides:

* *High bandwidth*: Up to 400 Gbps on modern network infrastructure
* *Low latency*: Sub-microsecond latency for GPU-to-GPU transfers
* *CPU offload*: Direct GPU-to-network-to-GPU transfers without CPU involvement
* *Scalability*: Efficient multi-node distributed inference and training

RoCE networking is useful for:

*Disaggregated prefill and decode serving*::
Separates initial token generation, or prefill, from subsequent generation, or decode, across different GPU pools. RoCE enables high-speed KV cache transfers between stages, improving throughput and resource use.

*Wide Expert Parallel (WideEP)*::
Distributes expert layers of Mixture-of-Experts (MoE) models such as Mixtral across multiple GPUs and nodes. RoCE provides the low-latency communication required for expert routing and token processing.

*Multi-node tensor parallelism*::
Splits large models across multiple nodes with tensor parallelism. RoCE reduces communication strain for all-reduce operations during inference.

*Distributed training*::
Enables efficient gradient synchronization across nodes for large-scale model training.

A complete RoCE deployment for LLM inference includes:

* *NVIDIA GPU Operator*: Manages GPU drivers, device plugins, and monitoring
* *Node Feature Discovery (NFD) Operator*: Detects hardware capabilities on cluster nodes
* *SR-IOV network Operator* (bare metal) or *Cluster network* (IBM Cloud): Configures secondary high-speed networks
* *NCCL (NVIDIA Collective Communications Library)*: Provides optimized multi-GPU communication primitives
* *Distributed Inference Server with llm-d*: Kubernetes-native framework for serving LLMs

include::enabling-roce-for-distributed-llm-deployments.adoc[leveloffset=+1]

include::troubleshooting-roce-networking.adoc[leveloffset=+1]

include::optimizing-roce-performance-for-llm-deployments.adoc[leveloffset=+1]

include::validating-roce-networking-performance.adoc[leveloffset=+1]

== Next steps

ifdef::upstream[]
* link:{odhdocshome}/monitoring-distributed-workloads[Monitoring distributed workloads]
* link:{odhdocshome}/customize-models-to-build-gen-ai-applications[Customizing models to build gen AI applications]
* link:{odhdocshome}/managing-and-monitoring-models[Managing and monitoring models]
endif::[]

* Experiment with different parallelization strategies for your specific models
* Monitor performance metrics to optimize configuration
* Scale your deployment based on workload requirements

[role='_additional-resources']
== Additional resources

* link:https://docs.openshift.com/container-platform/latest/networking/hardware_networks/about-sriov.html[About SR-IOV hardware networks]
* link:https://docs.openshift.com/container-platform/latest/operators/operator-reference.html#node-feature-discovery-operator[Node Feature Discovery Operator]
* link:https://docs.nvidia.com/networking/display/rdmacore/RDMA+Core[NVIDIA RDMA Core Documentation]
* link:https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/index.html[NCCL User Guide]
* link:https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/index.html[NVIDIA GPU Operator Documentation]
* link:https://docs.nvidia.com/datacenter/cloud-native/network-operator/latest/index.html[NVIDIA Network Operator Documentation]
* link:https://cloud.ibm.com/docs/containers?topic=containers-cluster-network[IBM Cloud cluster network for NVIDIA accelerated computing]
* link:https://www.ibm.com/blog/announcement/ibm-cloud-cluster-network-for-nvidia-accelerated-computing/[Announcing IBM Cloud cluster network]
ifdef::upstream[]
* link:{odhdocshome}/deploying-models[Deploying models]
* link:{odhdocshome}/working-with-accelerators[Working with accelerators]
* link:{odhdocshome}/working-with-distributed-workloads[Working with distributed workloads]
endif::[]

ifdef::parent-context[:context: {parent-context}]
ifndef::parent-context[:!context:]
