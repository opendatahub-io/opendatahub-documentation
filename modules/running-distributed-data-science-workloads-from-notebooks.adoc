:_module-type: PROCEDURE

[id="running-distributed-data-science-workloads-from-notebooks_{context}"]
= Running distributed data science workloads from notebooks

[role='_abstract']
To run a distributed workload from a notebook, you must configure a Ray cluster.
You must also provide environment-specific information such as cluster authentication details.

In the examples in this procedure, you edit the demo notebooks to provide the required information.

.Prerequisites
ifndef::upstream[]
* You can access a data science cluster that is configured to run distributed workloads as described in link:{rhoaidocshome}{default-format-url}/working_with_distributed_workloads/configuring-distributed-workloads_distributed-workloads[Configuring distributed workloads].
endif::[]
ifdef::upstream[]
* You can access a data science cluster that is configured to run distributed workloads as described in link:{odhdocshome}/working-with-distributed-workloads/#configuring-distributed-workloads_distributed-workloads[Configuring distributed workloads].
endif::[]

ifndef::upstream[]
* Your cluster administrator has created the required Kueue resources as described in link:{rhoaidocshome}{default-format-url}/working_with_distributed_workloads/configuring-distributed-workloads_distributed-workloads#configuring-quota-management-for-distributed-workloads_distributed-workloads[Configuring quota management for distributed workloads].
endif::[]
ifdef::upstream[]
* Your cluster administrator has created the required Kueue resources as described in link:{odhdocshome}/working-with-distributed-workloads/#configuring-quota-management-for-distributed-workloads_distributed-workloads[Configuring quota management for distributed workloads].
endif::[]

* You can access the following software from your data science cluster:
** A Ray cluster image that is compatible with your hardware architecture
** The data sets and models to be used by the workload
** The Python dependencies for the workload, either in a Ray image or in your own Python Package Index (PyPI) server

ifndef::upstream[]
* You can access a data science project that contains a workbench, and the workbench is running a default notebook image that contains the CodeFlare SDK, for example, the *Standard Data Science* notebook. 
For information about projects and workbenches, see link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects[Working on data science projects].
endif::[]
ifdef::upstream[]
* You can access a data science project that contains a workbench, and the workbench is running a default notebook image that contains the CodeFlare SDK, for example, the *Standard Data Science* notebook. 
For information about projects and workbenches, see link:{odhdocshome}/working-on-data-science-projects[Working on data science projects].
endif::[]

* You have Admin access for the data science project.
** If you created the project, you automatically have Admin access. 
** If you did not create the project, your cluster administrator must give you Admin access.

* You have logged in to {productname-long}.
* You have launched your notebook server and logged in to your notebook editor.
The examples in this procedure refer to the JupyterLab integrated development environment (IDE).

ifndef::upstream[]
* You have downloaded the demo notebooks provided by the CodeFlare SDK, as described in link:{rhoaidocshome}{default-format-url}/working_with_distributed_workloads/running-distributed-workloads_distributed-workloads#downloading-the-demo-notebooks-from-the-codeflare-sdk_distributed-workloads[Downloading the demo notebooks from the CodeFlare SDK].
endif::[]
ifdef::upstream[]
* You have downloaded the demo notebooks provided by the CodeFlare SDK, as described in link:{odhdocshome}/working_with_distributed_workloads/#downloading-the-demo-notebooks-from-the-codeflare-sdk_distributed-workloads[Downloading the demo notebooks from the CodeFlare SDK].
endif::[]


.Procedure
. Check whether your cluster administrator has defined a _default_ local queue for the Ray cluster:

.. In the OpenShift web console, select your project from the *Project* list.
.. Click *Search*, and from the *Resources* list, select *LocalQueue* to show the list of local queues for your project.
+
If no local queue is listed, contact your cluster administrator.
.. Review the details of each local queue: 
... Click the local queue name. 
... Click the *YAML* tab, and review the `metadata.annotations` section. 
+
If the `kueue.x-k8s.io/default-queue` annotation is set to `'true'`, the queue is configured as the default local queue. 
+
[NOTE]
====
If your cluster administrator does not define a default local queue, you must specify a local queue in each notebook.
====

. In the JupyterLab interface, open the *demo-notebooks > guided-demos* folder. 
. Open all of the notebooks by double-clicking each notebook file.
+
Notebook files have the `.ipynb` file name extension.
. In each notebook, ensure that the `import` section imports the required components from the CodeFlare SDK, as follows:
+
.Example import section
[source,bash]
----
from codeflare_sdk import Cluster, ClusterConfiguration, TokenAuthentication
----

. In each notebook, update the `TokenAuthentication` section to provide the `token` and `server` details to authenticate to the OpenShift cluster by using the CodeFlare SDK.
+
You can find your token and server details as follows:

.. In the {productname-short} top navigation bar, click the application launcher icon (image:images/osd-app-launcher.png[The application launcher]) and then click *OpenShift Console* to open the OpenShift web console.
.. In the upper-right corner of the OpenShift web console, click your user name and select *Copy login command*. 
.. After you have logged in, click *Display Token*.
.. In the *Log in with this token* section, find the required values as follows:
* The `token` value is the text after the `--token=` prefix.
* The `server` value is the text after the `--server=` prefix.

+
[NOTE]
====
The `token` and `server` values are security credentials, treat them with care.

* Do not save the token and server details in a notebook. 
* Do not store the token and server details in Git.

The token expires after 24 hours.
====

. Optional: If you want to use custom certificates, update the `TokenAuthentication` section to add the `ca_cert_path` parameter to specify the location of the custom certificates, as shown in the following example:
+
.Example authentication section
[source,bash]
----
auth = TokenAuthentication(
    token = "XXXXX",
    server = "XXXXX",
    skip_tls=False,
    ca_cert_path="/path/to/cert"
)
auth.login()
----
+
Alternatively, you can set the `CF_SDK_CA_CERT_PATH` environment variable to specify the location of the custom certificates.

. In each notebook, update the cluster configuration section as follows:

.. If the `namespace` value is specified, replace the example value with the name of your project.
+
If you omit this line, the Ray cluster is created in the current project. 

.. If the `image` value is specified, replace the example value with a link to your Ray cluster image.
+
If you omit this line, the default Ray cluster image `quay.io/modh/ray:2.35.0-py39-cu121` is used.
The default Ray image is compatible with NVIDIA GPUs that are supported by CUDA 12.1.
This image is an AMD64 image, which might not work on other architectures. 
+
An additional Ray cluster image `quay.io/modh/ray:2.35.0-py39-rocm61` is available as Developer Preview software.
This image is compatible with AMD accelerators that are supported by ROCm 6.1. 
This image is an AMD64 image, which might not work on other architectures.
+
[IMPORTANT]
====
The ROCm-compatible Ray cluster image is Developer Preview software only. 
Developer Preview software is not supported by Red{nbsp}Hat in any way and is not functionally complete or production-ready. 
Do not use Developer Preview software for production or business-critical workloads. 
Developer Preview software provides early access to upcoming product software in advance of its possible inclusion in a Red{nbsp}Hat product offering. 
Customers can use this software to test functionality and provide feedback during the development process. 
This software might not have any documentation, is subject to change or removal at any time, and has received limited testing. 
Red{nbsp}Hat might provide ways to submit feedback on Developer Preview software without an associated SLA.

For more information about the support scope of Red{nbsp}Hat Developer Preview software, see link:https://access.redhat.com/support/offerings/devpreview/[Developer Preview Support Scope].
====

.. If your cluster administrator has not configured a default local queue, specify the local queue for the Ray cluster, as shown in the following example:
+
.Example local queue assignment
[source,bash,subs="+quotes"]
----
local_queue="_your_local_queue_name_"
----

.. Optional: Assign a dictionary of `labels` parameters to the Ray cluster for identification and management purposes, as shown in the following example:
+
.Example labels assignment
[source,bash,subs="+quotes"]
----
labels = {"exampleLabel1": "exampleLabel1Value", "exampleLabel2": "exampleLabel2Value"}
----

. In the `2_basic_interactive.ipynb` notebook, ensure that the following Ray cluster authentication code is included after the Ray cluster creation section.
+
.Ray cluster authentication code
[source,bash,subs="+quotes"]
----
from codeflare_sdk import generate_cert
generate_cert.generate_tls_cert(cluster.config.name, cluster.config.namespace)
generate_cert.export_env(cluster.config.name, cluster.config.namespace)
----
+
[NOTE]
====
Mutual Transport Layer Security (mTLS) is enabled by default in the CodeFlare component in {productname-short}.
You must include the Ray cluster authentication code to enable the Ray client that runs within a notebook to connect to a secure Ray cluster that has mTLS enabled.
====

. Run the notebooks in the order indicated by the file-name prefix (`0_`, `1_`, and so on).

ifndef::upstream[]
.. In each notebook, run each cell in turn, and review the cell output.
.. If an error is shown, review the output to find information about the problem and the required corrective action. 
For example, replace any deprecated parameters as instructed.
See also link:{rhoaidocshome}{default-format-url}/working_with_distributed_workloads/troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads[Troubleshooting common problems with distributed workloads for users].
endif::[]
ifdef::upstream[]
.. In each notebook, run each cell in turn, and review the cell output.
..If an error is shown, review the output to find information about the problem and the required corrective action. 
For example, replace any deprecated parameters as instructed.
See also link:{odhdocshome}/working_with_distributed_workloads/#troubleshooting-common-problems-with-distributed-workloads-for-users_distributed-workloads[Troubleshooting common problems with distributed workloads for users].
endif::[]

.Verification
. The notebooks run to completion without errors. 
. In the notebooks, the output from the `cluster.status()` function or `cluster.details()` function indicates that the Ray cluster is `Active`.

////
[role='_additional-resources']
.Additional resources
<Do we want to link to additional resources?>


* link:https://url[link text]
////
