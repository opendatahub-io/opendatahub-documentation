:_module-type: PROCEDURE

[id="running-distributed-data-science-workloads-from-notebooks_{context}"]
= Running distributed data science workloads from notebooks

[role='_abstract']
To run a distributed data science workload from a notebook, you must first provide the link to your Ray cluster image.

.Prerequisites
ifndef::upstream[]
* You have access to a data science cluster that is configured to run distributed workloads as described in link:{rhoaidocshome}{default-format-url}/working_with_distributed_workloads/configuring-distributed-workloads_distributed-workloads[Configuring distributed workloads].
endif::[]
ifdef::upstream[]
* You have access to a data science cluster that is configured to run distributed workloads as described in link:{odhdocshome}/working-with-distributed-workloads/#configuring-distributed-workloads_distributed-workloads[Configuring distributed workloads].
endif::[]

ifndef::upstream[]
* Your cluster administrator has created the required Kueue resources as described in link:{rhoaidocshome}{default-format-url}/working_with_distributed_workloads/configuring-distributed-workloads_distributed-workloads#configuring-quota-management-for-distributed-workloads_distributed-workloads[Configuring quota management for distributed workloads].
endif::[]
ifdef::upstream[]
* Your cluster administrator has created the required Kueue resources as described in link:{odhdocshome}/working-with-distributed-workloads/#configuring-quota-management-for-distributed-workloads_distributed-workloads[Configuring quota management for distributed workloads].
endif::[]

ifndef::upstream[]
* Optional: Your cluster administrator has defined a _default_ local queue for the Ray cluster by creating a `LocalQueue` resource and adding the following annotation to its configuration details, as described in link:{rhoaidocshome}{default-format-url}/working_with_distributed_workloads/configuring-distributed-workloads_distributed-workloads#configuring-quota-management-for-distributed-workloads_distributed-workloads[Configuring quota management for distributed workloads]:
+
[source,bash]
----
"kueue.x-k8s.io/default-queue": "true"
----
+
[NOTE]
====
If your cluster administrator does not define a default local queue, you must specify a local queue in each notebook.
====
endif::[]
ifdef::upstream[]
* Optional: Your cluster administrator has defined a _default_ local queue for the Ray cluster by creating a `LocalQueue` resource and adding the following annotation to its configuration details, as described in link:{odhdocshome}/working-with-distributed-workloads/#configuring-quota-management-for-distributed-workloads_distributed-workloads[Configuring quota management for distributed workloads]:
+
[source,bash]
----
"kueue.x-k8s.io/default-queue": "true"
----
+
[NOTE]
====
If your cluster administrator does not define a default local queue, you must specify a local queue in each notebook.
====
endif::[]

ifndef::upstream[]
* You have created a data science project that contains a workbench, and the workbench is running a default notebook image that contains the CodeFlare SDK, for example, the *Standard Data Science* notebook. 
For information about how to create a project, see link:{rhoaidocshome}/working_on_data_science_projects/working-on-data-science-projects_nb-server#creating-a-data-science-project_nb-server[Creating a data science project].
For a complete list of the default notebook images and their preinstalled packages, see the table in link:{rhoaidocshome}/working_on_data_science_projects/creating-and-importing-notebooks_notebooks#notebook-images-for-data-scientists_notebooks[Notebook images for data scientists].
endif::[]
ifdef::upstream[]
* You have created a data science project that contains a workbench, and the workbench is running a default notebook image that contains the CodeFlare SDK, for example, the *Standard Data Science* notebook. 
For information about how to create a project, see link:{odhdocshome}/working-on-data-science-projects/#_using_data_science_projects[Creating a data science project].
For a complete list of the default notebook images and their preinstalled packages, see the table in link:{odhdocshome}/working-on-data-science-projects/#_using_data_science_projects[Notebook images for data scientists].
endif::[]

* You have Admin access for the data science project.
** If you created the project, you automatically have Admin access. 
** If you did not create the project, your cluster administrator must give you Admin access.

* You have launched your notebook server and logged in to your notebook editor.
The examples in this procedure refer to the JupyterLab integrated development environment (IDE).

.Procedure
. Download the demo notebooks provided by the CodeFlare SDK.
The demo notebooks provide guidelines for how to use the CodeFlare stack in your own notebooks.
+
To access the demo notebooks, clone the `codeflare-sdk` repository as follows:

.. In the JupyterLab interface, click *Git > Clone a Repository*.
.. In the "Clone a repo" dialog, enter `https://github.com/project-codeflare/codeflare-sdk.git` and then click *Clone*.
The *codeflare-sdk* repository is listed in the left navigation pane.
. Run a distributed workload job as shown in the following example:
.. In the JupyterLab interface, in the left navigation pane, double-click *codeflare-sdk*.
.. Double-click *demo-notebooks*, and then double-click *guided-demos*.
.. Update each example demo notebook as follows:
** If not already specified, update the `import` section to import the `generate_cert` component:
+
.Updated import section
[source,bash]
----
from codeflare_sdk import generate_cert
----

** Replace the default namespace value with the name of your data science project.
** In the `TokenAuthentication` section of your notebook code, provide the token and server details to authenticate to the OpenShift cluster by using the CodeFlare SDK.
** Replace the link to the example community image with a link to your Ray cluster image.
** Ensure that the following Ray cluster authentication code is included after the Ray cluster creation section.
+
.Ray cluster authentication code
[source,bash,subs="+quotes"]
----
generate_cert.generate_tls_cert(cluster.config.name, cluster.config.namespace)
generate_cert.export_env(cluster.config.name, cluster.config.namespace)
----
+
[NOTE]
====
Mutual Transport Layer Security (mTLS) is enabled by default in the CodeFlare component in {productname-short}.
You must include the Ray cluster authentication code to enable the Ray client that runs within a notebook to connect to a secure Ray cluster that has mTLS enabled.
====


ifndef::upstream[]
** If you have not configured a default local queue by including the `kueue.x-k8s.io/default-queue: 'true'` annotation as described in link:{rhoaidocshome}{default-format-url}/working_with_distributed_workloads/configuring-distributed-workloads_distributed-workloads#configuring-quota-management-for-distributed-workloads_distributed-workloads[Configuring quota management for distributed workloads], update the `ClusterConfiguration` section to specify the local queue for the Ray cluster, as shown in the following example:
+
.Example local queue assignment
[source,bash,subs="+quotes"]
----
local_queue="_your_local_queue_name_"
----
endif::[]
ifdef::upstream[]
** If you have not configured a default local queue by including the `kueue.x-k8s.io/default-queue: 'true'` annotation as described in link:{odhdocshome}/working-with-distributed-workloads/#configuring-quota-management-for-distributed-workloads_distributed-workloads[Configuring quota management for distributed workloads], update the `ClusterConfiguration` section to specify the local queue for the Ray cluster, as shown in the following example:
+
.Example local queue assignment
[source,bash]
----
local_queue="your_local_queue_name"
----
endif::[]

.. Run the notebooks.


.Verification
The notebooks run to completion without errors. In the notebooks, the output from the `cluster.status()` function or `cluster.details()` function indicates that the Ray cluster is `Active`.

////
[role='_additional-resources']
.Additional resources
<Do we want to link to additional resources?>


* link:https://url[link text]
////
