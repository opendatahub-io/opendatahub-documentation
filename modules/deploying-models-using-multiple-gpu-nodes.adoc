:_module-type: PROCEDURE

[id="deploying-models-using-multiple-gpu-nodes_{context}"]
= Deploying models by using multiple GPU nodes

[role='_abstract']
Deploy models across multiple GPU nodes to handle large models, such as large language models (LLMs).

You can serve models on {productname-long} across multiple GPU nodes using the vLLM serving framework. Multi-node inferencing uses the `vllm-multinode-runtime` custom runtime, which uses the same image as the *vLLM NVIDIA GPU ServingRuntime for KServe* runtime and also includes information necessary for multi-GPU inferencing.

You can deploy the model from a persistent volume claim (PVC) or from an Open Container Initiative (OCI) container image.

ifndef::upstream[]
[IMPORTANT]
====
*Deploying models by using multiple GPU nodes* is currently available in {productname-long} as a Technology Preview feature only. Technology Preview features are not supported with {org-name} production service level agreements (SLAs) and might not be functionally complete. {org-name} does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.

For more information about the support scope of {org-name} Technology Preview features, see link:https://access.redhat.com/support/offerings/techpreview[Technology Preview Features Support Scope]
====
endif::[]

.Prerequisites

* You have cluster administrator privileges for your {openshift-platform} cluster.

* You have installed the {openshift-cli} as described in the appropriate documentation for your cluster:
ifdef::upstream,self-managed[]
** link:https://docs.redhat.com/en/documentation/openshift_container_platform/{ocp-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli[Installing the OpenShift CLI^] for OpenShift Container Platform  
** link:https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws/{rosa-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli[Installing the OpenShift CLI^] for {rosa-productname}
endif::[]
ifdef::cloud-service[]
** link:https://docs.redhat.com/en/documentation/openshift_dedicated/{osd-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli[Installing the OpenShift CLI^] for OpenShift Dedicated  
** link:https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws_classic_architecture/{rosa-classic-latest-version}/html/cli_tools/openshift-cli-oc#installing-openshift-cli[Installing the OpenShift CLI^] for {rosa-classic-productname}
endif::[]
ifndef::upstream[]
* You have enabled the operators for your GPU type, such as Node Feature Discovery Operator, NVIDIA GPU Operator. For more information about enabling accelerators, see link:{rhoaidocshome}{default-format-url}/managing_openshift_ai/enabling-accelerators[Enabling accelerators^].
endif::[]
ifdef::upstream[]
* You have enabled the operators for your GPU type, such as Node Feature Discovery Operator, NVIDIA GPU Operator. For more information about enabling accelerators, see link:{odhdocshome}/working-with-accelerators[Working with accelerators^].
endif::[]

** You are using an NVIDIA GPU (`nvidia.com/gpu`).
** You have specified the GPU type through either the `ServingRuntime` or `InferenceService`. If the GPU type specified in the `ServingRuntime` differs from what is set in the `InferenceService`, both GPU types are assigned to the resource and can cause errors. 
* You have enabled KServe on your cluster.
* You have only one head pod in your setup. Do not adjust the replica count using the `min_replicas` or `max_replicas` settings in the `InferenceService`. Creating additional head pods can cause them to be excluded from the Ray cluster.	
* *To deploy from a PVC*: You have a persistent volume claim (PVC) set up and configured for ReadWriteMany (RWX) access mode.
* *To deploy from an OCI container image*:
** You have stored a model in an OCI container image.
** If the model is stored in a private OCI repository, you have configured an image pull secret.

.Procedure
. In a terminal window, if you are not already logged in to your {openshift-platform} cluster as a cluster administrator, log in to the {openshift-platform} CLI as shown in the following example:
+
[source]
----
$ oc login <openshift_cluster_url> -u <admin_username> -p <password>
----
+

. Select or create a namespace for deploying the model. For example, run the following command to create the `kserve-demo` namespace:
+
[source]
----
oc new-project kserve-demo
----
+

. *(Deploying a model from a PVC only)* Create a PVC for model storage in the namespace where you want to deploy the model. Create a storage class using `Filesystem volumeMode` and use this storage class for your PVC. The storage size must be larger than the size of the model files on disk. For example:
+
NOTE: If you have already configured a PVC or are deploying a model from an OCI container image, you can skip this step.
+
[source]
----
kubectl apply -f - 
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: granite-8b-code-base-pvc
spec:
  accessModes:
    - ReadWriteMany
  volumeMode: Filesystem
  resources:
    requests:
      storage: <model size>
  storageClassName: <storage class>
----
+

.. Create a pod to download the model to the PVC you created. Update the sample YAML with your bucket name, model path, and credentials:
+
[source]
----
apiVersion: v1
kind: Pod
metadata:
  name: download-granite-8b-code
  labels:
    name: download-granite-8b-code
spec:
  volumes:
    - name: model-volume
      persistentVolumeClaim:
        claimName: granite-8b-code-base-pvc
  restartPolicy: Never
  initContainers:
    - name: fix-volume-permissions
      image: quay.io/quay/busybox@sha256:92f3298bf80a1ba949140d77987f5de081f010337880cd771f7e7fc928f8c74d 
      command: ["sh"]
      args: ["-c", "mkdir -p /mnt/models/$(MODEL_PATH) && chmod -R 777 /mnt/models"] <1>
      volumeMounts:
        - mountPath: "/mnt/models/"
          name: model-volume
      env:
        - name: MODEL_PATH
          value: <model path> <2>
  containers:
    - resources:
        requests:
          memory: 40Gi
      name: download-model
      imagePullPolicy: IfNotPresent
      image: quay.io/opendatahub/kserve-storage-initializer:v0.14 <3>
      args:
        - 's3://$(BUCKET_NAME)/$(MODEL_PATH)/' 
        - /mnt/models/$(MODEL_PATH)
      env:
        - name: AWS_ACCESS_KEY_ID
          value: <id> <4>
        - name: AWS_SECRET_ACCESS_KEY
          value: <secret> <5>
        - name: BUCKET_NAME
          value: <bucket_name> <6>
        - name: MODEL_PATH
          value: <model path> <2>
        - name: S3_USE_HTTPS
          value: "1"
        - name: AWS_ENDPOINT_URL
          value: <AWS endpoint> <7>
        - name: awsAnonymousCredential
          value: 'false'
        - name: AWS_DEFAULT_REGION
          value: <region> <8>
        - name: S3_VERIFY_SSL
          value: 'true' <9>
      volumeMounts:
        - mountPath: "/mnt/models/"
          name: model-volume
----
<1> The `chmod` operation is permitted only if your pod is running as root. Remove`chmod -R 777` from the arguments if you are not running the pod as root.
<2> Specify the path to the model.
ifndef::upstream[]
<3> The value for `containers.image`, located in your `InferenceService`. To access this value, run the following command: `oc get configmap inferenceservice-config -n pass:attributes[{operator-default-namespace}] -oyaml | grep kserve-storage-initializer:`
endif::[]
ifdef::upstream[]
<3> The value for `containers.image`, located in your `download-model` container. To access this value, run the following command: `oc get configmap inferenceservice-config -n opendatahub -oyaml | grep kserve-storage-initializer:`
endif::[]
<4> The access key ID to your S3 bucket.
<5> The secret access key to your S3 bucket.
<6> The name of your S3 bucket.
<7> The endpoint to your S3 bucket.
<8> The region for your S3 bucket if using an AWS S3 bucket. If using other S3-compatible storage, such as ODF or Minio, you can remove the `AWS_DEFAULT_REGION` environment variable.
<9> If you encounter SSL errors, change `S3_VERIFY_SSL` to `false`.
+

. Create the `vllm-multinode-runtime` custom runtime in your project namespace:
+
ifndef::upstream[]
[source]
----
oc process vllm-multinode-runtime-template -n redhat-ods-applications|oc apply -n kserve-demo -f -
----
endif::[]
ifdef::upstream[]
[source]
----
oc process vllm-multinode-runtime-template -n opendatahub|oc apply  -f -
----
endif::[]
+

. Deploy the model using the following `InferenceService` configuration:
+
[source]
----
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  annotations:
    serving.kserve.io/deploymentMode: RawDeployment
    serving.kserve.io/autoscalerClass: external
  name: <inference service name>
spec:
  predictor:
    model:
      modelFormat:
        name: vLLM
      runtime: vllm-multinode-runtime
      storageUri: <storage_uri_path> <1>
    workerSpec: {} <2>
----
+
<1> Specify the path to your model based on your deployment method:
+
* *For PVC*: `pvc://<pvc_name>/<model_path>`
* *For an OCI container image*: `oci://<registry_host>/<org_or_username>/<repository_name><tag_or_digest>`
<2> The following configuration can be added to the `InferenceService`:
+
* `workerSpec.tensorParallelSize`: Determines how many GPUs are used per node. The GPU type count in both the head and worker node deployment resources is updated automatically. Ensure that the value of `workerSpec.tensorParallelSize` is at least `1`.
* `workerSpec.pipelineParallelSize`: Determines how many nodes are used to balance the model in deployment. This variable represents the total number of nodes, including both the head and worker nodes. Ensure that the value of `workerSpec.pipelineParallelSize` is at least `2`. Do not modify this value in production environments.
+
NOTE: You may need to specify additional arguments, depending on your environment and model size.
+

. Deploy the model by applying the `InferenceService` configuration:
+
[source]
----
oc apply -f <inference-service-file.yaml>
----

.Verification

To confirm that you have set up your environment to deploy models on multiple GPU nodes, check the GPU resource status, the `InferenceService` status, the Ray cluster status, and send a request to the model.

* Check the GPU resource status:

** Retrieve the pod names for the head and worker nodes:
+
[source]
----
# Get pod name
podName=$(oc get pod -l app=isvc.granite-8b-code-base-pvc-predictor --no-headers|cut -d' ' -f1)
workerPodName=$(oc get pod -l app=isvc.granite-8b-code-base-pvc-predictor-worker --no-headers|cut -d' ' -f1)

oc wait --for=condition=ready pod/${podName} --timeout=300s
# Check the GPU memory size for both the head and worker pods:
echo "### HEAD NODE GPU Memory Size"
kubectl exec $podName -- nvidia-smi
echo "### Worker NODE GPU Memory Size"
kubectl exec $workerPodName -- nvidia-smi
----
+

.Sample response
+
[source]
----
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A10G                    On  |   00000000:00:1E.0 Off |                    0 |
|  0%   33C    P0             71W /  300W |19031MiB /  23028MiB <1>|      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
         ...                                                               
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A10G                    On  |   00000000:00:1E.0 Off |                    0 |
|  0%   30C    P0             69W /  300W |18959MiB /  23028MiB <2>|      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+        
----
+
Confirm that the model loaded properly by checking the values of <1> and <2>. If the model did not load, the value of these fields is `0MiB`.

* Verify the status of your `InferenceService` using the following command:
ifndef::upstream[]
NOTE: In the Technology Preview, you can only use port forwarding for inferencing.
endif::[]
+
[source]
----
oc wait --for=condition=ready pod/${podName} -n $DEMO_NAMESPACE --timeout=300s
export MODEL_NAME=granite-8b-code-base-pvc
----
+

.Sample response
[source]
----
   NAME                 URL                                                   READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION                          AGE
   granite-8b-code-base-pvc   http://granite-8b-code-base-pvc.default.example.com   
----

* Send a request to the model to confirm that the model is available for inference:
+
[source]
----
oc wait --for=condition=ready pod/${podName} -n vllm-multinode --timeout=300s

oc port-forward $podName 8080:8080 &

curl http://localhost:8080/v1/completions \
       -H "Content-Type: application/json" \
       -d "{
            'model': "$MODEL_NAME",
            'prompt': 'At what temperature does Nitrogen boil?',
            'max_tokens': 100,
            'temperature': 0
        }"
----
+



