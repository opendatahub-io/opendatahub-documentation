:_module-type: PROCEDURE

[id='enabling-data-science-pipelines-2_{context}']
= Enabling Data Science Pipelines 2.0

ifdef::upstream[]
From {productname-long} version 2.10.0, data science pipelines are based on link:https://www.kubeflow.org/docs/components/pipelines/v2/[KubeFlow Pipelines (KFP) version 2.0]. DSP 2.0 is enabled and deployed by default in {productname-short}.
endif::[]

ifndef::upstream[]
ifdef::self-managed[]
From {productname-short} version 2.9, data science pipelines are based on link:https://www.kubeflow.org/docs/components/pipelines/v2/[KubeFlow Pipelines (KFP) version 2.0]. DSP 2.0 is enabled and deployed by default in {productname-short}.
endif::[]
ifdef::cloud-service[]
Data science pipelines in {productname-short} are now based on link:https://www.kubeflow.org/docs/components/pipelines/v2/[KubeFlow Pipelines (KFP) version 2.0]. DSP 2.0 is enabled and deployed by default in {productname-short}.
endif::[]

[IMPORTANT]
====
Data Science Pipelines (DSP) 2.0 contains an installation of Argo Workflows. {productname-short} does not support direct customer usage of this installation of Argo Workflows.

ifdef::upstream[]
If there is an existing installation of Argo Workflows that is not installed by DSP on your cluster, installations of and upgrades to {productname-short} 2.10.0 with DSP will fail.

To install or upgrade to {productname-short} 2.10.0 with DSP, ensure that no separate installation of Argo Workflows exists on your cluster.
endif::[]
ifndef::upstream[]
ifdef::self-managed[]
If there is an existing installation of Argo Workflows that is not installed by DSP on your cluster, installations of and upgrades to {productname-short} 2.9 with DSP will fail.
To install or upgrade to {productname-short} 2.9 with DSP, ensure that no separate installation of Argo Workflows exists on your cluster.
endif::[]
ifdef::cloud-service[]
If there is an existing installation of Argo Workflows that is not installed by DSP on your cluster, installations of and upgrades to {productname-short} with DSP 2.0 will fail.
To install or upgrade to {productname-short} with DSP 2.0, ensure that no separate installation of Argo Workflows exists on your cluster.
endif::[]
endif::[]
====

== Installing {productname-short} with DSP 2.0

ifdef::upstream[]
To install {productname-short} 2.10.0, ensure that there is no installation of Argo Workflows that is not installed by DSP on your cluster, and follow the installation steps described in link:{odhdocshome}/installing-open-data-hub/[Installing {productname-short}].
endif::[]

ifndef::upstream[]
ifdef::cloud-service[]
//RHOAI CS
To install {productname-short} with DSP 2.0, ensure that there is no installation of Argo Workflows that is not installed by DSP on your cluster, and follow the installation steps described in link:{rhoaidocshome}{default-format-url}/installing_and_uninstalling_openshift_ai_cloud_service/index[Installing and uninstalling OpenShift AI Cloud Service].
endif::[]

//RHOAI self-managed & disconnected
ifdef::self-managed[]
To install {productname-short} 2.9, ensure that there is no installation of Argo Workflows that is not installed by DSP on your cluster, and follow the installation steps described in link:{rhoaidocshome}{default-format-url}/installing_and_uninstalling_openshift_ai_self-managed/index[Installing and uninstalling OpenShift AI Self-Managed], or for disconnected environments, see link:{rhoaidocshome}{default-format-url}/installing_and_uninstalling_openshift_ai_self-managed_in_a_disconnected_environment[Installing and uninstalling {productname-long} in a disconnected environment].
endif::[]
endif::[]

== Upgrading to DSP 2.0

ifdef::upstream[]
To upgrade to {productname-short} 2.10.0 with DSP 2.0, ensure that there is no installation of Argo Workflows that is not installed by DSP on your cluster, and follow the upgrade steps described in link:{odhdocshome}/upgrading-open-data-hub/[Upgrading {productname-short}].
endif::[]

ifndef::upstream[]
ifdef::cloud-service[]
//RHOAI CS
To upgrade {productname-short}, ensure that there is no installation of Argo Workflows that is not installed by DSP on your cluster, and follow the upgrade steps described in link:{rhoaidocshome}{default-format-url}/upgrading_openshift_ai_cloud_service/index[Upgrading {productname-short } AI Cloud Service].
endif::[]

//RHOAI self-managed & disconnected
ifdef::self-managed[]
To upgrade to {productname-short} 2.9, ensure that there is no installation of Argo Workflows that is not installed by DSP on your cluster, and follow the upgrade steps described in link:{rhoaidocshome}{default-format-url}/upgrading_openshift_ai_self-managed/index[Upgrading {productname-short} Self-Managed], or for disconnected environments, link:{rhoaidocshome}{default-format-url}/upgrading_openshift_ai_self-managed_in_a_disconnected_environment/index[Upgrading {productname-long} in a disconnected environment].
endif::[]
endif::[]

ifndef::upstream[]
== Migrating pipelines from DSP 1.0 to 2.0

{productname-short} does not automatically migrate existing DSP 1.0 instances to 2.0. To use existing pipelines with DSP 2.0, you must manually migrate them.

. On {productname-short} 2.9, create a new data science project.
. Configure a new pipeline server. 
. Update and recompile your DSP 1.0 pipelines as described in link:https://www.kubeflow.org/docs/components/pipelines/v2/migration/[Migrate from KFP SDK v1: v1 to v2 migration instructions and breaking changes].
. Import your updated pipelines to your new DSP 2.0-based data science project.
. (Optional) Remove your DSP 1.0 pipeline server.

[IMPORTANT]
====
Data Science Pipelines 1.0 used the `kfp-tekton` Python library. Data Science Pipelines 2.0 does not use `kfp-tekton`. You can uninstall `kfp-tekton` when there are no remaining DSP 1.0 pipeline servers in use on your cluster.

For Data Science Pipelines 2.0, use the latest version of the KFP SDK. For more information, see the link:https://kubeflow-pipelines.readthedocs.io[Kubeflow Pipelines SDK API Reference].
====

== Accessing DSP 1.0 pipelines and history

You can view historical DSP 1.0 pipeline run information in the {openshift-platform} Console under *Pipelines > Project > PipelineRuns*. 

You can still connect to the KFP API server by using the `kfp-tekton` SDK for programmatic access to your pipelines and pipeline run history. For more information, see link:https://www.kubeflow.org/docs/components/pipelines/v1/sdk/pipelines-with-tekton/[Kubeflow Pipelines SDK for Tekton].


== Uninstalling the OpenShift Pipelines Operator

When your migration to DSP 2.0 is complete, and if you are not using OpenShift Pipelines for any purpose other than Data Science Pipelines 1.0, you can remove the OpenShift Pipelines Operator.

[IMPORTANT]
====
Before removing the OpenShift Pipelines Operator, ensure that migration of your DSP 1.0 pipelines to 2.0 is complete, and that there are no remaining DSP 1.0 pipeline servers in use on your cluster.
====

[role="_additional-resources"]
.Additional resources

* link:https://kubeflow-pipelines.readthedocs.io[Kubeflow Pipelines SDK API Reference].
* link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects/working-on-data-science-projects_nb-server#creating-a-data-science-project_nb-server[Creating a data science project]
* link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects/working-with-data-science-pipelines_ds-pipelines#configuring-a-pipeline-server_ds-pipelines[Configuring a pipeline server]
* link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects/working-with-data-science-pipelines_ds-pipelines#importing-a-data-science-pipeline_ds-pipelines[Importing a data science pipeline]
* link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects/working-with-data-science-pipelines_ds-pipelines#deleting-a-pipeline-server_ds-pipelines[Deleting a pipeline server]

endif::[]

