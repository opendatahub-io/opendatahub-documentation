:_module-type: PROCEDURE

[id='enabling-data-science-pipelines-2_{context}']
= Enabling data science pipelines 2.0

ifdef::upstream[]
From {productname-long} version 2.10.0, data science pipelines are based on link:https://www.kubeflow.org/docs/components/pipelines/v2/[KubeFlow Pipelines (KFP) version 2.0]. data science pipelines 2.0 is enabled and deployed by default in {productname-short}.
endif::[]

ifndef::upstream[]
ifdef::self-managed[]
From {productname-short} version 2.9, data science pipelines are based on link:https://www.kubeflow.org/docs/components/pipelines/v2/[KubeFlow Pipelines (KFP) version 2.0]. data science pipelines 2.0 is enabled and deployed by default in {productname-short}.
endif::[]
ifdef::cloud-service[]
Data science pipelines in {productname-short} are now based on link:https://www.kubeflow.org/docs/components/pipelines/v2/[KubeFlow Pipelines (KFP) version 2.0]. data science pipelines 2.0 is enabled and deployed by default in {productname-short}.
endif::[]
endif::[]

[NOTE]
====
The `PipelineConf` class is deprecated, and there is no KFP 2.0 equivalent.
====

[IMPORTANT]
====
Data science pipelines 2.0 contains an installation of Argo Workflows. {productname-short} does not support direct customer usage of this installation of Argo Workflows.

ifdef::upstream[]
To install or upgrade to {productname-short} 2.10.0 with data science pipelines, ensure that your cluster does not have an existing installation of Argo Workflows that is not installed by {productname-short}.
endif::[]
ifndef::upstream[]
ifdef::self-managed[]
To install or upgrade to {productname-short} 2.9 with data science pipelines, ensure that your cluster does not have an existing installation of Argo Workflows that is not installed by {productname-short}.
endif::[]
ifdef::cloud-service[]
To install or upgrade to {productname-short} with data science pipelines 2.0, ensure that your cluster does not have an existing installation of Argo Workflows that is not installed by {productname-short}.
endif::[]
endif::[]

Argo Workflows resources that are created by {productname-short} have the following labels in the OpenShift Console under *Administration > CustomResourceDefinitions*, in the `argoproj.io` group:
[source]
----
 labels:
    app.kubernetes.io/part-of: data-science-pipelines-operator
    app.opendatahub.io/data-science-pipelines-operator: 'true'
----
====

== Installing {productname-short} with data science pipelines 2.0

ifdef::upstream[]
To install {productname-short} 2.10.0, ensure that there is no installation of Argo Workflows that is not installed by data science pipelines on your cluster, and follow the installation steps described in link:{odhdocshome}/installing-open-data-hub/[Installing {productname-short}].

If there is an existing installation of Argo Workflows that is not installed by data science pipelines on your cluster, data science pipelines will be disabled after you install {productname-short} 2.10.0 with data science pipelines.

To enable data science pipelines, remove the separate installation of Argo Workflows from your cluster. Data science pipelines will be enabled automatically. 
endif::[]

ifndef::upstream[]
ifdef::cloud-service[]
//RHOAI CS
To install {productname-short} with data science pipelines 2.0, ensure that there is no installation of Argo Workflows that is not installed by data science pipelines on your cluster, and follow the installation steps described in link:{rhoaidocshome}{default-format-url}/installing_and_uninstalling_openshift_ai_cloud_service/index[Installing and uninstalling OpenShift AI Cloud Service].

If there is an existing installation of Argo Workflows that is not installed by data science pipelines on your cluster, data science pipelines will be disabled after you install {productname-short}.

To enable data science pipelines, remove the separate installation of Argo Workflows from your cluster. Data science pipelines will be enabled automatically. 
endif::[]

//RHOAI self-managed & disconnected
ifdef::self-managed[]
To install {productname-short} 2.9, ensure that there is no installation of Argo Workflows that is not installed by data science pipelines on your cluster, and follow the installation steps described in link:{rhoaidocshome}{default-format-url}/installing_and_uninstalling_openshift_ai_self-managed/index[Installing and uninstalling OpenShift AI Self-Managed], or for disconnected environments, see link:{rhoaidocshome}{default-format-url}/installing_and_uninstalling_openshift_ai_self-managed_in_a_disconnected_environment[Installing and uninstalling {productname-long} in a disconnected environment].

If there is an existing installation of Argo Workflows that is not installed by data science pipelines on your cluster, data science pipelines will be disabled after you install {productname-short} 2.9 or later.

To enable data science pipelines, remove the separate installation of Argo Workflows from your cluster. Data science pipelines will be enabled automatically. 
endif::[]
endif::[]

== Upgrading to data science pipelines 2.0

ifdef::upstream[]
[IMPORTANT]
====
After you upgrade to {productname-short} 2.9 or later, pipelines created with data science pipelines 1.0 continue to run, but are inaccessible from the {productname-short} dashboard. If you are a current data science pipelines user, do not upgrade to {productname-short} with data science pipelines 2.0 until you are ready to migrate to the new pipelines solution.
====

To upgrade to {productname-short} {vernum} with data science pipelines 2.0, ensure that there is no installation of Argo Workflows that is not installed by data science pipelines on your cluster, and follow the upgrade steps described in link:{odhdocshome}/upgrading-open-data-hub/[Upgrading {productname-short}].

If you upgrade to {productname-short} {vernum} with data science pipelines enabled and an Argo Workflows installation that is not installed by data science pipelines exists on your cluster, {productname-short} components will not be upgraded. To complete the component upgrade, disable data science pipelines or remove the separate installation of Argo Workflows. The component upgrade will complete automatically.
endif::[]
ifndef::upstream[]
ifdef::cloud-service[]
//RHOAI CS
[IMPORTANT]
====
After you upgrade to {productname-short} with data science pipelines 2.0, pipelines created with data science pipelines 1.0 continue to run, but are inaccessible from the {productname-short} dashboard. If you are a current data science pipelines user, do not upgrade to {productname-short} with data science pipelines 2.0 until you are ready to migrate to the new pipelines solution.
====

To upgrade to data science pipelines 2.0, follow these steps:

. Ensure that your cluster does not have an existing installation of Argo Workflows that is not installed by {productname-short}, and then follow the upgrade steps described in link:{rhoaidocshome}{default-format-url}/upgrading_openshift_ai_cloud_service/index[Upgrading {productname-short} AI Cloud Service].
+
If you upgrade to {productname-short} with data science pipelines 2.0 enabled, and there is an existing installation of Argo Workflows that is not installed by data science pipelines on your cluster, {productname-short} components will not be upgraded. To complete the component upgrade, disable data science pipelines or remove the separate installation of Argo Workflows from your cluster. The component upgrade will then complete automatically. 
. Update your workbenches to use the notebook image version 2024.1 or later. For more information, see link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects/using-project-workbenches_projects#updating-a-project-workbench_projects[Updating a project workbench].
. Manually migrate your pipelines from data science pipelines 1.0 to 2.0. For more information, see link:{rhoaidocshome}{default-format-url}/working_with_data_science_pipelines#migrating_pipelines_from_data-science-pipelines_1_0_to_2_0[Migrating pipelines from data science pipelines 1.0 to 2.0].
endif::[]

//RHOAI self-managed & disconnected
ifdef::self-managed[]
[IMPORTANT]
====
After you upgrade to {productname-short} 2.9 or later, pipelines created with data science pipelines 1.0 continue to run, but are inaccessible from the {productname-short} dashboard. If you are a current data science pipelines user, do not upgrade to {productname-short} with data science pipelines 2.0 until you are ready to migrate to the new pipelines solution.
====

To upgrade to data science pipelines 2.0, follow these steps:

. Ensure that your cluster does not have an existing installation of Argo Workflows that is not installed by {productname-short}, and then follow the upgrade steps described in link:{rhoaidocshome}{default-format-url}/upgrading_openshift_ai_self-managed/index[Upgrading {productname-short} Self-Managed], or for disconnected environments, link:{rhoaidocshome}{default-format-url}/upgrading_openshift_ai_self-managed_in_a_disconnected_environment/index[Upgrading {productname-long} in a disconnected environment].
+
If you upgrade to {productname-short} 2.9 or later with data science pipelines enabled, and there is an existing installation of Argo Workflows that is not installed by data science pipelines on your cluster, {productname-short} components will not be upgraded. To complete the component upgrade, disable data science pipelines or remove the separate installation of Argo Workflows from your cluster. The component upgrade will then complete automatically.
. Update your workbenches to use the notebook image version 2024.1 or later. For more information, see link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects/using-project-workbenches_projects#updating-a-project-workbench_projects[Updating a project workbench]. 
. Manually migrate your pipelines from data science pipelines 1.0 to 2.0. For more information, see link:{rhoaidocshome}{default-format-url}/working_with_data_science_pipelines#migrating_pipelines_from_data-science-pipelines_1_0_to_2_0[Migrating pipelines from data science pipelines 1.0 to 2.0].
endif::[]
endif::[]

ifndef::upstream[]
== Migrating pipelines from data science pipelines 1.0 to 2.0

{productname-short} does not automatically migrate existing data science pipelines 1.0 instances to 2.0. To use existing pipelines with data science pipelines 2.0, you must manually migrate them.

ifdef::self-managed[]
. On {productname-short} 2.9, create a new data science project.
endif::[]
ifdef::cloud-service[]
. On {productname-short} with data science pipelines 2.0, create a new data science project.
endif::[]
. Configure a new pipeline server. 
. Update and recompile your data science pipelines 1.0 pipelines as described in link:https://www.kubeflow.org/docs/components/pipelines/v2/migration/[Migrate from KFP SDK v1: v1 to v2 migration instructions and breaking changes].
+
[NOTE]
====
data science pipelines 2.0 does not use the `kfp-tekton` library. In most cases, you can replace usage of `kfp-tekton` with the `kfp` library.
====
. Import your updated pipelines to your new data science pipelines 2.0-based data science project.
. (Optional) Remove your data science pipelines 1.0 pipeline server.

[IMPORTANT]
====
Data science pipelines 1.0 used the `kfp-tekton` Python libraryData science pipelines 2.0 does not use `kfp-tekton`. You can uninstall `kfp-tekton` when there are no remaining data science pipelines 1.0 pipeline servers in use on your cluster.

For Data science pipelines 2.0, use the latest version of the KFP SDK. For more information, see the link:https://kubeflow-pipelines.readthedocs.io[Kubeflow Pipelines SDK API Reference].
====

== Accessing data science pipelines 1.0 pipelines and history

You can view historical data science pipelines 1.0 pipeline run information in the {openshift-platform} Console under *Pipelines > Project > PipelineRuns*. 

You can still connect to the KFP API server by using the `kfp-tekton` SDK for programmatic access to your pipelines and pipeline run history. For more information, see link:https://www.kubeflow.org/docs/components/pipelines/v1/sdk/pipelines-with-tekton/[Kubeflow Pipelines SDK for Tekton].


== Uninstalling the OpenShift Pipelines Operator

When your migration to data science pipelines 2.0 is complete, and if you are not using OpenShift Pipelines for any purpose other than data science pipelines 1.0, you can remove the OpenShift Pipelines Operator.

[IMPORTANT]
====
Before removing the OpenShift Pipelines Operator, ensure that migration of your data science pipelines 1.0 pipelines to 2.0 is complete, and that there are no remaining data science pipelines 1.0 pipeline servers in use on your cluster.
====

[role="_additional-resources"]
.Additional resources

* link:https://pypi.org/project/kfp/[PyPI: kfp^]
* link:https://kubeflow-pipelines.readthedocs.io[Kubeflow Pipelines SDK API Reference].
* link:{rhoaidocshome}{default-format-url}/working_on_data_science_projects/using-data-science-projects_projects#creating-a-data-science-project_projects[Creating a data science project]
* link:{rhoaidocshome}{default-format-url}/working_with_data_science_pipelines/managing-data-science-pipelines_ds-pipelines#configuring-a-pipeline-server_ds-pipelines[Configuring a pipeline server]
* link:{rhoaidocshome}{default-format-url}/working_with_data_science_pipelines/managing-data-science-pipelines_ds-pipelines#importing-a-data-science-pipeline_ds-pipelines[Importing a data science pipeline]
* link:{rhoaidocshome}{default-format-url}/working_with_data_science_pipelines/managing-data-science-pipelines_ds-pipelines#deleting-a-pipeline-server_ds-pipelines[Deleting a pipeline server]

endif::[]