:_module-type: PROCEDURE

[id="viewing-performance-metrics-for-a-nim-model_{context}"]
= Viewing performance metrics for a NIM model

[role='_abstract']

You can observe the following performance metrics for a NIM model deployed on the NVIDIA NIM model serving platform:

* *Number of requests* - The number of requests that have failed or succeeded for a specific model.
* *Average response time (ms)* - The average time it takes a specific model to respond to requests.
* *CPU utilization (%)* - The percentage of the CPU limit per model replica that is currently utilized by a specific model.
* *Memory utilization (%)* - The percentage of the memory limit per model replica that is utilized by a specific model.

You can specify a time range and a refresh interval for these metrics to help you determine, for example, the peak usage hours and model performance at a specified time.

.Prerequisites

* You have enabled the NVIDIA NIM model serving platform.
* You have deployed a NIM model on the NVIDIA NIM model serving platform.
ifndef::upstream[]
* If you are using {productname-short} groups, you are part of the user group or admin group (for example, {oai-user-group} or {oai-admin-group}) in OpenShift.
endif::[]
ifdef::upstream[]
* If you are using {productname-short} groups, you are part of the user group or admin group (for example, {odh-user-group} or {odh-admin-group}) in OpenShift.
endif::[]
* The `disableKServeMetrics` {productname-short} dashboard configuration option is set to its default value of `false`:
+
[source]
----
disableKServeMetrics: false
----
ifdef::upstream[]
For more information, see link:{odhdocshome}/managing-odh/#ref-dashboard-configuration-options_dashboard[Dashboard configuration options].
endif::[]
ifndef::upstream[]
For more information, see link:{rhoaidocshome}{default-format-url}/managing_openshift_ai/customizing-the-dashboard#ref-dashboard-configuration-options_dashboard[Dashboard configuration options].
endif::[]

.Procedure

. From the {productname-short} dashboard navigation menu, click *Data science projects*.
+
The *Data science projects* page opens.
. Click the name of the project that contains the NIM model that you want to monitor.

. In the project details page, click the *Models* tab.

. Click the NIM model that you want to observe.

. On the *Endpoint performance* tab, set the following options:

** *Time range* - Specifies how long to track the metrics. You can select one of these values: 1 hour, 24 hours, 7 days, and 30 days.

** *Refresh interval* - Specifies how frequently the graphs on the metrics page are refreshed to show the latest data. You can select one of these values: 15 seconds, 30 seconds, 1 minute, 5 minutes, 15 minutes, 30 minutes, 1 hour, 2 hours, and 1 day.

. Scroll down to view data graphs for performance metrics.

.Verification

The *Endpoint performance* tab shows graphs of performance metrics for the deployed NIM model.


//.Additional resources
