---
layout: docs
title: Working with data science pipelines
permalink: /docs/working-with-data-science-pipelines
custom_css: asciidoc.css
---

:upstream:
include::_artifacts/document-attributes-global.adoc[]

= Working with data science pipelines
:doctype: book
:toc: left
:compat-mode:
:context: ds-pipelines
 
[role='_abstract']
As a data scientist, you can enhance your data science projects on {productname-short} by building portable machine learning (ML) workflows with data science pipelines, using Docker containers. This enables you to standardize and automate machine learning workflows to enable you to develop and deploy your data science models.

For example, the steps in a machine learning workflow might include items such as data extraction, data processing, feature extraction, model training, model validation, and model serving. Automating these activities enables your organization to develop a continuous process of retraining and updating a model based on newly received data. This can help resolve challenges related to building an integrated machine learning deployment and continuously operating it in production.

ifndef::upstream[]
You can also use the Elyra JupyterLab extension to create and run data science pipelines within JupyterLab. For more information, see link:{rhoaidocshome}{default-format-url}/working_with_data_science_pipelines/working-with-pipelines-in-jupyterlab_ds-pipelines[Working with pipelines in JupyterLab].
endif::[]
ifdef::upstream[]
You can also use the Elyra JupyterLab extension to create and run data science pipelines within JupyterLab. For more information, see link:{odhdocshome}/working-with-data-science-pipelines/#working-with-pipelines-in-jupyterlab_ds-pipelines[Working with pipelines in JupyterLab].
endif::[]

ifdef::upstream[]
From {productname-long} version 2.10.0, data science pipelines are based on link:https://www.kubeflow.org/docs/components/pipelines/v2/[KubeFlow Pipelines (KFP) version 2.0]. For more information, see link:{odhdocshome}/working-with-data-science-pipelines/#enabling-data-science-pipelines-2_ds-pipelines[Enabling data science pipelines 2.0].
endif::[]
ifndef::upstream[]
ifdef::self-managed[]
From {productname-short} version 2.9, data science pipelines are based on link:https://www.kubeflow.org/docs/components/pipelines/v2/[KubeFlow Pipelines (KFP) version 2.0]. For more information, see link:{rhoaidocshome}{default-format-url}/working_with_data_science_pipelines/enabling-data-science-pipelines-2_ds-pipelines[Enabling data science pipelines 2.0].
endif::[]
ifdef::cloud-service[]
Data science pipelines in {productname-short} are now based on link:https://www.kubeflow.org/docs/components/pipelines/v2/[KubeFlow Pipelines (KFP) version 2.0]. For more information, see link:{rhoaidocshome}{default-format-url}working_on_data_science_projects/working-with-data-science-pipelines_ds-pipelines#enabling-data-science-pipelines-2_ds-pipelines[Enabling data science pipelines 2.0].
endif::[]
endif::[]

A data science pipeline in {productname-short} consists of the 4 following components:

1. Pipeline server: A server that is attached to your data science project and hosts your data science pipeline.
2. Pipeline: A pipeline defines the configuration of your machine learning workflow and the relationship between each component in the workflow.
** Pipeline code: A definition of your pipeline in a YAML file.
** Pipeline graph: A graphical illustration of the steps executed in a pipeline run and the relationship between them.
3. Pipeline experiment: A workspace where you can try different configurations of your pipelines. You can use experiments to organize your runs into logical groups.
*** Archived pipeline experiment: An archived pipeline experiment.
*** Pipeline artifact: An output artifact produced by a pipeline component. 
*** Pipeline execution: The execution of a task in a pipeline.
4. Pipeline run: An execution of your pipeline.
** Active run: A pipeline run that is executing, or stopped.
** Scheduled run: A pipeline run that is scheduled to execute at least once.
** Archived run: An archived pipeline run. 

This feature is based on Kubeflow Pipelines 2.0. Use the latest Kubeflow Pipelines 2.0 SDK to build your data science pipeline in Python code. After you have built your pipeline, use the SDK to compile it into an Intermediate Representation (IR) YAML file. The {productname-short} user interface enables you to track and manage pipelines and pipeline runs.

You can store your pipeline artifacts in an S3-compatible object storage bucket so that you do not consume local storage. To do this, you must first configure write access to your S3 bucket on your storage account.

//Enabling data science pipelines 2.0
include::modules/enabling-data-science-pipelines-2.adoc[leveloffset=+1]

include::assemblies/managing-data-science-pipelines.adoc[leveloffset=+1]

include::assemblies/managing-pipeline-experiments.adoc[leveloffset=+1]

include::assemblies/managing-pipeline-runs.adoc[leveloffset=+1]

include::assemblies/working-with-pipeline-logs.adoc[leveloffset=+1]

include::assemblies/working-with-pipelines-in-jupyterlab.adoc[leveloffset=+1]


[role='_additional-resources']
== Additional resources
* link:https://www.kubeflow.org/docs/components/pipelines/v2/[Kubeflow Pipelines 2.0 Documentation]

